{"version":3,"sources":["../../../../src/lib/hybrid-search.ts","../../../../src/lib/app-settings.ts","../../../../src/lib/chunked-processing.ts","../../../../src/lib/chart-schema.ts","../../../../src/lib/ai-service-enhanced.ts"],"sourcesContent":["import { db as prisma } from \"./db\";\nimport { SemanticVectorService } from \"./semantic-vector\";\n\nexport interface HybridSearchResult {\n\tid: string; // Chapter ID (BigInt as string)\n\tsubject: string;\n\ttitle: string;\n\tcontent: string;\n\tcreated_at: Date | null;\n\trrf_score: number;\n\tsemantic_rank?: number;\n\tkeyword_rank?: number;\n\tcitation?: {\n\t\tpageNumber: number;\n\t\timageUrl: string;\n\t\tboundingBox: any;\n\t\tlayoutItems?: any[];\n\t\tchunkContent: string;\n\t\ttitle: string;\n\t};\n}\n\nexport class HybridSearchService {\n\t/**\n\t * The RRF Hybrid Search Implementation (Parallel Ensemble)\n\t * Enforces board_id filtering for multi-tenancy.\n\t */\n\tstatic async search(\n\t\tquery: string,\n\t\tlimit: number = 20,\n\t\tfilters: {\n\t\t\tboardId: string;\n\t\t\tsubjectId?: number;\n\t\t\tchapterId?: number;\n\t\t}\n\t): Promise<{\n\t\tresults: HybridSearchResult[];\n\t\tsearchMethod:\n\t\t| \"hybrid\"\n\t\t| \"semantic_fallback\"\n\t\t| \"tsvector_only\"\n\t\t| \"vector_only\"\n\t\t| \"keyword_only\";\n\t\tstats: {\n\t\t\ttsvectorResults: number;\n\t\t\tsemanticResults: number;\n\t\t\tfinalResults: number;\n\t\t};\n\t}> {\n\t\ttry {\n\t\t\tconsole.log(`[HYBRID RRF] Starting search for: \"${query}\"`, filters);\n\n\t\t\tif (!filters.boardId) {\n\t\t\t\tthrow new Error(\"boardId is required for search\");\n\t\t\t}\n\n\t\t\t// 1. Handle Empty Query\n\t\t\t// If query is empty but filters are provided, return all matching chunks\n\t\t\t// (needed for analytical queries that need complete data)\n\t\t\tconst isEmptyQuery = !query || !query.trim();\n\t\t\tconst hasFilters = filters.subjectId || filters.chapterId;\n\n\t\t\tif (isEmptyQuery && !hasFilters) {\n\t\t\t\treturn {\n\t\t\t\t\tresults: [],\n\t\t\t\t\tsearchMethod: \"tsvector_only\",\n\t\t\t\t\tstats: { tsvectorResults: 0, semanticResults: 0, finalResults: 0 },\n\t\t\t\t};\n\t\t\t}\n\n\t\t\t// 2. Generate Embedding (use a dummy query if empty, but we'll filter by filters only)\n\t\t\tconst embedding = isEmptyQuery\n\t\t\t\t? await SemanticVectorService.generateEmbedding(\"dummy\") // Dummy embedding for empty queries\n\t\t\t\t: await SemanticVectorService.generateEmbedding(query);\n\n\t\t\t// 3. Prepare inputs\n\t\t\tconst boardId = filters.boardId;\n\t\t\tconst subjectId = filters.subjectId || null;\n\t\t\tconst chapterId = filters.chapterId || null;\n\t\t\tconst chapterIdBigInt = chapterId ? BigInt(chapterId) : null;\n\n\t\t\t// 4. RRF Query\n\t\t\t// For empty queries with filters, use a simpler query that just filters by board/subject/chapter\n\t\t\tlet results: any[];\n\n\t\t\tif (isEmptyQuery && hasFilters) {\n\t\t\t\t// Empty query with filters - return all chunks matching filters\n\t\t\t\tconsole.log(\n\t\t\t\t\t`[HYBRID RRF] Empty query with filters - returning all matching chunks`\n\t\t\t\t);\n\t\t\t\tconsole.log(\n\t\t\t\t\t`[HYBRID RRF] Filters: boardId=${boardId}, subjectId=${subjectId}, chapterId=${chapterId} (type: ${typeof chapterId})`\n\t\t\t\t);\n\n\t\t\t\t// When chapterId is provided, skip board filtering\n\t\t\t\t// The chapter's accessible_boards already controls access at the chapter level\n\t\t\t\tif (chapterIdBigInt) {\n\t\t\t\t\tconsole.log(\n\t\t\t\t\t\t`[HYBRID RRF] chapterId provided - skipping board filter, returning all chunks for chapter`\n\t\t\t\t\t);\n\t\t\t\t\tresults = await prisma.$queryRawUnsafe<any[]>(\n\t\t\t\t\t\t`\n\t\t\t\t\t\tSELECT \n\t\t\t\t\t\t\tcc.id as chunk_id,\n\t\t\t\t\t\t\tc.id as chapter_id,\n\t\t\t\t\t\t\tc.id as id,\n\t\t\t\t\t\t\tc.title,\n\t\t\t\t\t\t\ts.name as subject_name,\n\t\t\t\t\t\t\ts.name as subject,\n\t\t\t\t\t\t\tcc.content as chunk_content,\n\t\t\t\t\t\t\tcc.content as content,\n\t\t\t\t\t\t\tcc.page_number,\n\t\t\t\t\t\t\tcc.bbox,\n\t\t\t\t\t\t\tc.created_at,\n\t\t\t\t\t\t\tcp.image_url,\n\t\t\t\t\t\t\t1.0 as rrf_score,\n\t\t\t\t\t\t\t1 as semantic_rank,\n\t\t\t\t\t\t\t1 as keyword_rank\n\t\t\t\t\t\tFROM chapter_chunks cc\n\t\t\t\t\t\tJOIN chapters c ON cc.chapter_id = c.id\n\t\t\t\t\t\tJOIN subjects s ON c.subject_id = s.id\n\t\t\t\t\t\tLEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n\t\t\t\t\t\tWHERE c.id = $1\n\t\t\t\t\t\t  AND ($2::int IS NULL OR c.subject_id = $2)\n\t\t\t\t\t\t  AND c.is_active = true\n\t\t\t\t\t\t  AND c.processing_status = 'COMPLETED'\n\t\t\t\t\t\tORDER BY cc.chunk_index ASC\n\t\t\t\t\t\tLIMIT $3\n\t\t\t\t\t\t`,\n\t\t\t\t\t\tchapterIdBigInt, // $1\n\t\t\t\t\t\tsubjectId, // $2\n\t\t\t\t\t\tlimit // $3\n\t\t\t\t\t);\n\t\t\t\t} else {\n\t\t\t\t\t// No chapterId - apply board filtering\n\t\t\t\t\tresults = await prisma.$queryRawUnsafe<any[]>(\n\t\t\t\t\t\t`\n\t\t\t\t\t\tSELECT \n\t\t\t\t\t\t\tcc.id as chunk_id,\n\t\t\t\t\t\t\tc.id as chapter_id,\n\t\t\t\t\t\t\tc.id as id,\n\t\t\t\t\t\t\tc.title,\n\t\t\t\t\t\t\ts.name as subject_name,\n\t\t\t\t\t\t\ts.name as subject,\n\t\t\t\t\t\t\tcc.content as chunk_content,\n\t\t\t\t\t\t\tcc.content as content,\n\t\t\t\t\t\t\tcc.page_number,\n\t\t\t\t\t\t\tcc.bbox,\n\t\t\t\t\t\t\tc.created_at,\n\t\t\t\t\t\t\tcp.image_url,\n\t\t\t\t\t\t\t1.0 as rrf_score,\n\t\t\t\t\t\t\t1 as semantic_rank,\n\t\t\t\t\t\t\t1 as keyword_rank\n\t\t\t\t\t\tFROM chapter_chunks cc\n\t\t\t\t\t\tJOIN chapters c ON cc.chapter_id = c.id\n\t\t\t\t\t\tJOIN subjects s ON c.subject_id = s.id\n\t\t\t\t\t\tLEFT JOIN chapter_chunk_boards ccb ON cc.id = ccb.chunk_id AND ccb.board_id = $1\n\t\t\t\t\t\tLEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n\t\t\t\t\t\tWHERE (c.is_global = true OR ccb.chunk_id IS NOT NULL)\n\t\t\t\t\t\t  AND ($2::int IS NULL OR c.subject_id = $2)\n\t\t\t\t\t\t  AND c.is_active = true\n\t\t\t\t\t\t  AND c.processing_status = 'COMPLETED'\n\t\t\t\t\t\tORDER BY cc.chunk_index ASC\n\t\t\t\t\t\tLIMIT $3\n\t\t\t\t\t\t`,\n\t\t\t\t\t\tboardId, // $1\n\t\t\t\t\t\tsubjectId, // $2\n\t\t\t\t\t\tlimit // $3\n\t\t\t\t\t);\n\t\t\t\t}\n\n\t\t\t\tconsole.log(`[HYBRID RRF] Query returned ${results.length} results`);\n\t\t\t} else {\n\t\t\t\t// Normal query with search terms\n\t\t\t\t// When chapterId is provided, skip board filtering\n\t\t\t\tconst chapterIdBigInt = chapterId ? BigInt(chapterId) : null;\n\n\t\t\t\tif (chapterIdBigInt) {\n\t\t\t\t\t// chapterId provided - skip board filtering\n\t\t\t\t\tconsole.log(\n\t\t\t\t\t\t`[HYBRID RRF] chapterId provided - skipping board filter in search query`\n\t\t\t\t\t);\n\t\t\t\t\tresults = await prisma.$queryRawUnsafe<any[]>(\n\t\t\t\t\t\t`\n        WITH semantic_search AS (\n            SELECT \n                cc.id as chunk_id, \n                c.id as chapter_id, \n                c.title, \n                s.name as subject_name,\n                cc.content as chunk_content,\n                cc.page_number,\n                cc.bbox,\n                c.created_at,\n                cp.image_url,\n                RANK() OVER (ORDER BY cc.semantic_vector <=> $1::vector) as rank\n            FROM chapter_chunks cc\n            JOIN chapters c ON cc.chapter_id = c.id\n            JOIN subjects s ON c.subject_id = s.id\n            LEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n            WHERE cc.semantic_vector IS NOT NULL\n              AND c.id = $2\n              AND ($3::int IS NULL OR c.subject_id = $3)\n              AND c.is_active = true\n              AND c.processing_status = 'COMPLETED'\n            ORDER BY cc.semantic_vector <=> $1::vector\n            LIMIT 50\n        ),\n        keyword_search AS (\n            SELECT \n                cc.id as chunk_id, \n                c.id as chapter_id, \n                c.title, \n                s.name as subject_name,\n                cc.content as chunk_content,\n                cc.page_number,\n                cc.bbox,\n                c.created_at,\n                cp.image_url,\n                ts_rank_cd(cc.search_vector, websearch_to_tsquery('english', $4)) as search_rank,\n                RANK() OVER (ORDER BY ts_rank_cd(cc.search_vector, websearch_to_tsquery('english', $4)) DESC) as rank\n            FROM chapter_chunks cc\n            JOIN chapters c ON cc.chapter_id = c.id\n            JOIN subjects s ON c.subject_id = s.id\n            LEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n            WHERE cc.search_vector @@ websearch_to_tsquery('english', $4)\n              AND c.id = $2\n              AND ($3::int IS NULL OR c.subject_id = $3)\n              AND c.is_active = true\n              AND c.processing_status = 'COMPLETED'\n            ORDER BY search_rank DESC\n            LIMIT 50\n        )\n        SELECT \n            COALESCE(s.chapter_id, k.chapter_id) as id,\n            COALESCE(s.subject_name, k.subject_name) as subject,\n            COALESCE(s.title, k.title) as title,\n            COALESCE(s.chunk_content, k.chunk_content) as content,\n            COALESCE(s.created_at, k.created_at) as created_at,\n            \n            (COALESCE(1.0 / (60 + s.rank), 0.0) + COALESCE(1.0 / (60 + k.rank), 0.0)) as rrf_score,\n            \n            s.rank as semantic_rank,\n            k.rank as keyword_rank,\n            \n            COALESCE(s.page_number, k.page_number) as page_number,\n            COALESCE(s.bbox, k.bbox) as bbox,\n            COALESCE(s.image_url, k.image_url) as image_url\n        FROM semantic_search s\n        FULL OUTER JOIN keyword_search k ON s.chunk_id = k.chunk_id\n        ORDER BY rrf_score DESC\n        LIMIT $5\n\t\t\t`,\n\t\t\t\t\t\t`[${embedding.join(\",\")}]`, // $1\n\t\t\t\t\t\tchapterIdBigInt, // $2\n\t\t\t\t\t\tsubjectId, // $3\n\t\t\t\t\t\tquery, // $4\n\t\t\t\t\t\tlimit // $5\n\t\t\t\t\t);\n\t\t\t\t} else {\n\t\t\t\t\t// No chapterId - apply board filtering\n\t\t\t\t\tresults = await prisma.$queryRawUnsafe<any[]>(\n\t\t\t\t\t\t`\n        WITH semantic_search AS (\n            SELECT \n                cc.id as chunk_id, \n                c.id as chapter_id, \n                c.title, \n                s.name as subject_name,\n                cc.content as chunk_content,\n                cc.page_number,\n                cc.bbox,\n                c.created_at,\n                cp.image_url,\n                RANK() OVER (ORDER BY cc.semantic_vector <=> $1::vector) as rank\n            FROM chapter_chunks cc\n            JOIN chapters c ON cc.chapter_id = c.id\n            JOIN subjects s ON c.subject_id = s.id\n            LEFT JOIN chapter_chunk_boards ccb ON cc.id = ccb.chunk_id AND ccb.board_id = $2\n            LEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n            WHERE cc.semantic_vector IS NOT NULL\n              AND (c.is_global = true OR ccb.chunk_id IS NOT NULL)\n              AND ($3::int IS NULL OR c.subject_id = $3)\n              AND c.is_active = true\n              AND c.processing_status = 'COMPLETED'\n            ORDER BY cc.semantic_vector <=> $1::vector\n            LIMIT 50\n        ),\n        keyword_search AS (\n            SELECT \n                cc.id as chunk_id, \n                c.id as chapter_id, \n                c.title, \n                s.name as subject_name,\n                cc.content as chunk_content,\n                cc.page_number,\n                cc.bbox,\n                c.created_at,\n                cp.image_url,\n                ts_rank_cd(cc.search_vector, websearch_to_tsquery('english', $4)) as search_rank,\n                RANK() OVER (ORDER BY ts_rank_cd(cc.search_vector, websearch_to_tsquery('english', $4)) DESC) as rank\n            FROM chapter_chunks cc\n            JOIN chapters c ON cc.chapter_id = c.id\n            JOIN subjects s ON c.subject_id = s.id\n            LEFT JOIN chapter_chunk_boards ccb ON cc.id = ccb.chunk_id AND ccb.board_id = $2\n            LEFT JOIN chapter_pages cp ON c.id = cp.chapter_id AND cc.page_number = cp.page_number\n            WHERE cc.search_vector @@ websearch_to_tsquery('english', $4)\n              AND (c.is_global = true OR ccb.chunk_id IS NOT NULL)\n              AND ($3::int IS NULL OR c.subject_id = $3)\n              AND c.is_active = true\n              AND c.processing_status = 'COMPLETED'\n            ORDER BY search_rank DESC\n            LIMIT 50\n        )\n        SELECT \n            COALESCE(s.chapter_id, k.chapter_id) as id,\n            COALESCE(s.subject_name, k.subject_name) as subject,\n            COALESCE(s.title, k.title) as title,\n            COALESCE(s.chunk_content, k.chunk_content) as content,\n            COALESCE(s.created_at, k.created_at) as created_at,\n            \n            (COALESCE(1.0 / (60 + s.rank), 0.0) + COALESCE(1.0 / (60 + k.rank), 0.0)) as rrf_score,\n            \n            s.rank as semantic_rank,\n            k.rank as keyword_rank,\n            \n            COALESCE(s.page_number, k.page_number) as page_number,\n            COALESCE(s.bbox, k.bbox) as bbox,\n            COALESCE(s.image_url, k.image_url) as image_url\n        FROM semantic_search s\n        FULL OUTER JOIN keyword_search k ON s.chunk_id = k.chunk_id\n        ORDER BY rrf_score DESC\n        LIMIT $5\n\t\t\t`,\n\t\t\t\t\t\t`[${embedding.join(\",\")}]`, // $1\n\t\t\t\t\t\tboardId, // $2\n\t\t\t\t\t\tsubjectId, // $3\n\t\t\t\t\t\tquery, // $4\n\t\t\t\t\t\tlimit // $5\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tconsole.log(`[HYBRID RRF] Found ${results.length} results.`);\n\n\t\t\tconst semanticCount = results.filter((r) => r.semantic_rank).length;\n\t\t\tconst keywordCount = results.filter((r) => r.keyword_rank).length;\n\n\t\t\tlet searchMethod: \"hybrid\" | \"vector_only\" | \"keyword_only\" = \"hybrid\";\n\t\t\tif (semanticCount > 0 && keywordCount === 0) searchMethod = \"vector_only\";\n\t\t\telse if (keywordCount > 0 && semanticCount === 0)\n\t\t\t\tsearchMethod = \"keyword_only\";\n\n\t\t\tconst mappedResults: HybridSearchResult[] = results.map((r: any) => {\n\t\t\t\tlet parsedBbox = r.bbox;\n\t\t\t\tif (typeof r.bbox === \"string\") {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tparsedBbox = JSON.parse(r.bbox);\n\t\t\t\t\t} catch (e) {\n\t\t\t\t\t\tparsedBbox = null;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tlet boundingBox: number[] | undefined = undefined;\n\t\t\t\tif (parsedBbox && Array.isArray(parsedBbox) && parsedBbox.length > 0) {\n\t\t\t\t\tif (typeof parsedBbox[0] === \"object\" && parsedBbox[0].bbox) {\n\t\t\t\t\t\tboundingBox = parsedBbox[0].bbox;\n\t\t\t\t\t} else if (typeof parsedBbox[0] === \"number\") {\n\t\t\t\t\t\tboundingBox = parsedBbox;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn {\n\t\t\t\t\tid: r.id.toString(),\n\t\t\t\t\tsubject: r.subject,\n\t\t\t\t\ttitle: r.title,\n\t\t\t\t\tcontent: r.content,\n\t\t\t\t\tcreated_at: r.created_at,\n\t\t\t\t\trrf_score: r.rrf_score,\n\t\t\t\t\tsemantic_rank: r.semantic_rank,\n\t\t\t\t\tkeyword_rank: r.keyword_rank,\n\t\t\t\t\tcitation: r.page_number\n\t\t\t\t\t\t? {\n\t\t\t\t\t\t\tpageNumber: r.page_number,\n\t\t\t\t\t\t\timageUrl: r.image_url || \"\", // Use Supabase URL directly from chapter_pages\n\t\t\t\t\t\t\tboundingBox: boundingBox || [0, 0, 1, 1],\n\t\t\t\t\t\t\tchunkContent: r.content || \"\",\n\t\t\t\t\t\t\ttitle: r.title,\n\t\t\t\t\t\t}\n\t\t\t\t\t\t: undefined,\n\t\t\t\t};\n\t\t\t});\n\n\t\t\treturn {\n\t\t\t\tresults: mappedResults,\n\t\t\t\tsearchMethod,\n\t\t\t\tstats: {\n\t\t\t\t\ttsvectorResults: keywordCount,\n\t\t\t\t\tsemanticResults: semanticCount,\n\t\t\t\t\tfinalResults: results.length,\n\t\t\t\t},\n\t\t\t};\n\t\t} catch (error) {\n\t\t\tconsole.error(\"[HYBRID RRF] Error:\", error);\n\t\t\treturn {\n\t\t\t\tresults: [],\n\t\t\t\tsearchMethod: \"keyword_only\",\n\t\t\t\tstats: { tsvectorResults: 0, semanticResults: 0, finalResults: 0 },\n\t\t\t};\n\t\t}\n\t}\n}\n","import prisma from \"@/lib/prisma\";\n\n// Minimal settings store using raw SQL to avoid Prisma schema changes.\n// It lazily creates the table `app_settings` if it does not exist, concurrency-safe.\n\nlet ensureOncePromise: Promise<void> | null = null;\nasync function ensureTableOnce(): Promise<void> {\n  if (ensureOncePromise) return ensureOncePromise;\n  ensureOncePromise = (async () => {\n    try {\n      // Serialize DDL using a Postgres advisory lock to avoid concurrent DDL conflicts\n      await prisma.$executeRawUnsafe(\n        `SELECT pg_advisory_lock(hashtext('cid_ai_app_settings_ddl_lock'));`\n      );\n      try {\n        await prisma.$executeRawUnsafe(\n          `CREATE TABLE IF NOT EXISTS app_settings (\n            key TEXT PRIMARY KEY,\n            value TEXT NOT NULL,\n            created_at TIMESTAMPTZ DEFAULT NOW(),\n            updated_at TIMESTAMPTZ DEFAULT NOW()\n          );`\n        );\n        await prisma.$executeRawUnsafe(\n          `CREATE OR REPLACE FUNCTION set_updated_at()\n           RETURNS TRIGGER AS $$\n           BEGIN\n             NEW.updated_at = NOW();\n             RETURN NEW;\n           END;\n           $$ LANGUAGE plpgsql;`\n        );\n        await prisma.$executeRawUnsafe(\n          `DO $$\n           BEGIN\n             IF NOT EXISTS (\n               SELECT 1 FROM pg_trigger WHERE tgname = 'app_settings_set_updated_at'\n             ) THEN\n               CREATE TRIGGER app_settings_set_updated_at\n               BEFORE UPDATE ON app_settings\n               FOR EACH ROW EXECUTE FUNCTION set_updated_at();\n             END IF;\n           END $$;`\n        );\n      } finally {\n        await prisma.$executeRawUnsafe(\n          `SELECT pg_advisory_unlock(hashtext('cid_ai_app_settings_ddl_lock'));`\n        );\n      }\n    } catch (e) {\n      // Do not block; log and continue. If permissions disallow DDL, reads will just 404.\n      console.warn(\"[APP-SETTINGS] ensureTable failed (non-fatal)\", e);\n    }\n  })();\n  return ensureOncePromise;\n}\n\nexport async function getSetting(key: string): Promise<string | null> {\n  try {\n    await ensureTableOnce();\n    const rows: Array<{ value: string }> = await prisma.$queryRawUnsafe(\n      `SELECT value FROM app_settings WHERE key = $1 LIMIT 1`,\n      key\n    );\n    if (rows && rows.length > 0) return rows[0].value;\n    return null;\n  } catch (e) {\n    console.warn(\"[APP-SETTINGS] getSetting failed\", e);\n    return null;\n  }\n}\n\nexport async function setSetting(key: string, value: string): Promise<void> {\n  await ensureTableOnce();\n  await prisma.$executeRawUnsafe(\n    `INSERT INTO app_settings (key, value)\n     VALUES ($1, $2)\n     ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value`,\n    key,\n    value\n  );\n}\n\nexport async function getSettingInt(key: string, defaultValue: number): Promise<number> {\n  const v = await getSetting(key);\n  if (v == null) return defaultValue;\n  const n = Number(v);\n  if (!Number.isFinite(n) || n <= 0) return defaultValue;\n  return Math.floor(n);\n}\n\nexport async function setSettingInt(key: string, value: number): Promise<void> {\n  if (!Number.isFinite(value) || value <= 0) {\n    throw new Error(\"Value must be a positive integer\");\n  }\n  await setSetting(key, String(Math.floor(value)));\n}\n","import { SearchResult, ChatMessage } from \"./ai-service-enhanced\";\nimport { generateAIResponse } from \"./ai-service-enhanced\";\n\n// Server-safe implementation - no browser globals\n\n// Constants for chunked processing\nconst CHUNK_SIZE = 15; // Reduced from 20 for better performance\nconst MAX_CONCURRENT_REQUESTS = 3; // Reduced from 5 to avoid rate limits\nconst LARGE_CONTEXT_THRESHOLD = 8000; // Warn if final context exceeds this (in characters)\n\n/**\n * Result from processing a single chunk\n */\ninterface ChunkResult {\n\ttext: string;\n\tinputTokens: number;\n\toutputTokens: number;\n\terror: boolean;\n\terrorMessage?: string;\n}\n\n/**\n * Generate smart extraction prompt based on query type\n */\nfunction generateExtractionPrompt(question: string): string {\n\tconst lowerQuestion = question.toLowerCase();\n\n\t// Count/statistical queries - need only IDs and categories\n\tif (lowerQuestion.match(/\\b(count|how many|number of|total)\\b/)) {\n\t\treturn `From these records, extract ONLY: Case ID and Category.\nFormat each as: \"ID: [id], Category: [category]\"\nOne line per record. No additional text or narrative.`;\n\t}\n\n\t// List queries - need structured data\n\tif (lowerQuestion.match(/\\b(list|show all|give me all|display)\\b/)) {\n\t\treturn `From these records, extract ONLY: ID, Title, Date.\nFormat as: \"ID: [id] | Title: [title] | Date: [date]\"\nOne line per record. Be concise. No narrative.`;\n\t}\n\n\t// Age-related queries - need names and ages\n\tif (lowerQuestion.match(/\\b(age|years old|victim.*age|suspect.*age)\\b/)) {\n\t\treturn `From these records, extract ONLY names and ages of victims/suspects.\nFormat as: \"Name (Age: X)\" or \"Name (Age: Unknown)\" if not found.\nOne per line. Omit entries without names. No other text.`;\n\t}\n\n\t// Victim and suspect queries\n\tif (lowerQuestion.match(/\\b(victim|suspect)\\b.*\\b(name|who|person)/)) {\n\t\treturn `From these records, extract ONLY victim and suspect information.\nFormat as:\n- Victim: [name], Age: [age]\n- Suspect: [name], Age: [age]\nOne set per record. Mark as \"Unknown\" if not found. Keep it concise.`;\n\t}\n\n\t// Location/place queries\n\tif (lowerQuestion.match(/\\b(location|place|where|address)\\b/)) {\n\t\treturn `From these records, extract ONLY locations/places mentioned.\nFormat as: \"Case ID: [id] | Location: [location]\"\nOne line per record. Be specific. No narrative.`;\n\t}\n\n\t// Summary/pattern/trend queries - need key data points\n\tif (lowerQuestion.match(/\\b(summar|pattern|trend|distribution|analysis)\\b/)) {\n\t\treturn `From these records, extract key data points relevant to: \"${question}\"\nFormat as concise bullet points. Include:\n- Case IDs\n- Relevant categories/attributes\n- Key numbers or facts\nRaw data only - analysis will come later. Be concise.`;\n\t}\n\n\t// Group by queries\n\tif (\n\t\tlowerQuestion.match(\n\t\t\t/\\b(group by|grouped by|organize by|by category)\\b/\n\t\t)\n\t) {\n\t\treturn `From these records, extract: ID, relevant grouping field (category), and title.\nFormat as: \"[grouping]: ID [id] - [title]\"\nOne line per record. Keep it structured and concise.`;\n\t}\n\n\t// Default - generic extraction with emphasis on conciseness\n\treturn `From these records, extract information relevant to: \"${question}\"\nFormat as structured bullet points. Include case IDs for reference.\nBe concise - extract key data only. Analysis comes later.\nKeep responses short - aim for 2-3 lines per record max.`;\n}\n\n/**\n * Progress callback type\n */\ntype ProgressCallback = (message: string) => void;\n\n/**\n * Process chunks with concurrency limiting and token tracking\n */\nasync function processChunksWithConcurrency(\n\tchunks: SearchResult[][],\n\textractionPrompt: string,\n\tconversationHistory: ChatMessage[],\n\tonProgress?: ProgressCallback\n): Promise<ChunkResult[]> {\n\tconst results: ChunkResult[] = [];\n\n\t// Process chunks in batches to limit concurrency\n\tfor (let i = 0; i < chunks.length; i += MAX_CONCURRENT_REQUESTS) {\n\t\tconst batch = chunks.slice(i, i + MAX_CONCURRENT_REQUESTS);\n\t\tconst batchPromises = batch.map((chunk, batchIndex) =>\n\t\t\tprocessChunk(\n\t\t\t\tchunk,\n\t\t\t\ti + batchIndex,\n\t\t\t\tchunks.length,\n\t\t\t\textractionPrompt,\n\t\t\t\tconversationHistory\n\t\t\t)\n\t\t);\n\n\t\tconst batchNum = Math.floor(i / MAX_CONCURRENT_REQUESTS) + 1;\n\t\tconst totalBatches = Math.ceil(chunks.length / MAX_CONCURRENT_REQUESTS);\n\n\t\tconsole.log(\n\t\t\t`[CHUNKED-PROCESSING] Processing batch ${batchNum}/${totalBatches} with ${batch.length} chunks`\n\t\t);\n\n\t\t// Report progress\n\t\tif (onProgress) {\n\t\t\tconst processedSoFar = i;\n\t\t\tconst totalChunks = chunks.length;\n\t\t\tonProgress(\n\t\t\t\t`Processing chunks ${processedSoFar + 1}-${Math.min(\n\t\t\t\t\tprocessedSoFar + batch.length,\n\t\t\t\t\ttotalChunks\n\t\t\t\t)} of ${totalChunks}...`\n\t\t\t);\n\t\t}\n\n\t\tconst batchResults = await Promise.all(batchPromises);\n\t\tresults.push(...batchResults);\n\t}\n\n\treturn results;\n}\n\n/**\n * Prepare context for a chunk of records\n */\nfunction prepareContextForChunk(records: SearchResult[]): string {\n\tif (records.length === 0) {\n\t\treturn \"No records in this chunk.\";\n\t}\n\n\t// Create a structured context for this chunk\n\tlet context = `CHUNK CONTEXT (${records.length} records):\\n\\n`;\n\n\t// Add record index for quick reference\n\tcontext += \"RECORD INDEX:\\n\";\n\trecords.forEach((record, index) => {\n\t\tcontext += `[${index + 1}] Title: ${record.title} | Subject: ${\n\t\t\trecord.subject || \"Uncategorized\"\n\t\t} | Date: ${\n\t\t\trecord.entry_date_real?.toLocaleDateString() || \"Unknown date\"\n\t\t}\\n`;\n\t});\n\n\t// Add full record details\n\tcontext += \"\\nDETAILED RECORDS:\\n\";\n\trecords.forEach((record, index) => {\n\t\tconst content = record.note || \"No content available\";\n\n\t\tcontext += `\\n[RECORD ${index + 1}]`;\n\t\tcontext += `\\nTitle: ${record.title}`;\n\t\tcontext += `\\nSubject: ${record.subject || \"Uncategorized\"}`;\n\t\tcontext += `\\nDate: ${\n\t\t\trecord.entry_date_real?.toLocaleDateString() || \"Unknown date\"\n\t\t}`;\n\t\tcontext += `\\nContent: ${content}`;\n\t\tcontext += \"\\n---\\n\";\n\t});\n\n\treturn context;\n}\n\n/**\n * Process a single chunk of records with proper error handling and token tracking\n */\nasync function processChunk(\n\tchunk: SearchResult[],\n\tindex: number,\n\ttotalChunks: number,\n\textractionPrompt: string,\n\tconversationHistory: ChatMessage[]\n): Promise<ChunkResult> {\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] Starting chunk ${index + 1}/${totalChunks} with ${\n\t\t\tchunk.length\n\t\t} records`\n\t);\n\tconst startTime = Date.now();\n\n\ttry {\n\t\tconst chunkContext = prepareContextForChunk(chunk);\n\n\t\t// FIX #4: Pass conversation history to chunks for context awareness\n\t\tconst chunkResponse = await generateAIResponse(\n\t\t\textractionPrompt,\n\t\t\tchunkContext,\n\t\t\tconversationHistory, // ✅ Now includes conversation history\n\t\t\t\"analytical_query\"\n\t\t);\n\n\t\tconst elapsed = Date.now() - startTime;\n\t\tconsole.log(\n\t\t\t`[CHUNKED-PROCESSING] Completed chunk ${\n\t\t\t\tindex + 1\n\t\t\t}/${totalChunks} in ${elapsed}ms (tokens: ${\n\t\t\t\tchunkResponse.inputTokens\n\t\t\t} in, ${chunkResponse.outputTokens} out)`\n\t\t);\n\n\t\t// FIX #1 & #2: Return proper result with tokens and error status\n\t\treturn {\n\t\t\ttext: chunkResponse.text,\n\t\t\tinputTokens: chunkResponse.inputTokens || 0,\n\t\t\toutputTokens: chunkResponse.outputTokens || 0,\n\t\t\terror: false,\n\t\t};\n\t} catch (error) {\n\t\tconst errorMessage =\n\t\t\terror instanceof Error ? error.message : \"Unknown error\";\n\t\tconsole.error(\n\t\t\t`[CHUNKED-PROCESSING] ❌ Error processing chunk ${\n\t\t\t\tindex + 1\n\t\t\t}/${totalChunks}:`,\n\t\t\terrorMessage\n\t\t);\n\n\t\t// FIX #2: Return empty text and mark as error (don't pollute data with error messages!)\n\t\treturn {\n\t\t\ttext: \"\",\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t\terror: true,\n\t\t\terrorMessage,\n\t\t};\n\t}\n}\n\n/**\n * Process records in parallel chunks for analytical queries\n * FIX #4: Added conversationHistory parameter\n */\nexport async function processChunkedAnalyticalQuery(\n\tquestion: string,\n\trecords: SearchResult[],\n\tconversationHistory: ChatMessage[] = [],\n\tonProgress?: ProgressCallback\n): Promise<{ text: string; inputTokens: number; outputTokens: number }> {\n\tconst chunks = [];\n\tconst recordCount = records.length;\n\n\t// Divide records into chunks\n\tfor (let i = 0; i < recordCount; i += CHUNK_SIZE) {\n\t\tchunks.push(records.slice(i, i + CHUNK_SIZE));\n\t}\n\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] Processing ${recordCount} records in ${chunks.length} chunks with max ${MAX_CONCURRENT_REQUESTS} concurrent requests`\n\t);\n\n\t// Report initial progress\n\tif (onProgress) {\n\t\tonProgress(\n\t\t\t`Processing ${recordCount} records in ${chunks.length} chunks...`\n\t\t);\n\t}\n\n\t// FIX #3: Generate smart extraction prompt based on query type\n\tconst extractionPrompt = generateExtractionPrompt(question);\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] Using extraction strategy: ${extractionPrompt\n\t\t\t.split(\"\\n\")[0]\n\t\t\t.substring(0, 80)}...`\n\t);\n\n\t// FIX #1: Track tokens from ALL chunks\n\tlet totalInputTokens = 0;\n\tlet totalOutputTokens = 0;\n\n\t// Process chunks with concurrency limiting\n\tconst chunkStartTime = Date.now();\n\tconst chunkResults = await processChunksWithConcurrency(\n\t\tchunks,\n\t\textractionPrompt,\n\t\tconversationHistory,\n\t\tonProgress\n\t);\n\tconst chunkProcessingTime = Date.now() - chunkStartTime;\n\n\t// FIX #1: Accumulate tokens from all chunks\n\tchunkResults.forEach((result) => {\n\t\ttotalInputTokens += result.inputTokens;\n\t\ttotalOutputTokens += result.outputTokens;\n\t});\n\n\t// FIX #2: Check for failed chunks and handle gracefully\n\tconst failedChunks = chunkResults.filter((r) => r.error);\n\tconst successfulChunks = chunkResults.filter((r) => !r.error);\n\n\tif (failedChunks.length > 0) {\n\t\tconsole.warn(\n\t\t\t`[CHUNKED-PROCESSING] ⚠️ ${failedChunks.length}/${chunks.length} chunks failed. Results may be incomplete.`\n\t\t);\n\t\tfailedChunks.forEach((chunk, idx) => {\n\t\t\tconsole.warn(\n\t\t\t\t`[CHUNKED-PROCESSING] Failed chunk ${idx + 1}: ${chunk.errorMessage}`\n\t\t\t);\n\t\t});\n\t}\n\n\t// Extract only successful chunk data\n\tconst extractedData = successfulChunks.map((r) => r.text);\n\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] All chunks processed in ${chunkProcessingTime}ms (${successfulChunks.length}/${chunks.length} successful). Total tokens from chunks: ${totalInputTokens} in, ${totalOutputTokens} out`\n\t);\n\n\t// FIX #2 & #5: Build final context with warnings and monitoring\n\tlet finalContext = `Here is data extracted from ${successfulChunks.length}/${chunks.length} chunks (${recordCount} total records):`;\n\n\t// Add warning if some chunks failed\n\tif (failedChunks.length > 0) {\n\t\tfinalContext += `\\n\\n⚠️ WARNING: ${failedChunks.length} chunk(s) failed to process due to errors. The analysis below is based on ${successfulChunks.length} successful chunks and may be incomplete.\\n`;\n\t}\n\n\tfinalContext += `\\n\\n${extractedData.join(\"\\n\\n=== NEXT CHUNK ===\\n\\n\")}`;\n\n\tfinalContext += `\\n\\nBased on all the extracted information above, provide a complete, organized answer to the user's question: \"${question}\"`;\n\n\t// If chunks failed, remind AI to acknowledge incomplete data\n\tif (failedChunks.length > 0) {\n\t\tfinalContext += `\\n\\nNote: Some data chunks failed to process. Please acknowledge in your response that the results may be incomplete (e.g., \"Based on available data...\" or \"From the processed records...\").`;\n\t}\n\n\t// FIX #5: Monitor final context size and log warning\n\tconst finalContextSize = finalContext.length;\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] Final context size: ${finalContextSize} characters (~${Math.round(\n\t\t\tfinalContextSize / 4\n\t\t)} tokens)`\n\t);\n\n\tif (finalContextSize > LARGE_CONTEXT_THRESHOLD) {\n\t\tconsole.warn(\n\t\t\t`[CHUNKED-PROCESSING] ⚠️ Large final context detected (${finalContextSize} chars). This may slow down synthesis or cause timeouts. Consider refining extraction prompts for more concise output.`\n\t\t);\n\t}\n\n\t// Report synthesis progress\n\tif (onProgress) {\n\t\tonProgress(\n\t\t\t`Synthesizing final response from ${successfulChunks.length} chunks...`\n\t\t);\n\t}\n\n\t// Final synthesis\n\tconst synthesisStartTime = Date.now();\n\tconst finalResponse = await generateAIResponse(\n\t\tquestion,\n\t\tfinalContext,\n\t\tconversationHistory, // FIX #4: Include conversation history in final synthesis\n\t\t\"analytical_query\"\n\t);\n\tconst synthesisTime = Date.now() - synthesisStartTime;\n\n\t// FIX #1: Add synthesis tokens to total\n\ttotalInputTokens += finalResponse.inputTokens || 0;\n\ttotalOutputTokens += finalResponse.outputTokens || 0;\n\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] Final synthesis completed in ${synthesisTime}ms (tokens: ${finalResponse.inputTokens} in, ${finalResponse.outputTokens} out)`\n\t);\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] ✅ Total processing time: ${\n\t\t\tchunkProcessingTime + synthesisTime\n\t\t}ms`\n\t);\n\tconsole.log(\n\t\t`[CHUNKED-PROCESSING] ✅ Total token usage: ${totalInputTokens} input + ${totalOutputTokens} output = ${\n\t\t\ttotalInputTokens + totalOutputTokens\n\t\t} tokens`\n\t);\n\n\t// FIX #1: Return accurate total tokens\n\treturn {\n\t\ttext: finalResponse.text,\n\t\tinputTokens: totalInputTokens,\n\t\toutputTokens: totalOutputTokens,\n\t};\n}\n\n/**\n * Determine if a query is analytical and requires chunked processing\n */\nexport function isAnalyticalQuery(\n\tquestion: string,\n\tqueryType: string\n): boolean {\n\t// Check query type\n\tif (queryType === \"analytical_query\") return true;\n\n\t// Check for analytical keywords\n\tconst analyticalPatterns = [\n\t\t/\\ball\\b.*\\b(victim|suspect|witness|location|place|date|time|age|name)/i,\n\t\t/\\blist\\b.*\\b(all|every|each)\\b/i,\n\t\t/\\bsummarize\\b|\\bsummary\\b/i,\n\t\t/\\bcount\\b|\\btotal\\b|\\bnumber of\\b/i,\n\t\t/\\baverage\\b|\\bmean\\b|\\bmedian\\b/i,\n\t\t/\\btrend\\b|\\bpattern\\b/i,\n\t\t/\\bcompare\\b|\\bcomparison\\b/i,\n\t\t/\\brelationship\\b|\\bcorrelation\\b/i,\n\t];\n\n\treturn analyticalPatterns.some((pattern) => pattern.test(question));\n}\n","import { z } from \"zod\";\n\nexport const ChartSchema = z.object({\n    title: z.string().describe(\"A short, descriptive title for the chart\"),\n    type: z.enum([\"bar\", \"line\", \"pie\", \"area\"]).describe(\"The type of chart to render\"),\n    description: z.string().describe(\"A 1-sentence insight about this data\"),\n    xAxisKey: z.string().describe(\"The JSON key to use for the X-axis (e.g., 'month', 'category')\"),\n    seriesKeys: z.array(z.string()).describe(\"List of JSON keys to plot as data series (e.g., ['revenue', 'cost'])\"),\n    data: z.union([z.string(), z.array(z.record(z.any()))]).describe(\"The raw data points as a JSON string or array of objects\"),\n});\n\nexport type ChartConfig = z.infer<typeof ChartSchema>;\n","import { prisma } from \"@/lib/prisma\";\nimport {\n\tgetGeminiClient,\n\trecordKeyUsage,\n\tgetActiveModelNames,\n\tgetProviderApiKey,\n} from \"@/lib/ai-key-store\";\nimport { HybridSearchService } from \"./hybrid-search\";\nimport { getSettingInt } from \"@/lib/app-settings\";\nimport { processChunkedAnalyticalQuery } from \"./chunked-processing\";\nimport { ChartSchema } from \"@/lib/chart-schema\";\nimport { generateObject } from \"ai\";\nimport { createGoogleGenerativeAI } from \"@ai-sdk/google\";\nimport { z } from \"zod\";\nimport { QuestionType } from \"@/generated/prisma\";\n\n// Developer logging toggle - set to true to see query logs in console\nconst DEV_LOGGING = true;\n\n// Configuration for relevance extraction\n// This feature extracts only relevant information from records to reduce token usage\n// while maintaining answer quality. It's particularly useful for large datasets.\nconst RELEVANCE_EXTRACTION_CONFIG = {\n\tenabled: false, // Set to false to disable relevance extraction completely\n\tthreshold: 50, // Enable for queries with more than this many records\n\tdebug: true, // Set to true to see detailed extraction logs\n};\n\n// AI API timeout configuration (in milliseconds)\nconst AI_API_TIMEOUT = 90000; // 90 seconds\n\n/**\n * RELEVANCE EXTRACTION FEATURE\n *\n * This feature intelligently extracts only relevant information from database records\n * to reduce token usage while maintaining answer quality. It works by:\n *\n * 1. Analyzing the user's query for keywords\n * 2. Scoring each record's relevance based on query matches\n * 3. For highly relevant records: keeping full content\n * 4. For less relevant records: extracting only key sentences\n *\n * Configuration:\n * - enabled: Set to false to disable completely\n * - threshold: Number of records above which extraction is enabled\n * - debug: Set to true for detailed logging\n *\n * To disable: Set RELEVANCE_EXTRACTION_CONFIG.enabled = false\n * To adjust threshold: Change RELEVANCE_EXTRACTION_CONFIG.threshold\n */\n\n// Performance timing helper function\nfunction timeStart(label: string) {\n\tconst uniqueLabel = `${label}_${Date.now()}_${Math.random()\n\t\t.toString(36)\n\t\t.substr(2, 9)}`;\n\tconsole.time(`[TIMING] ${uniqueLabel}`);\n\tconsole.log(`[TIMING-START] ${label}`);\n\treturn { startTime: Date.now(), uniqueLabel };\n}\n\nfunction timeEnd(\n\tlabel: string,\n\ttimingData: { startTime: number; uniqueLabel: string }\n) {\n\tconst elapsed = Date.now() - timingData.startTime;\n\tconsole.timeEnd(`[TIMING] ${timingData.uniqueLabel}`);\n\tconsole.log(`[TIMING-END] ${label} took ${elapsed}ms`);\n\treturn elapsed;\n}\n\n/**\n * Wrap an AI API call with a timeout to prevent hanging requests\n * @param promise The AI API call promise\n * @param timeoutMs Timeout in milliseconds (default: 60s)\n * @param operation Description of the operation (for error messages)\n * @returns The result of the promise if it completes before timeout\n * @throws Error if the promise times out\n */\nasync function withTimeout<T>(\n\tpromise: Promise<T>,\n\ttimeoutMs: number = AI_API_TIMEOUT,\n\toperation: string = \"AI API call\"\n): Promise<T> {\n\tconst timeoutPromise = new Promise<never>((_, reject) => {\n\t\tsetTimeout(() => {\n\t\t\treject(\n\t\t\t\tnew Error(`${operation} timed out after ${timeoutMs / 1000} seconds`)\n\t\t\t);\n\t\t}, timeoutMs);\n\t});\n\n\treturn Promise.race([promise, timeoutPromise]);\n}\n\n/**\n * Developer logging helper function\n */\nfunction devLog(message: string, data?: any) {\n\tif (DEV_LOGGING) {\n\t\tconsole.log(`[AI-SERVICE-DEV] ${message}`);\n\t\tif (data !== undefined) {\n\t\t\tconsole.log(data);\n\t\t}\n\t\tconsole.log(\"---\");\n\t}\n}\n\n// Gemini client is now constructed per-call via getGeminiClient() to support\n// admin-managed key rotation and dynamic provider/model selection.\n\nexport interface ChatMessage {\n\tid: string;\n\trole: \"user\" | \"assistant\";\n\tcontent: string;\n\toriginalContent?: string; // For translation toggle\n\ttimestamp: Date;\n\tsources?: Array<{\n\t\tid: number;\n\t\ttitle: string;\n\t\trelevance?: number;\n\t\tsimilarity?: number; // For UI display (converted from RRF score or semantic_similarity)\n\t\tcitation?: {\n\t\t\tpageNumber: number;\n\t\t\timageUrl: string;\n\t\t\tboundingBox: any;\n\t\t};\n\t}>;\n\ttokenCount?: {\n\t\tinput: number;\n\t\toutput: number;\n\t};\n\tfilters?: {\n\t\tboardId?: string;\n\t\tsubjectId?: number;\n\t\tchapterId?: number;\n\t\tcategory?: string;\n\t};\n\tchartData?: {\n\t\ttitle: string;\n\t\ttype: \"bar\" | \"line\" | \"pie\" | \"area\";\n\t\tdescription: string;\n\t\txAxisKey: string;\n\t\tseriesKeys: string[];\n\t\tdata: Array<Record<string, string | number>>;\n\t};\n\tsuggestedResponses?: string[];\n}\n\nexport interface SearchResult {\n\tid: string | number;\n\tsubject?: string; // Subject name (mapped from search results)\n\ttitle: string;\n\tnote: string | null;\n\tentry_date_real: Date | null;\n\n\t// NEW FIELDS from RRF\n\trrf_score?: number; // The main score now (approx 0.00 ~ 0.033)\n\tsemantic_rank?: number; // Where it ranked semantically\n\tkeyword_rank?: number; // Where it ranked by keyword\n\n\t// OLD FIELDS (Keep optional for backward compatibility)\n\trank?: number;\n\tts_rank?: number;\n\tsemantic_similarity?: number;\n\tcombined_score?: number;\n\n\t// Citation data for split-screen view\n\tcitation?: {\n\t\tpageNumber: number;\n\t\timageUrl: string;\n\t\tboundingBox: any;\n\t};\n\n\t// Internal flag\n\t_processed?: boolean;\n}\n\n/**\n * Quick pattern-based query classification (no AI needed for simple queries)\n * Returns null if AI analysis is needed\n */\nfunction quickClassifyQuery(query: string): {\n\tcoreSearchTerms: string;\n\tsemanticConcepts?: string; // Optional for backward compatibility\n\tinstructionalTerms: string;\n\tqueryType:\n\t| \"specific_search\"\n\t| \"follow_up\"\n\t| \"elaboration\"\n\t| \"general\"\n\t| \"recent_files\"\n\t| \"analytical_query\"\n\t| \"list_all\"\n\t| \"visualization\";\n\tcontextNeeded: boolean;\n\tinputTokens: number;\n\toutputTokens: number;\n} | null {\n\tconst q = query.toLowerCase().trim();\n\n\t// Pattern 1: Simple question words (who, what, where, when, why, how)\n\tif (/^(who|what|where|when|why|how)\\s+/i.test(q)) {\n\t\treturn {\n\t\t\tcoreSearchTerms: query,\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"specific_search\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 2: List all / Show all queries\n\tif (/^(show|list|display|get|find|give me)\\s+(all|every|the)/i.test(q)) {\n\t\treturn {\n\t\t\tcoreSearchTerms: \"\",\n\t\t\tinstructionalTerms: \"list all\",\n\t\t\tqueryType: \"list_all\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 3: Analytical queries (summarize, count, total, average, how many)\n\tif (\n\t\t/^(summarize|count|total|average|how many|tell me about|analyze|sum up)/i.test(\n\t\t\tq\n\t\t)\n\t) {\n\t\treturn {\n\t\t\tcoreSearchTerms: query,\n\t\t\tinstructionalTerms: \"analyze\",\n\t\t\tqueryType: \"analytical_query\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 3.5: Visualization queries (chart, graph, plot)\n\tif (\n\t\t/^(create|make|show|generate|plot)\\s+(a\\s+)?(chart|graph|plot|visualization)/i.test(\n\t\t\tq\n\t\t) ||\n\t\t/\\b(bar chart|line chart|pie chart|scatter plot|histogram)\\b/i.test(q)\n\t) {\n\t\treturn {\n\t\t\tcoreSearchTerms: query,\n\t\t\tinstructionalTerms: \"visualize\",\n\t\t\tqueryType: \"visualization\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 4: Recent/Latest queries\n\tif (\n\t\t/\\b(recent|latest|newest|last|most recent)\\s+(files?|records?|cases?|entries?)\\b/i.test(\n\t\t\tq\n\t\t)\n\t) {\n\t\treturn {\n\t\t\tcoreSearchTerms: query,\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"recent_files\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 6: Simple search queries (find, search for, look for)\n\tif (/^(find|search|look for|get me)\\s+/i.test(q)) {\n\t\tconst searchTerms = query.replace(/^(find|search|look for|get me)\\s+/i, \"\");\n\t\treturn {\n\t\t\tcoreSearchTerms: searchTerms,\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"specific_search\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 7: Follow-up questions (short queries, pronouns)\n\tif (\n\t\tq.length < 20 &&\n\t\t(/\\b(he|she|they|it|him|her|them|this|that|these|those)\\b/i.test(q) ||\n\t\t\t/(more|details?|tell me more|elaborate|explain)/i.test(q))\n\t) {\n\t\treturn {\n\t\t\tcoreSearchTerms: query,\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"follow_up\",\n\t\t\tcontextNeeded: true,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// Pattern 8: Greetings / General queries\n\tif (\n\t\t/^(hi|hello|hey|greetings|good morning|good afternoon|good evening)/i.test(\n\t\t\tq\n\t\t) ||\n\t\t/^(help|what can you do|how does this work)/i.test(q)\n\t) {\n\t\treturn {\n\t\t\tcoreSearchTerms: \"\",\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"general\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n\n\t// No pattern matched - need AI analysis\n\treturn null;\n}\n\n/**\n * Analyze user query and extract search keywords using AI\n */\nexport async function analyzeQueryForSearch(\n\tcurrentQuery: string,\n\tconversationHistory: ChatMessage[] = [],\n\topts: { provider?: \"gemini\"; model?: string; keyId?: number } = {}\n): Promise<{\n\tcoreSearchTerms: string;\n\tsemanticConcepts?: string; // Optional for backward compatibility\n\tinstructionalTerms: string;\n\tqueryType:\n\t| \"specific_search\"\n\t| \"follow_up\"\n\t| \"elaboration\"\n\t| \"general\"\n\t| \"recent_files\"\n\t| \"analytical_query\"\n\t| \"list_all\"\n\t| \"visualization\";\n\tcontextNeeded: boolean;\n\tinputTokens: number;\n\toutputTokens: number;\n}> {\n\ttry {\n\t\t// Try quick pattern-based classification first (90% of queries)\n\t\tconst quickResult = quickClassifyQuery(currentQuery);\n\t\tif (quickResult) {\n\t\t\tconsole.log(\n\t\t\t\t`[QUERY ANALYSIS] Quick pattern match: ${quickResult.queryType} (skipped AI analysis)`\n\t\t\t);\n\t\t\treturn quickResult;\n\t\t}\n\n\t\t// Pattern matching failed - use AI analysis for complex queries\n\t\tconsole.log(\"[QUERY ANALYSIS] Using AI analysis for complex query\");\n\n\t\tconst { client, keyId } = await getGeminiClient({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\tconst attemptModels = Array.from(\n\t\t\tnew Set([opts.model, ...dbModels].filter(Boolean))\n\t\t);\n\t\t// Fallback to .env or hardcoded default if no models are available\n\t\tif (attemptModels.length === 0) {\n\t\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\t\tattemptModels.push(fallbackModel);\n\t\t}\n\n\t\t// Build conversation context\n\t\tconst recentHistory = conversationHistory.slice(-6); // Last 3 exchanges\n\t\tconst historyContext =\n\t\t\trecentHistory.length > 0\n\t\t\t\t? `\\nRECENT CONVERSATION:\\n${recentHistory\n\t\t\t\t\t.map((msg) => `${msg.role.toUpperCase()}: ${msg.content}`)\n\t\t\t\t\t.join(\"\\n\")}\\n`\n\t\t\t\t: \"\";\n\n\t\tconst prompt = `You are the Query Processor for a high-precision RAG system.\nYour goal is to break down the user's request into database search parameters.\n\nCURRENT USER QUERY: \"${currentQuery}\"\n${historyContext}\n\nAnalyze the query and output the following JSON object:\n\n{\n  \"coreSearchTerms\": \"string\",   // KEYWORDS: Strict nouns, IDs, Names, Dates (for exact text match)\n  \"semanticConcepts\": \"string\",  // CONCEPTS: Descriptive phrasing, intent, or meaning (for vector search)\n  \"queryType\": \"string\",         // ENUM: specific_search, analytical_query, follow_up, elaboration, general, recent_files, list_all, visualization\n  \"instructionalTerms\": \"string\", // Verbs/Actions (e.g., \"summarize\", \"list\", \"compare\", \"chart\")\n  \"contextNeeded\": boolean       // True if the query relies on previous messages\n}\n\nGUIDELINES:\n\n1. coreSearchTerms: Extract ONLY exact identifiers.\n   - User: \"Invoice Q-881 payment status\" -> \"Invoice Q-881\"\n   - User: \"Project Alpha deliverables\" -> \"Project Alpha deliverables\"\n\n2. semanticConcepts: Extract the *meaning* or *topic*.\n   - User: \"Invoice Q-881 payment status\" -> \"payment status pending overdue\"\n   - User: \"Project Alpha deliverables\" -> \"project completion tasks milestones deliverables\"\n\n3. queryType:\n   - \"analytical_query\": If asking for trends, counts, averages, or summaries across multiple files.\n   - \"specific_search\": If looking for a specific fact, file, or person.\n   - \"follow_up\": Questions referring to previous results (\"Who caught her?\", \"What happened next?\").\n   - \"elaboration\": Requests for more details (\"Elaborate\", \"Tell me more\", \"Explain further\").\n   - \"general\": General questions or greetings.\n   - \"recent_files\": Queries asking for recent/latest/newest files (handled separately).\n   - \"recent_files\": Queries asking for recent/latest/newest files (handled separately).\n   - \"list_all\": Queries asking to list or show all records/files (e.g., \"list all records\", \"show all documents\").\n   - \"visualization\": Requests to create charts, graphs, or plots (e.g., \"Plot revenue over time\").\n\n4. IMPORTANT:\n   - If the query asks for \"recent\", \"latest\", \"newest\", or \"most recent\" files/records/cases, classify it as \"recent_files\".\n   - If the query asks to \"list all\", \"show all\", \"find all\", or similar requests to display all records, classify it as \"list_all\".\n   - For follow_up and elaboration queries, you MUST extract keywords from the conversation history.\n\nExamples:\n- \"Documents from 2007?\" → {\"coreSearchTerms\": \"documents 2007\", \"semanticConcepts\": \"documents records year 2007\", \"instructionalTerms\": \"\", \"queryType\": \"specific_search\", \"contextNeeded\": false}\n- \"What happened next?\" → {\"coreSearchTerms\": \"next follow up continuation\", \"semanticConcepts\": \"sequence progression continuation\", \"instructionalTerms\": \"\", \"queryType\": \"follow_up\", \"contextNeeded\": true}\n- \"Summarize the project on Alpha\" → {\"coreSearchTerms\": \"Alpha\", \"semanticConcepts\": \"project summary overview details\", \"instructionalTerms\": \"summarize project\", \"queryType\": \"analytical_query\", \"contextNeeded\": false}\n\nRespond ONLY with valid JSON.`;\n\n\t\tlet text: string = \"\";\n\t\tlet analysisInputTokens = 0;\n\t\tlet analysisOutputTokens = 0;\n\t\tlet lastError: any = null;\n\t\tfor (const modelName of attemptModels) {\n\t\t\ttry {\n\t\t\t\tconst model = client.getGenerativeModel({ model: modelName as string });\n\t\t\t\tconsole.log(\n\t\t\t\t\t`[AI] provider=gemini model=${modelName} keyId=${keyId ?? \"env-fallback\"\n\t\t\t\t\t}`\n\t\t\t\t);\n\t\t\t\t// Count input tokens using provider-native method\n\t\t\t\ttry {\n\t\t\t\t\tconst countRes: any = await model.countTokens({\n\t\t\t\t\t\tcontents: [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\trole: \"user\",\n\t\t\t\t\t\t\t\tparts: [{ text: prompt }],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t],\n\t\t\t\t\t});\n\t\t\t\t\tanalysisInputTokens = (countRes?.totalTokens ??\n\t\t\t\t\t\tcountRes?.totalTokenCount ??\n\t\t\t\t\t\t0) as number;\n\t\t\t\t} catch (e) {\n\t\t\t\t\tanalysisInputTokens = estimateTokenCount(prompt);\n\t\t\t\t\tconsole.warn(\n\t\t\t\t\t\t\"[QUERY ANALYSIS] countTokens failed; using heuristic\",\n\t\t\t\t\t\te\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t\tconst result = await withTimeout(\n\t\t\t\t\tmodel.generateContent(prompt),\n\t\t\t\t\tAI_API_TIMEOUT,\n\t\t\t\t\t\"Query analysis AI call\"\n\t\t\t\t);\n\t\t\t\tconst response = await result.response;\n\t\t\t\ttext = response.text().trim();\n\t\t\t\t// Capture output tokens from usage metadata if available\n\t\t\t\tconst usage: any = (response as any)?.usageMetadata;\n\t\t\t\tanalysisOutputTokens = (usage?.candidatesTokenCount ??\n\t\t\t\t\tusage?.totalTokenCount ??\n\t\t\t\t\testimateTokenCount(text)) as number;\n\t\t\t\tif (keyId) await recordKeyUsage(keyId, true);\n\t\t\t\tlastError = null;\n\t\t\t\tbreak;\n\t\t\t} catch (e: any) {\n\t\t\t\tlastError = e;\n\t\t\t\tif (keyId) await recordKeyUsage(keyId, false);\n\t\t\t\tconst errorMsg = e?.message || String(e);\n\t\t\t\tif (errorMsg.includes(\"timed out\")) {\n\t\t\t\t\tconsole.warn(\n\t\t\t\t\t\t`[AI] Query analysis timeout after ${AI_API_TIMEOUT / 1000\n\t\t\t\t\t\t}s with model: ${modelName}`\n\t\t\t\t\t);\n\t\t\t\t} else {\n\t\t\t\t\tconsole.warn(`[AI] model attempt failed: ${modelName}`, e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (lastError) throw lastError;\n\n\t\tconsole.log(\"[QUERY ANALYSIS] Raw AI response:\", text);\n\n\t\t// Extract JSON from response\n\t\tconst jsonMatch = text.match(/\\{[\\s\\S]*\\}/);\n\t\tif (!jsonMatch) {\n\t\t\tconsole.error(\"[QUERY ANALYSIS] No JSON found in response:\", text);\n\t\t\tthrow new Error(\"Invalid response format from AI\");\n\t\t}\n\n\t\tlet analysis;\n\t\ttry {\n\t\t\tanalysis = JSON.parse(jsonMatch[0]);\n\t\t\tconsole.log(\"[QUERY ANALYSIS] Parsed analysis:\", analysis);\n\t\t} catch (parseError) {\n\t\t\tconsole.error(\"[QUERY ANALYSIS] JSON parse error:\", parseError);\n\t\t\tconsole.error(\"[QUERY ANALYSIS] JSON string:\", jsonMatch[0]);\n\t\t\tthrow new Error(\"Invalid JSON format from AI\");\n\t\t}\n\n\t\t// Validate response\n\t\t// Allow empty coreSearchTerms for specific_search queries with instructional terms (like \"all documents\")\n\t\t// or for list_all or analytical queries\n\t\tconst hasValidTerms =\n\t\t\tanalysis.coreSearchTerms ||\n\t\t\t(analysis.queryType === \"specific_search\" &&\n\t\t\t\tanalysis.instructionalTerms) ||\n\t\t\tanalysis.queryType === \"list_all\" ||\n\t\t\tanalysis.queryType === \"analytical_query\";\n\n\t\tif (!hasValidTerms || !analysis.queryType) {\n\t\t\tconsole.error(\"[QUERY ANALYSIS] Incomplete analysis:\", analysis);\n\t\t\tthrow new Error(\"Incomplete analysis from AI\");\n\t\t}\n\n\t\treturn {\n\t\t\tcoreSearchTerms: analysis.coreSearchTerms,\n\t\t\tsemanticConcepts: analysis.semanticConcepts, // Extract semanticConcepts from JSON\n\t\t\tinstructionalTerms: analysis.instructionalTerms || \"\",\n\t\t\tqueryType: analysis.queryType,\n\t\t\tcontextNeeded: analysis.contextNeeded || false,\n\t\t\tinputTokens: analysisInputTokens,\n\t\t\toutputTokens: analysisOutputTokens,\n\t\t};\n\t} catch (error) {\n\t\tconsole.error(\"Query analysis error:\", error);\n\n\t\t// Fallback to simple keyword extraction\n\t\tconst fallbackKeywords = currentQuery\n\t\t\t.toLowerCase()\n\t\t\t.replace(/[^\\w\\s]/g, \" \")\n\t\t\t.split(\" \")\n\t\t\t.filter((word) => word.length > 2)\n\t\t\t.join(\" \");\n\n\t\treturn {\n\t\t\tcoreSearchTerms: fallbackKeywords || currentQuery,\n\t\t\tinstructionalTerms: \"\",\n\t\t\tqueryType: \"specific_search\",\n\t\t\tcontextNeeded: false,\n\t\t\tinputTokens: 0,\n\t\t\toutputTokens: 0,\n\t\t};\n\t}\n}\n\n/**\n * Get recent chapters sorted by date\n * Updated to use chapters table instead of file_list\n */\nexport async function getRecentFiles(\n\tlimit: number = 10\n): Promise<SearchResult[]> {\n\ttry {\n\t\tconsole.log(`[RECENT CHAPTERS] Fetching ${limit} most recent chapters`);\n\n\t\tconst chapters = await prisma.chapter.findMany({\n\t\t\tselect: {\n\t\t\t\tid: true,\n\t\t\t\ttitle: true,\n\t\t\t\tcreated_at: true,\n\t\t\t\tsubject: {\n\t\t\t\t\tselect: {\n\t\t\t\t\t\tname: true,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twhere: {\n\t\t\t\tis_active: true,\n\t\t\t\tprocessing_status: \"COMPLETED\",\n\t\t\t},\n\t\t\torderBy: {\n\t\t\t\tcreated_at: \"desc\",\n\t\t\t},\n\t\t\ttake: limit,\n\t\t});\n\n\t\tconsole.log(`[RECENT CHAPTERS] Found ${chapters.length} recent chapters`);\n\n\t\treturn chapters.map((chapter) => ({\n\t\t\tid: chapter.id.toString(),\n\t\t\tsubject: chapter.subject.name,\n\t\t\ttitle: chapter.title,\n\t\t\tnote: \"\", // Chapters don't have a note field, content is in chunks\n\t\t\tentry_date_real: chapter.created_at,\n\t\t\trank: 1.0, // All recent chapters have equal relevance\n\t\t}));\n\t} catch (error) {\n\t\tconsole.error(\"Recent chapters query error:\", error);\n\t\tthrow new Error(\"Failed to fetch recent chapters\");\n\t}\n}\n\n/**\n * Extract only relevant information from records to reduce token usage\n */\nexport function extractRelevantInformation(\n\trecords: SearchResult[],\n\tquery: string\n): SearchResult[] {\n\tif (!RELEVANCE_EXTRACTION_CONFIG.enabled) {\n\t\treturn records; // Return original records if feature is disabled\n\t}\n\n\tconsole.log(\n\t\t`[RELEVANCE-EXTRACTION] Extracting relevant information from ${records.length} records`\n\t);\n\n\t// Convert query to lowercase for matching\n\tconst queryLower = query.toLowerCase();\n\tconst queryWords = queryLower.split(/\\s+/).filter((word) => word.length > 2);\n\n\t// Use semantic search scores to determine relevance\n\treturn records.map((record) => {\n\t\tconst originalNote = record.note || \"\";\n\n\t\t// Calculate relevance score based on semantic search results\n\t\tconst relevanceScore = calculateRelevanceScore(record, queryWords, query);\n\n\t\t// Determine relevance level based on score\n\t\tconst relevanceLevel = determineRelevanceLevel(relevanceScore, record);\n\n\t\t// Apply appropriate extraction based on relevance level\n\t\tswitch (relevanceLevel) {\n\t\t\tcase \"high\":\n\t\t\t\t// Keep full content for highly relevant records\n\t\t\t\treturn record;\n\t\t\tcase \"medium\":\n\t\t\t\t// Extract key information for medium relevance\n\t\t\t\tconst extractedNote = extractKeyInformation(\n\t\t\t\t\toriginalNote,\n\t\t\t\t\tqueryWords,\n\t\t\t\t\tquery\n\t\t\t\t);\n\t\t\t\treturn {\n\t\t\t\t\t...record,\n\t\t\t\t\tnote: extractedNote,\n\t\t\t\t\t_processed: true,\n\t\t\t\t};\n\t\t\tcase \"low\":\n\t\t\tdefault:\n\t\t\t\t// Extract minimal information for low relevance\n\t\t\t\tconst minimalNote = extractMinimalInformation(\n\t\t\t\t\toriginalNote,\n\t\t\t\t\tqueryWords,\n\t\t\t\t\tquery\n\t\t\t\t);\n\t\t\t\treturn {\n\t\t\t\t\t...record,\n\t\t\t\t\tnote: minimalNote,\n\t\t\t\t\t_processed: true,\n\t\t\t\t};\n\t\t}\n\t});\n}\n\n/**\n * UPDATED: Calculate relevance score based on RRF results\n */\nfunction calculateRelevanceScore(\n\trecord: SearchResult,\n\tqueryWords: string[],\n\tquery: string\n): number {\n\tlet score = 0;\n\n\t// 1. Use RRF Score (Primary Signal)\n\t// RRF scores are small (0.0 - 0.033), so we multiply by 1000 to make them usable integers\n\tif (record.rrf_score !== undefined) {\n\t\tscore += record.rrf_score * 1000;\n\t\t// Example: 0.032 * 1000 = 32 points (Very High)\n\t\t// Example: 0.009 * 1000 = 9 points (Low)\n\t}\n\t// Fallback for legacy results\n\telse if (record.combined_score !== undefined) {\n\t\tscore += record.combined_score * 20;\n\t}\n\n\t// 2. Query word matches in title and content (Keep this logic, it's good)\n\tconst titleLower = record.title.toLowerCase();\n\tconst noteLower = (record.note || \"\").toLowerCase();\n\tconst subjectLower = (record.subject || \"\").toLowerCase();\n\n\tqueryWords.forEach((word) => {\n\t\t// Title matches get higher weight\n\t\tif (titleLower.includes(word)) {\n\t\t\tscore += 5;\n\t\t}\n\t\t// Content matches\n\t\tif (noteLower.includes(word)) {\n\t\t\tscore += 3;\n\t\t}\n\t\t// Subject matches\n\t\tif (subjectLower.includes(word)) {\n\t\t\tscore += 2;\n\t\t}\n\t});\n\n\t// 3. Analyze query type and adjust scoring (Keep this, it's good)\n\tconst queryType = analyzeQueryType(query);\n\tscore = adjustScoreForQueryType(score, queryType, record);\n\n\treturn score;\n}\n\n/**\n * Analyze the type of query to determine what information is most relevant\n */\nfunction analyzeQueryType(query: string): string {\n\tconst queryLower = query.toLowerCase();\n\n\t// Check for specific query patterns\n\tif (\n\t\t(queryLower.includes(\"person\") ||\n\t\t\tqueryLower.includes(\"people\") ||\n\t\t\tqueryLower.includes(\"individual\")) &&\n\t\t(queryLower.includes(\"name\") ||\n\t\t\tqueryLower.includes(\"who\") ||\n\t\t\tqueryLower.includes(\"participant\"))\n\t) {\n\t\t// Check if age is also mentioned\n\t\tif (\n\t\t\tqueryLower.includes(\"age\") ||\n\t\t\tqueryLower.includes(\"old\") ||\n\t\t\tqueryLower.includes(\"years\")\n\t\t) {\n\t\t\treturn \"person_with_age\";\n\t\t}\n\t\treturn \"person\";\n\t}\n\tif (\n\t\tqueryLower.includes(\"location\") ||\n\t\tqueryLower.includes(\"place\") ||\n\t\tqueryLower.includes(\"where\")\n\t) {\n\t\treturn \"location\";\n\t}\n\tif (\n\t\tqueryLower.includes(\"date\") ||\n\t\tqueryLower.includes(\"when\") ||\n\t\tqueryLower.includes(\"time\")\n\t) {\n\t\treturn \"temporal\";\n\t}\n\tif (queryLower.includes(\"age\") || queryLower.includes(\"old\")) {\n\t\treturn \"demographic\";\n\t}\n\tif (\n\t\tqueryLower.includes(\"list\") ||\n\t\tqueryLower.includes(\"all\") ||\n\t\tqueryLower.includes(\"every\")\n\t) {\n\t\treturn \"comprehensive\";\n\t}\n\tif (\n\t\tqueryLower.includes(\"count\") ||\n\t\tqueryLower.includes(\"number\") ||\n\t\tqueryLower.includes(\"total\")\n\t) {\n\t\treturn \"statistical\";\n\t}\n\n\treturn \"general\";\n}\n\n/**\n * Adjust relevance score based on query type\n */\nfunction adjustScoreForQueryType(\n\tscore: number,\n\tqueryType: string,\n\trecord: SearchResult\n): number {\n\tconst noteLower = (record.note || \"\").toLowerCase();\n\n\tswitch (queryType) {\n\t\tcase \"person\":\n\t\t\t// Boost score for records containing person/people information\n\t\t\tif (\n\t\t\t\tnoteLower.includes(\"person\") ||\n\t\t\t\tnoteLower.includes(\"people\") ||\n\t\t\t\tnoteLower.includes(\"individual\") ||\n\t\t\t\tnoteLower.includes(\"participant\") ||\n\t\t\t\tnoteLower.includes(\"member\") ||\n\t\t\t\tnoteLower.includes(\"name\")\n\t\t\t) {\n\t\t\t\tscore += 10;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase \"person_with_age\":\n\t\t\t// Boost score for records containing person information AND age\n\t\t\tif (\n\t\t\t\t(noteLower.includes(\"person\") ||\n\t\t\t\t\tnoteLower.includes(\"people\") ||\n\t\t\t\t\tnoteLower.includes(\"individual\") ||\n\t\t\t\t\tnoteLower.includes(\"participant\") ||\n\t\t\t\t\tnoteLower.includes(\"member\") ||\n\t\t\t\t\tnoteLower.includes(\"name\")) &&\n\t\t\t\t(noteLower.includes(\"age\") ||\n\t\t\t\t\tnoteLower.includes(\"old\") ||\n\t\t\t\t\tnoteLower.includes(\"years\"))\n\t\t\t) {\n\t\t\t\tscore += 10;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase \"location\":\n\t\t\t// Boost score for records containing location information\n\t\t\tif (\n\t\t\t\tnoteLower.includes(\"location\") ||\n\t\t\t\tnoteLower.includes(\"place\") ||\n\t\t\t\tnoteLower.includes(\"address\") ||\n\t\t\t\tnoteLower.includes(\"where\")\n\t\t\t) {\n\t\t\t\tscore += 8;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase \"temporal\":\n\t\t\t// Boost score for records containing date/time information\n\t\t\tif (\n\t\t\t\tnoteLower.includes(\"date\") ||\n\t\t\t\tnoteLower.includes(\"time\") ||\n\t\t\t\t/\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}/.test(noteLower)\n\t\t\t) {\n\t\t\t\tscore += 6;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase \"demographic\":\n\t\t\t// Boost score for records containing age information\n\t\t\tif (noteLower.includes(\"age\") || /\\d+\\s*years?\\s*old/.test(noteLower)) {\n\t\t\t\tscore += 7;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase \"comprehensive\":\n\t\t\t// For comprehensive queries, keep more records relevant\n\t\t\tscore += 3;\n\t\t\tbreak;\n\t\tcase \"statistical\":\n\t\t\t// For statistical queries, focus on records with numbers\n\t\t\tif (/\\d+/.test(noteLower)) {\n\t\t\t\tscore += 4;\n\t\t\t}\n\t\t\tbreak;\n\t}\n\n\treturn score;\n}\n\n/**\n * Determine relevance level based on calculated score\n */\n/**\n * UPDATED: Determine relevance level based on RRF\n */\nfunction determineRelevanceLevel(\n\tscore: number,\n\trecord: SearchResult\n): \"high\" | \"medium\" | \"low\" {\n\t// 1. Check RRF Score directly if available\n\tif (record.rrf_score !== undefined) {\n\t\t// Rank 1 in at least one list OR Top 10 in both\n\t\tif (record.rrf_score > 0.015) return \"high\";\n\t\t// Rank 50 in at least one list (Basic relevance)\n\t\tif (record.rrf_score > 0.005) return \"medium\";\n\t\treturn \"low\";\n\t}\n\n\t// 2. Fallback to calculated score (from calculateRelevanceScore above)\n\t// Since we multiplied RRF by 1000, a \"High\" score is around 30+\n\tif (score > 30) return \"high\";\n\tif (score > 15) return \"medium\";\n\treturn \"low\";\n}\n\n/**\n * Get patterns based on query type for more targeted extraction\n */\nfunction getPatternsForQueryType(\n\tqueryType: string\n): Array<{ pattern: RegExp; weight: number; label: string }> {\n\tconst basePatterns = [\n\t\t// Name patterns (always relevant)\n\t\t{ pattern: /([A-Z][a-z]+\\s+[A-Z][a-z]+)/g, weight: 2, label: \"Name\" },\n\t\t// File number patterns (always relevant)\n\t\t{ pattern: /file\\s*no[:\\s]*([^.!?]+)/gi, weight: 2, label: \"File\" },\n\t\t{\n\t\t\tpattern: /reference\\s*no[:\\s]*([^.!?]+)/gi,\n\t\t\tweight: 2,\n\t\t\tlabel: \"Reference\",\n\t\t},\n\t\t{ pattern: /id[:\\s]*([^.!?]+)/gi, weight: 2, label: \"ID\" },\n\t];\n\n\tswitch (queryType) {\n\t\tcase \"person\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /person[:\\s]+([^.!?]+)/gi, weight: 5, label: \"Person\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /participant[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Participant\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /member[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Member\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /individual[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Individual\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /contact[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Contact\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /name[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Name\",\n\t\t\t\t},\n\t\t\t];\n\t\tcase \"person_with_age\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /person[:\\s]+([^.!?]+)/gi, weight: 5, label: \"Person\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /participant[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Participant\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /member[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Member\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /individual[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Individual\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /contact[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Contact\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /name[:\\s]+([^.!?]+)/gi,\n\t\t\t\t\tweight: 5,\n\t\t\t\t\tlabel: \"Name\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /age[:\\s]+(\\d+)/gi, weight: 5, label: \"Age\" },\n\t\t\t\t{ pattern: /(\\d+)\\s*years?\\s*old/gi, weight: 5, label: \"Age\" },\n\t\t\t\t{ pattern: /(\\d+)\\s*years?/gi, weight: 3, label: \"Age\" },\n\t\t\t];\n\t\tcase \"location\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /location[:\\s]+([^.!?]+)/gi, weight: 5, label: \"Location\" },\n\t\t\t\t{ pattern: /place[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Place\" },\n\t\t\t\t{ pattern: /address[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Address\" },\n\t\t\t\t{ pattern: /where[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Location\" },\n\t\t\t];\n\t\tcase \"temporal\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /date[:\\s]+([^.!?]+)/gi, weight: 5, label: \"Date\" },\n\t\t\t\t{ pattern: /time[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Time\" },\n\t\t\t\t{\n\t\t\t\t\tpattern: /(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4})/g,\n\t\t\t\t\tweight: 4,\n\t\t\t\t\tlabel: \"Date\",\n\t\t\t\t},\n\t\t\t\t{ pattern: /when[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Time\" },\n\t\t\t];\n\t\tcase \"demographic\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /age[:\\s]+(\\d+)/gi, weight: 5, label: \"Age\" },\n\t\t\t\t{ pattern: /(\\d+)\\s*years?\\s*old/gi, weight: 5, label: \"Age\" },\n\t\t\t\t{ pattern: /(\\d+)\\s*years?/gi, weight: 3, label: \"Age\" },\n\t\t\t];\n\t\tcase \"statistical\":\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t{ pattern: /(\\d+)/g, weight: 3, label: \"Number\" },\n\t\t\t\t{ pattern: /count[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Count\" },\n\t\t\t\t{ pattern: /total[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Total\" },\n\t\t\t];\n\t\tdefault:\n\t\t\treturn [\n\t\t\t\t...basePatterns,\n\t\t\t\t// General patterns for unknown query types\n\t\t\t\t{ pattern: /person[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Person\" },\n\t\t\t\t{ pattern: /name[:\\s]+([^.!?]+)/gi, weight: 4, label: \"Name\" },\n\t\t\t\t{ pattern: /location[:\\s]+([^.!?]+)/gi, weight: 3, label: \"Location\" },\n\t\t\t\t{ pattern: /date[:\\s]+([^.!?]+)/gi, weight: 3, label: \"Date\" },\n\t\t\t\t{ pattern: /age[:\\s]+(\\d+)/gi, weight: 3, label: \"Age\" },\n\t\t\t];\n\t}\n}\n\n/**\n * Extract key information from a note based on query relevance\n */\nfunction extractKeyInformation(\n\tnote: string,\n\tqueryWords: string[],\n\tquery: string\n): string {\n\tif (!note) return \"\";\n\n\t// Split note into sentences\n\tconst sentences = note.split(/[.!?]+/).filter((s) => s.trim().length > 0);\n\n\t// Define patterns based on query type\n\tconst queryType = analyzeQueryType(query);\n\tconst patterns = getPatternsForQueryType(queryType);\n\n\t// Score sentences based on relevance\n\tconst scoredSentences = sentences.map((sentence) => {\n\t\tconst sentenceLower = sentence.toLowerCase();\n\t\tlet score = 0;\n\t\tconst extractedInfo: string[] = [];\n\n\t\t// Score based on query word matches\n\t\tqueryWords.forEach((word) => {\n\t\t\tif (sentenceLower.includes(word)) {\n\t\t\t\tscore += 3; // Higher weight for query matches\n\t\t\t}\n\t\t});\n\n\t\t// Score based on specific patterns\n\t\tpatterns.forEach(({ pattern, weight, label }) => {\n\t\t\tconst matches = sentence.match(pattern);\n\t\t\tif (matches) {\n\t\t\t\tscore += weight;\n\t\t\t\tmatches.forEach((match) => {\n\t\t\t\t\textractedInfo.push(`${label}: ${match.trim()}`);\n\t\t\t\t});\n\t\t\t}\n\t\t});\n\n\t\t// Score based on relevance keywords\n\t\tconst relevanceKeywords = [\n\t\t\t\"person\",\n\t\t\t\"people\",\n\t\t\t\"individual\",\n\t\t\t\"participant\",\n\t\t\t\"member\",\n\t\t\t\"location\",\n\t\t\t\"place\",\n\t\t\t\"date\",\n\t\t\t\"time\",\n\t\t\t\"age\",\n\t\t\t\"name\",\n\t\t\t\"address\",\n\t\t\t\"phone\",\n\t\t\t\"email\",\n\t\t\t\"contact\",\n\t\t\t\"project\",\n\t\t\t\"document\",\n\t\t\t\"record\",\n\t\t];\n\n\t\trelevanceKeywords.forEach((keyword) => {\n\t\t\tif (sentenceLower.includes(keyword)) {\n\t\t\t\tscore += 1;\n\t\t\t}\n\t\t});\n\n\t\treturn {\n\t\t\tsentence: sentence.trim(),\n\t\t\tscore,\n\t\t\textractedInfo,\n\t\t\thasRelevantInfo: extractedInfo.length > 0,\n\t\t};\n\t});\n\n\t// Filter and sort by score, prioritize sentences with extracted info\n\tconst relevantSentences = scoredSentences\n\t\t.filter((item) => item.score > 0 || item.hasRelevantInfo)\n\t\t.sort((a, b) => {\n\t\t\t// Prioritize sentences with extracted info\n\t\t\tif (a.hasRelevantInfo && !b.hasRelevantInfo) return -1;\n\t\t\tif (!a.hasRelevantInfo && b.hasRelevantInfo) return 1;\n\t\t\t// Then sort by score\n\t\t\treturn b.score - a.score;\n\t\t})\n\t\t.slice(0, 5); // Keep top 5 most relevant sentences\n\n\t// If we have sentences with extracted info, return those\n\tconst sentencesWithExtractedInfo = relevantSentences.filter(\n\t\t(item) => item.hasRelevantInfo\n\t);\n\tif (sentencesWithExtractedInfo.length > 0) {\n\t\treturn (\n\t\t\tsentencesWithExtractedInfo.map((item) => item.sentence).join(\". \") + \".\"\n\t\t);\n\t}\n\n\t// If no relevant sentences found, return a very short summary\n\tif (relevantSentences.length === 0) {\n\t\treturn `[Summary] ${note.substring(0, 100)}${note.length > 100 ? \"...\" : \"\"\n\t\t\t}`;\n\t}\n\n\t// Return relevant sentences\n\treturn relevantSentences.map((item) => item.sentence).join(\". \") + \".\";\n}\n\n/**\n * Extract minimal information for low relevance records\n */\nfunction extractMinimalInformation(\n\tnote: string,\n\t_queryWords: string[],\n\t_query: string\n): string {\n\tif (!note) return \"\";\n\n\t// Look for specific patterns that are always relevant\n\tconst essentialPatterns = [\n\t\t/person[:\\s]+([^.!?]+)/gi,\n\t\t/name[:\\s]+([^.!?]+)/gi,\n\t\t/individual[:\\s]+([^.!?]+)/gi,\n\t\t/participant[:\\s]+([^.!?]+)/gi,\n\t\t/location[:\\s]+([^.!?]+)/gi,\n\t\t/place[:\\s]+([^.!?]+)/gi,\n\t\t/date[:\\s]+([^.!?]+)/gi,\n\t\t/time[:\\s]+([^.!?]+)/gi,\n\t\t/age[:\\s]+(\\d+)/gi,\n\t\t/(\\d+)\\s*years?\\s*old/gi,\n\t\t/(\\d+)\\s*years?/gi, // More general age pattern\n\t\t/([A-Z][a-z]+\\s+[A-Z][a-z]+)/g, // Names\n\t];\n\n\tconst extractedInfo: string[] = [];\n\n\tessentialPatterns.forEach((pattern) => {\n\t\tconst matches = note.match(pattern);\n\t\tif (matches) {\n\t\t\tmatches.forEach((match) => {\n\t\t\t\textractedInfo.push(match.trim());\n\t\t\t});\n\t\t}\n\t});\n\n\t// If we found essential info, return it\n\tif (extractedInfo.length > 0) {\n\t\treturn `[Essential Info] ${extractedInfo.slice(0, 3).join(\", \")}.`;\n\t}\n\n\t// Otherwise return a very short summary\n\treturn `[Summary] ${note.substring(0, 50)}${note.length > 50 ? \"...\" : \"\"}`;\n}\n\n/**\n * Prepare context for AI with optional relevance extraction\n */\nexport function prepareContextForAI(\n\trecords: SearchResult[],\n\tquery?: string,\n\tuseRelevanceExtraction: boolean = false\n): string {\n\tconst recordCount = records.length;\n\tconsole.log(\n\t\t`[CONTEXT-PREP] Processing ${recordCount} records for context generation`\n\t);\n\tif (records.length === 0) {\n\t\treturn \"No relevant records found in the database.\";\n\t}\n\n\t// Apply relevance extraction if enabled\n\tlet processedRecords = records;\n\tif (useRelevanceExtraction && query) {\n\t\tprocessedRecords = extractRelevantInformation(records, query);\n\t\tconsole.log(\n\t\t\t`[CONTEXT-PREP] Applied relevance extraction to reduce token usage`\n\t\t);\n\n\t\t// Add debugging for age-related queries\n\t\tconst queryType = analyzeQueryType(query);\n\t\tif (queryType.includes(\"age\") || query.toLowerCase().includes(\"age\")) {\n\t\t\tconsole.log(`[CONTEXT-PREP] Age-related query detected: \"${query}\"`);\n\t\t\tconsole.log(`[CONTEXT-PREP] Query type: ${queryType}`);\n\n\t\t\t// Count how many records have age information\n\t\t\tconst recordsWithAge = processedRecords.filter((record) => {\n\t\t\t\tconst note = record.note || \"\";\n\t\t\t\treturn (\n\t\t\t\t\t/age[:\\s]+(\\d+)/gi.test(note) ||\n\t\t\t\t\t/(\\d+)\\s*years?\\s*old/gi.test(note) ||\n\t\t\t\t\t/(\\d+)\\s*years?/gi.test(note)\n\t\t\t\t);\n\t\t\t});\n\t\t\tconsole.log(\n\t\t\t\t`[CONTEXT-PREP] Records with age information: ${recordsWithAge.length}/${processedRecords.length}`\n\t\t\t);\n\t\t}\n\t}\n\n\t// Group records by subject for smarter organization\n\tconst recordsBySubject = processedRecords.reduce((acc, record) => {\n\t\tconst subject = record.subject || \"Uncategorized\";\n\t\tif (!acc[subject]) acc[subject] = [];\n\t\tacc[subject].push(record);\n\t\treturn acc;\n\t}, {} as Record<string, SearchResult[]>);\n\n\t// Create a structured index of all records for quick reference\n\tconst recordIndex = processedRecords.map((record) => {\n\t\tlet relevanceDisplay = \"Unknown\";\n\n\t\tif (record.rrf_score !== undefined) {\n\t\t\t// Convert RRF to a simple \"Rank Score\" for the AI to understand\n\t\t\t// Just showing the raw number is fine for Gemini 2.0\n\t\t\trelevanceDisplay = `Score ${record.rrf_score.toFixed(4)}`;\n\t\t} else if (record.rank !== undefined) {\n\t\t\trelevanceDisplay = (record.rank * 100).toFixed(1) + \"%\";\n\t\t}\n\n\t\treturn {\n\t\t\tid: record.id,\n\t\t\ttitle: record.title,\n\t\t\tsubject: record.subject || \"Uncategorized\",\n\t\t\tdate: record.entry_date_real?.toLocaleDateString() || \"Unknown date\",\n\t\t\trelevance: relevanceDisplay,\n\t\t};\n\t});\n\n\t// Build the full record details with optimized content\n\tconst detailedRecords = processedRecords.map((record) => {\n\t\tconst content = record.note || \"No content available\";\n\n\t\t// Debug: Log content length for age-related queries\n\t\tif (\n\t\t\tquery &&\n\t\t\t(query.toLowerCase().includes(\"age\") ||\n\t\t\t\tquery.toLowerCase().includes(\"victim\"))\n\t\t) {\n\t\t\tconsole.log(\n\t\t\t\t`[CONTEXT-PREP] Record ${record.id} (${record.title}): content length=${content.length\n\t\t\t\t}, preview=${content.substring(0, 200)}...`\n\t\t\t);\n\t\t}\n\n\t\t// Avoid duplicating metadata if it's already in the note\n\t\tconst subjectPattern = new RegExp(\n\t\t\t`Subject[^\\\\n]*${escapeRegExp(record.subject || \"\")}`,\n\t\t\t\"i\"\n\t\t);\n\t\tconst titlePattern = new RegExp(\n\t\t\t`Title[^\\\\n]*${escapeRegExp(record.title)}`,\n\t\t\t\"i\"\n\t\t);\n\n\t\t// Determine relevance display\n\t\tlet relevanceDisplay = \"\";\n\t\tif (record.rrf_score !== undefined) {\n\t\t\trelevanceDisplay = `Score ${record.rrf_score.toFixed(4)}`;\n\t\t} else if (record.rank !== undefined) {\n\t\t\trelevanceDisplay = (record.rank * 100).toFixed(1) + \"%\";\n\t\t}\n\n\t\treturn {\n\t\t\tid: record.id,\n\t\t\ttitle: record.title,\n\t\t\tsubject: record.subject || \"Uncategorized\",\n\t\t\tdate: record.entry_date_real?.toLocaleDateString() || \"Unknown date\",\n\t\t\tcontent: content,\n\t\t\trelevance: relevanceDisplay,\n\t\t};\n\t});\n\n\t// Build the optimized context\n\treturn `\nDATABASE CONTEXT:\n\n=== OVERVIEW ===\n Found ${processedRecords.length\n\t\t} relevant records from the educational content database.\nThe records span ${Object.keys(recordsBySubject).length} subjects.\nRecords are listed below ordered by relevance to your query.\n\n  === RECORD INDEX ===\n  ${recordIndex\n\t\t\t.map(\n\t\t\t\t(r, i) =>\n\t\t\t\t\t`[${i + 1}] Chapter: ${r.title} | Subject: ${r.subject} | Date: ${r.date\n\t\t\t\t\t} | Relevance: ${r.relevance}`\n\t\t\t)\n\t\t\t.join(\"\\n\")}\n\n=== FULL RECORD DETAILS ===\n${detailedRecords\n\t\t\t.map(\n\t\t\t\t(record, index) => `\n**Chapter: ${record.title}** (Relevance: ${record.relevance})\nSubject: ${record.subject}\nDate: ${record.date}\nContent: ${record.content}\n---`\n\t\t\t)\n\t\t\t.join(\"\\n\")}\n\n=== SUBJECT SUMMARY ===\n${Object.entries(recordsBySubject)\n\t\t\t.map(([subject, records]) => `${subject}: ${records.length} records`)\n\t\t\t.join(\"\\n\")}\n\nEND OF DATABASE CONTEXT\n`;\n}\n\n/**\n * Escape special regex characters in a string\n */\nfunction escapeRegExp(text: string): string {\n\treturn text.replace(/[.*+?^${}()|[\\]\\\\]/g, \"\\\\$&\");\n}\n\n/**\n * Simple utility to estimate token count for a text string\n * This is a very rough approximation - tokens are typically 4 characters on average in English\n */\nfunction estimateTokenCount(text: string): number {\n\treturn Math.ceil(text.length / 4);\n}\n\n/**\n * Generate AI response using Gemini with conversation context\n */\nexport async function generateAIResponse(\n\tquestion: string,\n\tcontext: string,\n\tconversationHistory: ChatMessage[] = [],\n\tqueryType: string = \"specific_search\",\n\topts: { provider?: \"gemini\"; model?: string; keyId?: number } = {}\n): Promise<{\n\ttext: string;\n\tinputTokens: number;\n\toutputTokens: number;\n\tchartData?: any;\n}> {\n\t// Ensure queryType is one of the allowed types, default to specific_search\n\tconst allowedQueryTypes = [\n\t\t\"specific_search\",\n\t\t\"analytical_query\",\n\t\t\"follow_up\",\n\t\t\"elaboration\",\n\t\t\"general\",\n\t\t\"recent_files\",\n\t\t\"visualization\",\n\t];\n\tif (!allowedQueryTypes.includes(queryType)) {\n\t\tqueryType = \"specific_search\";\n\t}\n\tconst { client, keyId } = await getGeminiClient({\n\t\tprovider: \"gemini\",\n\t\tkeyId: opts.keyId,\n\t});\n\tconst dbModels = await getActiveModelNames(\"gemini\");\n\tconst attemptModels = Array.from(\n\t\tnew Set([opts.model, ...dbModels].filter(Boolean))\n\t);\n\t// Fallback to .env or hardcoded default if no models are available\n\tif (attemptModels.length === 0) {\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\tattemptModels.push(fallbackModel);\n\t}\n\n\t// Build conversation context for follow-up questions\n\tconst recentHistory = conversationHistory.slice(-4); // Last 2 exchanges\n\tconst historyContext =\n\t\trecentHistory.length > 0\n\t\t\t? `\\nCONVERSATION HISTORY:\\n${recentHistory\n\t\t\t\t.map((msg) => `${msg.role.toUpperCase()}: ${msg.content}`)\n\t\t\t\t.join(\"\\n\")}\\n`\n\t\t\t: \"\";\n\n\t// Adjust prompt based on query type\n\tlet roleInstructions = \"\";\n\tswitch (queryType) {\n\t\tcase \"analytical_query\":\n\t\t\troleInstructions = `\n- **Goal:** Provide a clear, comprehensive explanation that helps students understand the topic.\n- **Structure:**\n  1. **Simple Overview:** Start with a 2-3 sentence explanation in plain language that answers the core question.\n  2. **Main Concepts:** Break down the topic into key concepts, explaining each one simply with examples.\n  3. **Organized Information:** Group related information by themes (e.g., \"Key Points\", \"How It Works\", \"Examples\", \"Important Details\").\n  4. **Visual Aids:** If numbers/dates are involved, create a Markdown table to make comparisons easy to understand.\n  5. **Summary:** End with a brief recap that reinforces the main points.\n- **Remember:** Use analogies, real-world examples, and step-by-step explanations to make complex topics easy to grasp.\n`;\n\t\t\tbreak;\n\t\tcase \"follow_up\":\n\t\t\troleInstructions = `\n- This is a follow-up question referring to previous conversation.\n- Use both the conversation history and database records to answer.\n- Connect the current question to what was discussed before, building on previous explanations.\n- If the student is asking for clarification, provide a simpler explanation or use a different analogy.\n`;\n\t\t\tbreak;\n\t\tcase \"elaboration\":\n\t\t\troleInstructions = `\n- The student wants more detailed information or a deeper explanation.\n- Provide comprehensive details from the study materials, but keep explanations simple and clear.\n- Expand on the information with additional context, examples, and analogies.\n- Break down complex details into smaller, understandable pieces.\n`;\n\t\t\tbreak;\n\t\tcase \"recent_files\":\n\t\t\troleInstructions = `\n- The student asked for recent/latest chapters or materials.\n- Present the information in a clear, organized manner.\n- Include chapter titles, subjects, and dates.\n- Mention they are sorted by most recent first.\n`;\n\t\t\tbreak;\n\t\tdefault: // specific_search and general\n\t\t\troleInstructions = `\n- Answer the student's specific question using the provided study materials.\n- Explain concepts in simple terms with examples and analogies.\n- Be factual and cite relevant information by referencing chapter titles.\n- Provide clear, organized information that's easy to follow.\n- If explaining a concept, start with the basics and build up to more complex ideas.\n`;\n\t}\n\n\tconst prompt = `You are a friendly and patient AI tutor helping students learn from their educational materials.\nYour task is to explain concepts in simple, easy-to-understand terms using *only* the provided Database Context.\n\n=== DATABASE CONTEXT ===\n${context}\n\n=== CONVERSATION HISTORY ===\n${historyContext}\n\n=== USER QUESTION ===\n\"${question}\"\n\n=== SYSTEM INSTRUCTIONS ===\n1. **Student-Friendly Explanations (MOST IMPORTANT):**\n   - Explain everything in simple, clear language that students can easily understand.\n   - Break down complex concepts into smaller, digestible parts.\n   - Use everyday analogies and examples to help students relate to the material.\n   - Avoid jargon unless necessary, and always explain technical terms when you use them.\n   - Use a conversational, encouraging tone - like a helpful teacher explaining to a student.\n   - If explaining a process, use step-by-step instructions with clear numbering or bullet points.\n   - Relate concepts to real-world examples that students can visualize.\n\n2. **Strict Citations:** You MUST support every factual claim with a reference to the source chapter/title.\n   - Format: Use the chapter title/name directly, or (Source: Chapter Title).\n   - Example: \"According to the chapter on Introduction (Source: Introduction), the concept works like this...\"\n   - Always use the exact chapter title as shown in the context.\n\n3. **Hybrid Synthesis:** The context contains both \"Keyword Matches\" (exact words) and \"Semantic Matches\" (related concepts).\n   - If the user asks about a specific topic, synthesize information from multiple relevant pages.\n   - Connect related concepts across different parts of the material.\n   - Always cite chapters by their title/name.\n\n4. **Formatting:**\n   - Use Markdown tables for structured data (comparisons, lists, key points).\n   - Use bullet points for lists and step-by-step explanations.\n   - **Bold** key terms, important concepts, and definitions.\n   - Use numbered lists for processes or sequences.\n   - Break up long explanations into short paragraphs for easy reading.\n\n5. **Data Visualization:**\n   - You have the capability to generate charts (bar, line, pie, area).\n   - If the user asks to \"visualize\", \"chart\", \"plot\", or \"graph\" data, acknowledge the request.\n   - The system will automatically detect this intent and generate the chart for you.\n   - You do not need to generate ASCII charts; a real interactive chart will be rendered.\n\n6. **Honesty:**\n   - If the provided records do not contain the answer, state: \"I cannot find information about [X] in the current study materials.\"\n   - Do not invent information.\n   - If you're not sure, say so and suggest what the student might look for.\n\n${roleInstructions}\n\nRemember: Your goal is to help students understand, not just to provide information. Make learning easy and enjoyable!\n\nAnswer:`;\n\n\t// Handle visualization queries using structured output\n\tif (queryType === \"visualization\") {\n\t\ttry {\n\t\t\tconsole.log(\"[AI-GEN] Generating structured chart configuration...\");\n\t\t\t// Use priority: opts.model → admin config → .env → fallback\n\t\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\t\tconst modelName = opts.model || dbModels[0] || fallbackModel;\n\n\t\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\" });\n\t\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\n\t\t\tif (!keyToUse) {\n\t\t\t\tthrow new Error(\"No Gemini API key configured for chart generation\");\n\t\t\t}\n\n\t\t\tconst google = createGoogleGenerativeAI({\n\t\t\t\tapiKey: keyToUse,\n\t\t\t});\n\n\t\t\t// @ts-ignore - Type instantiation is excessively deep\n\t\t\tconst { object } = await generateObject({\n\t\t\t\tmodel: google(modelName),\n\t\t\t\tschema: ChartSchema,\n\t\t\t\tprompt: `\nYou are a data visualization expert.\nContext:\n${context}\n\nConversation History:\n${historyContext}\n\nUser Query: ${question}\n\nGenerate a chart configuration based on the user's query and the provided context.\nIf the data is not sufficient to create a chart, create a chart with empty data and a title indicating \"Insufficient Data\".\nEnsure the data is cleaned (remove currency symbols, handle missing values).\n`,\n\t\t\t});\n\n\t\t\tlet parsedData: any = object.data;\n\t\t\tif (typeof object.data === \"string\") {\n\t\t\t\ttry {\n\t\t\t\t\tparsedData = JSON.parse(object.data);\n\t\t\t\t} catch (e) {\n\t\t\t\t\tconsole.error(\"Failed to parse chart data string:\", e);\n\t\t\t\t\tparsedData = [];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tconst chartConfig = { ...object, data: parsedData };\n\t\t\tconsole.log(\n\t\t\t\t\"[CHART] Generated chart config:\",\n\t\t\t\tJSON.stringify(chartConfig, null, 2)\n\t\t\t);\n\t\t\treturn {\n\t\t\t\ttext: `Here is the ${object.type} chart you requested based on the data.`,\n\t\t\t\tinputTokens: 0, // Estimate or track if possible\n\t\t\t\toutputTokens: 0,\n\t\t\t\tchartData: chartConfig,\n\t\t\t};\n\t\t} catch (error) {\n\t\t\tconsole.error(\"[CHART] Failed to generate chart:\", error);\n\t\t\tconsole.error(\n\t\t\t\t\"[CHART] Error details:\",\n\t\t\t\terror instanceof Error ? error.message : String(error)\n\t\t\t);\n\t\t\t// Fallback to normal generation if chart generation fails\n\t\t\tconsole.log(\"[CHART] Falling back to text generation...\");\n\t\t\t// Don't return here - let it fall through to normal generation\n\t\t}\n\t}\n\n\tlet lastError: any = null;\n\tfor (const modelName of attemptModels) {\n\t\ttry {\n\t\t\tconsole.log(\n\t\t\t\t`[AI-GEN] Sending request to Gemini API, model=${modelName}, prompt size: ${prompt.length} characters`\n\t\t\t);\n\t\t\tconst apiCallTiming = timeStart(\"Gemini API Call\");\n\t\t\tconst model = client.getGenerativeModel({ model: modelName as string });\n\t\t\t// Use Gemini native token counter for input tokens\n\t\t\tlet inputTokens = 0;\n\t\t\ttry {\n\t\t\t\tconst countRes: any = await model.countTokens({\n\t\t\t\t\tcontents: [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\trole: \"user\",\n\t\t\t\t\t\t\tparts: [{ text: prompt }],\n\t\t\t\t\t\t},\n\t\t\t\t\t],\n\t\t\t\t});\n\t\t\t\tinputTokens = (countRes?.totalTokens ??\n\t\t\t\t\tcountRes?.totalTokenCount ??\n\t\t\t\t\t0) as number;\n\t\t\t} catch (e) {\n\t\t\t\t// Fallback to heuristic only if countTokens fails\n\t\t\t\tinputTokens = estimateTokenCount(prompt);\n\t\t\t\tconsole.warn(\"[AI-GEN] countTokens failed; using heuristic\", e);\n\t\t\t}\n\t\t\tconst result = await withTimeout(\n\t\t\t\tmodel.generateContent(prompt),\n\t\t\t\tAI_API_TIMEOUT,\n\t\t\t\t`AI response generation (${modelName})`\n\t\t\t);\n\t\t\tconst response = await result.response;\n\t\t\tconst text = response.text();\n\t\t\ttimeEnd(\"Gemini API Call\", apiCallTiming);\n\n\t\t\t// Prefer usage metadata from Gemini for output tokens\n\t\t\tconst usage: any = (response as any)?.usageMetadata;\n\t\t\tconst outputTokens = (usage?.candidatesTokenCount ??\n\t\t\t\tusage?.totalTokenCount ??\n\t\t\t\testimateTokenCount(text)) as number;\n\n\t\t\tdevLog(\"AI response generated successfully\", {\n\t\t\t\tinputTokens,\n\t\t\t\toutputTokens,\n\t\t\t\tmodelName,\n\t\t\t});\n\t\t\tif (keyId) await recordKeyUsage(keyId, true);\n\n\t\t\treturn {\n\t\t\t\ttext: text,\n\t\t\t\tinputTokens,\n\t\t\t\toutputTokens,\n\t\t\t};\n\t\t} catch (error: any) {\n\t\t\tlastError = error;\n\t\t\tif (keyId) await recordKeyUsage(keyId, false);\n\t\t\tconst errorMsg = error?.message || String(error);\n\t\t\tif (errorMsg.includes(\"timed out\")) {\n\t\t\t\tconsole.warn(\n\t\t\t\t\t`[AI-GEN] Response generation timeout after ${AI_API_TIMEOUT / 1000\n\t\t\t\t\t}s with model: ${modelName}`\n\t\t\t\t);\n\t\t\t} else {\n\t\t\t\tconsole.warn(`[AI-GEN] model attempt failed: ${modelName}`, error);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t}\n\tconsole.error(\"AI response generation error (all models failed):\", lastError);\n\tif (lastError?.message && lastError.message.includes(\"429\")) {\n\t\tthrow new Error(\"RATE_LIMIT_EXCEEDED\");\n\t}\n\t// Provide more context in error message\n\tconst errorDetails = lastError\n\t\t? `: ${lastError.message || String(lastError)}`\n\t\t: \" (no models available or configured)\";\n\tthrow new Error(`Failed to generate AI response${errorDetails}`);\n}\n\n/**\n * Generate AI response using Gemini with streaming support\n * Returns an async generator that yields text chunks as they arrive\n */\nexport async function* generateAIResponseStream(\n\tquestion: string,\n\tcontext: string,\n\tconversationHistory: ChatMessage[] = [],\n\tqueryType: string = \"specific_search\",\n\topts: { provider?: \"gemini\"; model?: string; keyId?: number } = {}\n): AsyncGenerator<\n\t{\n\t\ttype: \"token\" | \"done\";\n\t\ttext?: string;\n\t\tinputTokens?: number;\n\t\toutputTokens?: number;\n\t},\n\tvoid,\n\tunknown\n> {\n\t// Ensure queryType is one of the allowed types, default to specific_search\n\tconst allowedQueryTypes = [\n\t\t\"specific_search\",\n\t\t\"analytical_query\",\n\t\t\"follow_up\",\n\t\t\"elaboration\",\n\t\t\"general\",\n\t\t\"recent_files\",\n\t];\n\tif (!allowedQueryTypes.includes(queryType)) {\n\t\tqueryType = \"specific_search\";\n\t}\n\n\tconst { client, keyId } = await getGeminiClient({\n\t\tprovider: \"gemini\",\n\t\tkeyId: opts.keyId,\n\t});\n\tconst dbModels = await getActiveModelNames(\"gemini\");\n\tconst attemptModels = Array.from(\n\t\tnew Set([opts.model, ...dbModels].filter(Boolean))\n\t);\n\t// Fallback to .env or hardcoded default if no models are available\n\tif (attemptModels.length === 0) {\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\tattemptModels.push(fallbackModel);\n\t}\n\n\t// Build conversation context for follow-up questions\n\tconst recentHistory = conversationHistory.slice(-4); // Last 2 exchanges\n\tconst historyContext =\n\t\trecentHistory.length > 0\n\t\t\t? `\\nCONVERSATION HISTORY:\\n${recentHistory\n\t\t\t\t.map((msg) => `${msg.role.toUpperCase()}: ${msg.content}`)\n\t\t\t\t.join(\"\\n\")}\\n`\n\t\t\t: \"\";\n\n\t// Adjust prompt based on query type\n\tlet roleInstructions = \"\";\n\tswitch (queryType) {\n\t\tcase \"analytical_query\":\n\t\t\troleInstructions = `\n- **Goal:** Teach the student about this topic step-by-step, ensuring they understand the \"why\" and \"how\".\n- **Teaching Style:**\n  1. **Hook:** Start with a relatable analogy or simple definition to grab interest.\n  2. **Step-by-Step Breakdown:** Explain the concept in logical steps. Don't dump information; guide them through it.\n  3. **Check for Understanding:** After a major point, ask a rhetorical checking question (e.g., \"See how that connects?\").\n  4. **Visuals:** Use tables for comparisons.\n  5. **Recap:** Summarize the key takeaway.\n- **Formatting:** Use Markdown headers (###) and double newlines.\n`;\n\t\t\tbreak;\n\t\tcase \"follow_up\":\n\t\t\troleInstructions = `\n- This is a follow-up question. Continue the \"lesson\" naturally.\n- Connect the new question to what we just discussed: \"That's a great follow-up! It connects back to...\"\n- If they are confused, try a different angle or analogy.\n- Keep the encouraging teacher tone.\n`;\n\t\t\tbreak;\n\t\tcase \"elaboration\":\n\t\t\troleInstructions = `\n- The student wants to go deeper. This is a \"teachable moment\".\n- Provide comprehensive details but keep it structured.\n- Use \"Let's zoom in on this part...\" or \"Here's the interesting detail...\"\n- Ensure they don't get lost in the details by constantly relating back to the main concept.\n`;\n\t\t\tbreak;\n\t\tcase \"recent_files\":\n\t\t\troleInstructions = `\n- The student asked for recent/latest chapters or materials.\n- Present the information in a clear, organized manner.\n- Include chapter titles, subjects, and dates.\n- Mention they are sorted by most recent first.\n`;\n\t\t\tbreak;\n\t\tdefault: // specific_search and general\n\t\t\troleInstructions = `\n- Answer the student's specific question directly but gently.\n- Start with the direct answer, then explain *why* it is the answer.\n- Use examples from the text to illustrate.\n- End with an encouraging check-in: \"Does that help clarify [Topic]?\"\n`;\n\t}\n\n\tconst prompt = `You are a friendly, patient, and wise AI teacher. Your goal is not just to answer questions, but to *guide* the student to understanding.\nYour task is to explain concepts in simple, easy-to-understand terms using *only* the provided Database Context.\n\n=== DATABASE CONTEXT ===\n${context}\n\n=== CONVERSATION HISTORY ===\n${historyContext}\n\n=== USER QUESTION ===\n\"${question}\"\n\n=== SYSTEM INSTRUCTIONS ===\n1. **Persona: The Friendly Teacher:**\n   - You are not just an AI; you are a patient, encouraging, and wise teacher.\n   - Your goal is to *guide* the student to understanding, not just give answers.\n   - Use phrases like \"Great question!\", \"Let's break this down,\" or \"Think of it this way...\"\n   - Be supportive. If a topic is hard, acknowledge it: \"This can be tricky, but we'll get it together.\"\n\n2. **Interactive Teaching Strategy:**\n   - **Step-by-Step:** Never overwhelm the student. Break complex answers into numbered steps.\n   - **Check-ins:** Occasionally ask if they are following along or want to dive deeper into a specific part.\n   - **Analogies:** Always use real-world analogies to explain abstract concepts.\n\n3. **Suggested Questions (OPTIONAL - for exploration):**\n   - If appropriate, you may provide 3 related follow-up questions in a JSON block to help students explore further.\n   - Format:\n   \\`\\`\\`json\n   {\n     \"related_questions\": [\n       \"Question 1?\",\n       \"Question 2?\",\n       \"Question 3?\"\n     ]\n   }\n   \\`\\`\\`\n   - Do not add any text after this JSON block.\n\n4. **Interactive Response Buttons (REQUIRED):**\n   - At the end of EVERY response, you MUST provide 2-3 short, clickable options for the student to reply with.\n   - These help guide the conversation and make learning interactive.\n   - Format:\n   \\`\\`\\`json\n   {\n     \"suggested_responses\": [\n       \"Tell me more\",\n       \"Give an example\",\n       \"What about...?\"\n     ]\n   }\n   \\`\\`\\`\n   - Keep them short (max 4-5 words).\n   - Make them natural responses that fit the context of your answer.\n   - Do not add any text after this JSON block.\n\n5. **Formatting Rules:**\n   - **NO TABLES:** Do not use Markdown tables. They do not render well on mobile devices. Use bulleted lists or clear text structures instead.\n   - Use **bold** for key terms.\n   - Use double newlines between paragraphs for better readability.\n\n=== TUTOR MODE INSTRUCTIONS (ACTIVE) ===\nYou are currently conducting an interactive lesson.\nCurrent Status: The student has started a formal learning session.\nYour Goal: Guide the student step-by-step through the chapter.\n\n**CRITICAL: RESPONSE LENGTH & PACING**\n1. **KEEP IT SHORT:** Your responses must be bite-sized (max 150 words).\n2. **ONE CONCEPT ONLY:** Explain *only* one small concept at a time.\n3. **NO DUMPING:** Do NOT summarize the whole chapter. Do NOT list every detail.\n4. **WAIT FOR STUDENT:** After explaining one concept and asking a question, STOP. Wait for the answer.\n\n**Tutor Loop Strategy:**\n1. **TEACH**: Explain *one* concept at a time clearly and simply. Use analogies.\n2. **CHECK**: Immediately after explaining, ask a specific question to verify understanding.\n   - Do NOT ask \"Do you understand?\".\n   - Ask a conceptual question like \"So, if X happens, what would happen to Y?\" or a simple problem to solve.\n3. **EVALUATE**:\n   - If the student answers correctly: Praise them briefly, then move to the next concept.\n   - If incorrect: Explain *why* it was wrong, simplify the concept, and ask a *new* question to check again.\n   - Do NOT move on until the student demonstrates mastery.\n\n**INTERACTIVE BUTTONS (REQUIRED):**\nAt the end of your response, you MUST provide 2-3 short, clickable options for the student to reply with.\nFormat:\n\\`\\`\\`json\n{\n  \"suggested_responses\": [\n    \"Yes, dive deeper\",\n    \"No, explain more\",\n    \"Give me an example\"\n  ]\n}\n\\`\\`\\`\n- These should be natural responses to your question.\n- Keep them short (max 4-5 words).\n\n**Tone:** Encouraging, patient, and structured. You are a personal tutor, not just a search engine.\n\n2. **Strict Citations:** You MUST support every factual claim with a reference to the source chapter/title.\n   - Format: Use the chapter title/name directly, or (Source: Chapter Title).\n   - Example: \"According to the chapter on Introduction (Source: Introduction), the concept works like this...\"\n   - Always use the exact chapter title as shown in the context.\n   - **NEVER mention \"Activity X.Y\", \"Figure X.Y\",\"Exercise X.Y\" or similar from the textbook.** Explain concepts directly without referencing textbook exercises.\n\n3. **Hybrid Synthesis:** The context contains both \"Keyword Matches\" (exact words) and \"Semantic Matches\" (related concepts).\n   - If the user asks about a specific topic, synthesize information from multiple relevant pages.\n   - Connect related concepts across different parts of the material.\n   - Always cite chapters by their title/name.\n\n4. **Formatting & Structure (CRITICAL):**\n   - **Use Double Newlines:** You MUST use double newlines (two blank lines) between paragraphs to ensure they render correctly.\n   - **Use Headers:** Use Markdown headers (###) to separate different sections of your answer.\n   - **Lists:** Use bullet points or numbered lists for steps, features, or key points.\n   - **Bold:** Use **bold** for key terms and important concepts.\n   - **Tables:** Use Markdown tables for comparisons or structured data.\n   - **Short Paragraphs:** Keep paragraphs short (2-3 sentences) for better readability.\n\n5. **Data Visualization:**\n   - You have the capability to generate charts (bar, line, pie, area).\n   - If the user asks to \"visualize\", \"chart\", \"plot\", or \"graph\" data, acknowledge the request.\n   - The system will automatically detect this intent and generate the chart for you.\n   - You do not need to generate ASCII charts; a real interactive chart will be rendered.\n\n6. **Knowledge Guidelines:**\n   - **Primary Source:** Base your answers on the provided chapter content.\n   - **Supplementary Knowledge Allowed:** You may use your general knowledge when:\n     * It helps explain or clarify concepts from the chapter\n     * The question is related to chapter topics and enhances understanding\n     * Providing examples or analogies that complement the chapter material\n   - **Strict Boundaries:** Do NOT answer if:\n     * The question is completely off-topic or unrelated to the chapter\n     * It's about a different subject matter not covered in this chapter\n   - **Be Transparent:** If you're adding information beyond the chapter, briefly acknowledge it: \"The chapter covers [X], and to help understand this better...\"\n   - **When Uncertain:** If the chapter doesn't cover something, say: \"I cannot find information about [X] in this chapter.\"\n\n${roleInstructions}\n\nRemember: Your goal is to help students understand, not just to provide information. Make learning easy and enjoyable!\n\nAnswer:`;\n\n\tlet lastError: any = null;\n\tfor (const modelName of attemptModels) {\n\t\ttry {\n\t\t\tconsole.log(\n\t\t\t\t`[AI-GEN-STREAM] Sending streaming request to Gemini API, model=${modelName}`\n\t\t\t);\n\t\t\tconst model = client.getGenerativeModel({ model: modelName as string });\n\n\t\t\t// Count input tokens\n\t\t\tlet inputTokens = 0;\n\t\t\ttry {\n\t\t\t\tconst countRes: any = await model.countTokens({\n\t\t\t\t\tcontents: [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\trole: \"user\",\n\t\t\t\t\t\t\tparts: [{ text: prompt }],\n\t\t\t\t\t\t},\n\t\t\t\t\t],\n\t\t\t\t});\n\t\t\t\tinputTokens = (countRes?.totalTokens ??\n\t\t\t\t\tcountRes?.totalTokenCount ??\n\t\t\t\t\t0) as number;\n\t\t\t} catch (e) {\n\t\t\t\tinputTokens = estimateTokenCount(prompt);\n\t\t\t\tconsole.warn(\"[AI-GEN-STREAM] countTokens failed; using heuristic\", e);\n\t\t\t}\n\n\t\t\t// Use priority: opts.model → admin config → .env → fallback\n\t\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\t\tconst streamingModelName = opts.model || dbModels[0] || fallbackModel;\n\t\t\tconst streamingModel = client.getGenerativeModel({\n\t\t\t\tmodel: streamingModelName as string,\n\t\t\t});\n\n\t\t\t// Generate streaming response\n\t\t\tconst result = await streamingModel.generateContentStream(prompt);\n\t\t\tlet fullText = \"\";\n\n\t\t\t// Stream tokens as they arrive\n\t\t\tfor await (const chunk of result.stream) {\n\t\t\t\tconst chunkText = chunk.text();\n\t\t\t\tfullText += chunkText;\n\t\t\t\tyield { type: \"token\", text: chunkText };\n\t\t\t}\n\n\t\t\t// Get final response for token counting\n\t\t\tconst response = await result.response;\n\t\t\tconst usage: any = (response as any)?.usageMetadata;\n\t\t\tconst outputTokens = (usage?.candidatesTokenCount ??\n\t\t\t\tusage?.totalTokenCount ??\n\t\t\t\testimateTokenCount(fullText)) as number;\n\n\t\t\tconsole.log(\n\t\t\t\t`[AI-GEN-STREAM] Streaming completed successfully with model: ${modelName}`\n\t\t\t);\n\t\t\tif (keyId) await recordKeyUsage(keyId, true);\n\n\t\t\t// Yield final metadata\n\t\t\tyield { type: \"done\", inputTokens, outputTokens };\n\t\t\treturn;\n\t\t} catch (error: any) {\n\t\t\tlastError = error;\n\t\t\tif (keyId) await recordKeyUsage(keyId, false);\n\t\t\tconsole.warn(`[AI-GEN-STREAM] model attempt failed: ${modelName}`, error);\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tconsole.error(\"AI response streaming error (all models failed):\", lastError);\n\tthrow new Error(\"Failed to generate AI response stream.\");\n}\n\n/**\n * Main chat function using enhanced search with conversation context\n */\nexport async function processChatMessageEnhanced(\n\tquestion: string,\n\tconversationHistory: ChatMessage[] = [],\n\tsearchLimit?: number,\n\t_useEnhancedSearch: boolean = true,\n\topts: { provider?: \"gemini\"; model?: string; keyId?: number } = {},\n\tfilters?: {\n\t\tboardId?: string;\n\t\tsubjectId?: number;\n\t\tchapterId?: number;\n\t}\n): Promise<{\n\tresponse: string;\n\tsources: Array<{\n\t\tid: number;\n\t\ttitle: string;\n\t\trelevance?: number;\n\t\tsimilarity?: number; // For UI display (converted from RRF score or semantic_similarity)\n\t\tcitation?: {\n\t\t\tpageNumber: number;\n\t\t\timageUrl: string;\n\t\t\tboundingBox: any;\n\t\t};\n\t}>;\n\tsearchQuery: string;\n\tsearchMethod:\n\t| \"hybrid\"\n\t| \"semantic_fallback\"\n\t| \"tsvector_only\"\n\t| \"vector_only\"\n\t| \"keyword_only\"\n\t| \"recent_files\"\n\t| \"analytical_fallback\";\n\tqueryType: string;\n\tanalysisUsed: boolean;\n\ttokenCount?: {\n\t\tinput: number;\n\t\toutput: number;\n\t};\n\tstats?: {\n\t\ttsvectorResults: number;\n\t\tsemanticResults: number;\n\t\tfinalResults: number;\n\t};\n\tchartData?: any;\n}> {\n\tconsole.log(`[ADMIN CHAT] User admin asked: \"${question}\"`);\n\n\tlet analysisUsed = false;\n\tlet queryForSearch = question;\n\tlet queryType = \"specific_search\"; // Default\n\tlet searchLimitForRecent = 10; // Default for recent files\n\n\tlet analysisInputTokens = 0;\n\tlet analysisOutputTokens = 0;\n\tlet analysis: any = null;\n\ttry {\n\t\tanalysis = await analyzeQueryForSearch(question, conversationHistory, opts);\n\t\tconsole.log(\"[CHAT ANALYSIS] Processing query:\", `\"${question}\"`);\n\t\tconsole.log(\n\t\t\t\"[CHAT ANALYSIS] Conversation history length:\",\n\t\t\tconversationHistory.length\n\t\t);\n\t\tconsole.log(\"[CHAT ANALYSIS] Query type:\", analysis.queryType);\n\t\tconsole.log(\n\t\t\t\"[CHAT ANALYSIS] Core search terms:\",\n\t\t\t`\"${analysis.coreSearchTerms}\"`\n\t\t);\n\t\tif (analysis.semanticConcepts) {\n\t\t\tconsole.log(\n\t\t\t\t\"[CHAT ANALYSIS] Semantic concepts:\",\n\t\t\t\t`\"${analysis.semanticConcepts}\"`\n\t\t\t);\n\t\t}\n\t\tconsole.log(\n\t\t\t\"[CHAT ANALYSIS] Instructional terms:\",\n\t\t\t`\"${analysis.instructionalTerms}\"`\n\t\t);\n\t\tconsole.log(\"[CHAT ANALYSIS] Context needed:\", analysis.contextNeeded);\n\n\t\tanalysisUsed = true;\n\t\t// Use coreSearchTerms if available, otherwise fall back to instructional terms\n\t\tqueryForSearch =\n\t\t\tanalysis.coreSearchTerms || analysis.instructionalTerms || \"\";\n\t\tqueryType = analysis.queryType;\n\n\t\t// Special handling for queries that want to show all records\n\t\tif (\n\t\t\t!queryForSearch &&\n\t\t\tanalysis.instructionalTerms?.toLowerCase().includes(\"all\")\n\t\t) {\n\t\t\t// For \"show all\" type queries, use empty query to match all records\n\t\t\tqueryForSearch = \"\"; // Empty query to match all records\n\t\t\tconsole.log(\n\t\t\t\t'[CHAT ANALYSIS] Using empty search term for \"show all\" query'\n\t\t\t);\n\t\t}\n\t\tanalysisInputTokens = analysis.inputTokens || 0;\n\t\tanalysisOutputTokens = analysis.outputTokens || 0;\n\t\tconsole.log(\n\t\t\t`[TOKENS] Analysis phase — input: ${analysisInputTokens}, output: ${analysisOutputTokens}`\n\t\t);\n\n\t\t// Check for a number in instructional terms for recent files query\n\t\tif (analysis.queryType === \"recent_files\" && analysis.instructionalTerms) {\n\t\t\tconst num = parseInt(analysis.instructionalTerms, 10);\n\t\t\tif (!isNaN(num)) {\n\t\t\t\tsearchLimitForRecent = num;\n\t\t\t}\n\t\t}\n\n\t\t// Handle list_all queries\n\t\tif (analysis.queryType === \"list_all\") {\n\t\t\tqueryForSearch = \"\"; // Return all records\n\t\t\tconsole.log(\n\t\t\t\t'[CHAT ANALYSIS] Query type is \"list_all\", returning all records'\n\t\t\t);\n\t\t}\n\t} catch (error) {\n\t\tconsole.error(\"Failed to analyze query with AI, using raw query.\", error);\n\t\t// Fallback to using the raw question if analysis fails\n\t\tqueryForSearch = question;\n\t\tanalysis = { queryType: \"specific_search\", contextNeeded: false };\n\t}\n\n\tlet records: SearchResult[] = [];\n\tlet searchMethod:\n\t\t| \"hybrid\"\n\t\t| \"semantic_fallback\"\n\t\t| \"tsvector_only\"\n\t\t| \"recent_files\"\n\t\t| \"analytical_fallback\" = \"hybrid\";\n\tlet searchStats;\n\n\tif (queryType === \"recent_files\") {\n\t\trecords = await getRecentFiles(searchLimitForRecent);\n\t\tsearchMethod = \"recent_files\";\n\t\tconsole.log(`[CHAT ANALYSIS] Found ${records.length} recent files.`);\n\t} else {\n\t\t// Use the new Hybrid Search Service\n\t\t// Determine effective search limit: prefer explicit argument; otherwise use admin-configured setting\n\t\tconst configuredLimit = await getSettingInt(\"ai.search.limit\", 30);\n\t\tlet effectiveSearchLimit =\n\t\t\tNumber.isFinite(searchLimit as any) && (searchLimit as number) > 0\n\t\t\t\t? Math.floor(searchLimit as number)\n\t\t\t\t: configuredLimit;\n\t\t// Clamp to sensible bounds\n\t\tif (effectiveSearchLimit < 1) effectiveSearchLimit = 1;\n\t\tif (effectiveSearchLimit > 200) effectiveSearchLimit = 200;\n\n\t\t// For analytical queries with filters, get ALL records matching the filters\n\t\t// instead of filtering by search terms (analytical queries need complete data)\n\t\t// For analytical queries with filters, get ALL records matching the filters\n\t\t// instead of filtering by search terms (analytical queries need complete data)\n\t\tconst hasFilters = filters && (filters.boardId || filters.subjectId);\n\t\tif (queryType === \"analytical_query\" && hasFilters) {\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Analytical query with filters - retrieving all records matching filters for complete analysis`\n\t\t\t);\n\t\t\tconst hybridSearchResponse = await HybridSearchService.search(\n\t\t\t\t\"\",\n\t\t\t\teffectiveSearchLimit,\n\t\t\t\t{\n\t\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t\t}\n\t\t\t);\n\t\t\trecords = hybridSearchResponse.results.map((r) => ({\n\t\t\t\t...r,\n\t\t\t\tsubject: r.subject,\n\t\t\t\tnote: r.content,\n\t\t\t\tentry_date_real: r.created_at,\n\t\t\t}));\n\t\t\tsearchMethod = \"analytical_fallback\";\n\t\t\tsearchStats = hybridSearchResponse.stats;\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Retrieved ${records.length} records matching filters for analytical analysis`\n\t\t\t);\n\t\t} else {\n\t\t\tconst hybridSearchResponse = await HybridSearchService.search(\n\t\t\t\tqueryForSearch,\n\t\t\t\teffectiveSearchLimit,\n\t\t\t\t{\n\t\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t\t}\n\t\t\t);\n\t\t\trecords = hybridSearchResponse.results.map((r) => ({\n\t\t\t\t...r,\n\t\t\t\tsubject: r.subject,\n\t\t\t\tnote: r.content,\n\t\t\t\tentry_date_real: r.created_at,\n\t\t\t}));\n\t\t\t// Map RRF search methods to expected types\n\t\t\tconst methodMap: Record<\n\t\t\t\tstring,\n\t\t\t\t| \"hybrid\"\n\t\t\t\t| \"semantic_fallback\"\n\t\t\t\t| \"tsvector_only\"\n\t\t\t\t| \"recent_files\"\n\t\t\t\t| \"analytical_fallback\"\n\t\t\t> = {\n\t\t\t\thybrid: \"hybrid\",\n\t\t\t\tsemantic_fallback: \"semantic_fallback\",\n\t\t\t\ttsvector_only: \"tsvector_only\",\n\t\t\t\tvector_only: \"semantic_fallback\", // Map vector_only to semantic_fallback\n\t\t\t\tkeyword_only: \"tsvector_only\", // Map keyword_only to tsvector_only\n\t\t\t};\n\t\t\tsearchMethod = methodMap[hybridSearchResponse.searchMethod] || \"hybrid\";\n\t\t\tsearchStats = hybridSearchResponse.stats;\n\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Hybrid search completed. Method: ${searchMethod}, Found: ${records.length} records.`\n\t\t\t);\n\t\t}\n\t}\n\n\t// Fallback for analytical queries: if no records found, retrieve all records for analysis\n\tif (queryType === \"analytical_query\" && records.length === 0) {\n\t\tconsole.log(\n\t\t\t`[CHAT ANALYSIS] Analytical query returned 0 records, falling back to all records for analysis`\n\t\t);\n\t\tconst configuredLimit = await getSettingInt(\"ai.search.limit\", 30);\n\t\tconst effectiveLimit = Math.min(configuredLimit, 200); // Cap at 200 for analytical queries\n\n\t\tconst allRecordsResponse = await HybridSearchService.search(\n\t\t\t\"\",\n\t\t\teffectiveLimit,\n\t\t\t{\n\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t}\n\t\t);\n\t\trecords = allRecordsResponse.results.map((r) => ({\n\t\t\t...r,\n\t\t\tcategory: r.subject,\n\t\t\tnote: r.content,\n\t\t\tentry_date_real: r.created_at,\n\t\t}));\n\t\tsearchMethod = \"analytical_fallback\";\n\t\tsearchStats = allRecordsResponse.stats;\n\t\tconsole.log(\n\t\t\t`[CHAT ANALYSIS] Fallback retrieved ${records.length} records for analytical analysis`\n\t\t);\n\t}\n\n\t// Normal processing is more efficient for all queries\n\tlet aiResponse;\n\tlet contextTime = 0;\n\tlet aiTime = 0;\n\tlet context = \"\";\n\n\t// Use chunked processing for analytical queries with many records\n\tif (analysis.queryType === \"analytical_query\" && records.length > 20) {\n\t\tconsole.log(\n\t\t\t`[CHAT PROCESSING] Using chunked processing for ${records.length} records`\n\t\t);\n\t\tconst chunkedTiming = timeStart(\"Chunked Processing\");\n\t\taiResponse = await processChunkedAnalyticalQuery(\n\t\t\tquestion,\n\t\t\trecords,\n\t\t\tconversationHistory\n\t\t);\n\t\tcontextTime = timeEnd(\"Chunked Processing\", chunkedTiming);\n\t\tconsole.log(\n\t\t\t`[CHAT PROCESSING] Chunked processing completed in ${contextTime}ms`\n\t\t);\n\t\taiTime = contextTime; // For chunked, aiTime is included\n\t} else {\n\t\t// Use normal processing for other queries or small record sets\n\t\tconsole.log(\n\t\t\t`[CHAT PROCESSING] Starting context preparation for ${records.length} records`\n\t\t);\n\t\tconst contextTiming = timeStart(\"Context Preparation\");\n\n\t\t// Use normal processing for all queries (relevance extraction disabled)\n\t\tcontext = prepareContextForAI(records, queryForSearch, false);\n\n\t\tcontextTime = timeEnd(\"Context Preparation\", contextTiming);\n\t\tconsole.log(`[CHAT PROCESSING] Context size: ${context.length} characters`);\n\n\t\t// Generate AI response\n\t\tconsole.log(`[CHAT PROCESSING] Starting AI response generation`);\n\t\tconst aiTiming = timeStart(\"AI Response Generation\");\n\t\taiResponse = await generateAIResponse(\n\t\t\tquestion,\n\t\t\tcontext,\n\t\t\tconversationHistory,\n\t\t\tanalysis.queryType\n\t\t);\n\n\t\taiTime = timeEnd(\"AI Response Generation\", aiTiming);\n\t\tconsole.log(\n\t\t\t`[TOKENS] Response phase — input: ${aiResponse.inputTokens}, output: ${aiResponse.outputTokens}`\n\t\t);\n\t}\n\n\t// Extract sources from the context that were used in the AI response\n\tconsole.log(`[CHAT PROCESSING] Starting source extraction`);\n\tconst sourceTiming = timeStart(\"Source Extraction\");\n\n\t// Common words to exclude from matching (expanded list)\n\tconst commonWords = new Set([\n\t\t\"this\",\n\t\t\"that\",\n\t\t\"with\",\n\t\t\"from\",\n\t\t\"they\",\n\t\t\"were\",\n\t\t\"been\",\n\t\t\"have\",\n\t\t\"file\",\n\t\t\"record\",\n\t\t\"document\",\n\t\t\"date\",\n\t\t\"time\",\n\t\t\"place\",\n\t\t\"report\",\n\t\t\"against\",\n\t\t\"under\",\n\t\t\"about\",\n\t\t\"information\",\n\t\t\"data\",\n\t]);\n\n\t// Normalize function: remove extension, replace underscores/dashes with spaces\n\tconst normalize = (s: string) =>\n\t\ts\n\t\t\t.toLowerCase()\n\t\t\t.replace(/\\.[^/.]+$/, \"\") // Remove file extension\n\t\t\t.replace(/[_-]/g, \" \") // Replace underscores and dashes with spaces\n\t\t\t.replace(/\\s+/g, \" \") // Normalize multiple spaces to single space\n\t\t\t.trim();\n\n\t/**\n\t * Score chunk by query intent - determines how well a chunk answers the question type\n\t */\n\tfunction scoreChunkByQueryIntent(\n\t\tchunkContent: string,\n\t\tquery: string,\n\t\tqueryType: string\n\t): number {\n\t\tif (!chunkContent) return 0;\n\n\t\tconst contentLower = chunkContent.toLowerCase();\n\t\tconst queryLower = query.toLowerCase();\n\n\t\t// Extract question words\n\t\tconst questionWords = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"];\n\t\tconst questionWord = questionWords.find((w) => queryLower.startsWith(w));\n\n\t\tlet intentScore = 0;\n\n\t\tif (questionWord === \"who\") {\n\t\t\t// Look for names, titles, people identifiers\n\t\t\tconst namePatterns =\n\t\t\t\t/\\b(mr|mrs|ms|miss|dr|prof|professor|sir|madam)\\s+\\w+/gi;\n\t\t\tif (namePatterns.test(contentLower)) intentScore += 5;\n\n\t\t\t// Look for age indicators (often associated with people)\n\t\t\tif (/\\b(age|years?\\s*old|aged)\\b/i.test(contentLower)) intentScore += 2;\n\n\t\t\t// Look for person-related keywords\n\t\t\tconst personKeywords = [\n\t\t\t\t\"victim\",\n\t\t\t\t\"accused\",\n\t\t\t\t\"suspect\",\n\t\t\t\t\"person\",\n\t\t\t\t\"individual\",\n\t\t\t\t\"man\",\n\t\t\t\t\"woman\",\n\t\t\t\t\"child\",\n\t\t\t\t\"national\",\n\t\t\t];\n\t\t\tpersonKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"what\") {\n\t\t\t// Look for definitions, descriptions, events\n\t\t\tconst definitionPatterns = /\\b(is|are|was|were|means?|refers?\\s+to)\\b/i;\n\t\t\tif (definitionPatterns.test(contentLower)) intentScore += 3;\n\n\t\t\t// Look for event descriptions\n\t\t\tconst eventKeywords = [\n\t\t\t\t\"happened\",\n\t\t\t\t\"occurred\",\n\t\t\t\t\"incident\",\n\t\t\t\t\"event\",\n\t\t\t\t\"case\",\n\t\t\t\t\"reported\",\n\t\t\t];\n\t\t\teventKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"when\") {\n\t\t\t// Look for dates, time references\n\t\t\tconst datePatterns =\n\t\t\t\t/\\b(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}|\\d{4}|\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec))/gi;\n\t\t\tif (datePatterns.test(contentLower)) intentScore += 5;\n\n\t\t\t// Look for time indicators\n\t\t\tconst timeKeywords = [\n\t\t\t\t\"at around\",\n\t\t\t\t\"at approximately\",\n\t\t\t\t\"on\",\n\t\t\t\t\"date\",\n\t\t\t\t\"time\",\n\t\t\t\t\"am\",\n\t\t\t\t\"pm\",\n\t\t\t\t\"hours?\",\n\t\t\t];\n\t\t\ttimeKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"where\") {\n\t\t\t// Look for locations, places\n\t\t\tconst locationKeywords = [\n\t\t\t\t\"location\",\n\t\t\t\t\"place\",\n\t\t\t\t\"district\",\n\t\t\t\t\"area\",\n\t\t\t\t\"region\",\n\t\t\t\t\"border\",\n\t\t\t\t\"near\",\n\t\t\t\t\"at\",\n\t\t\t\t\"in\",\n\t\t\t\t\"found\",\n\t\t\t];\n\t\t\tlocationKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\n\t\t\t// Look for location patterns (capitalized words often indicate places)\n\t\t\tconst capitalizedWords = contentLower.match(/\\b[A-Z][a-z]+\\b/g);\n\t\t\tif (capitalizedWords && capitalizedWords.length > 2) intentScore += 2;\n\t\t} else if (questionWord === \"why\" || questionWord === \"how\") {\n\t\t\t// Look for explanations, reasons, methods\n\t\t\tconst explanationKeywords = [\n\t\t\t\t\"because\",\n\t\t\t\t\"due to\",\n\t\t\t\t\"reason\",\n\t\t\t\t\"caused\",\n\t\t\t\t\"result\",\n\t\t\t\t\"method\",\n\t\t\t\t\"process\",\n\t\t\t\t\"by\",\n\t\t\t];\n\t\t\texplanationKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t}\n\n\t\t// Boost score if chunk content directly contains query keywords (excluding common words)\n\t\tconst queryWords = queryLower\n\t\t\t.split(/\\s+/)\n\t\t\t.filter((w) => w.length > 3 && !commonWords.has(w));\n\t\tconst matchingQueryWords = queryWords.filter((word) =>\n\t\t\tcontentLower.includes(word)\n\t\t);\n\t\tif (matchingQueryWords.length > 0) {\n\t\t\tintentScore += matchingQueryWords.length * 0.5;\n\t\t}\n\n\t\treturn intentScore;\n\t}\n\n\t/**\n\t * Calculate combined relevance score for chunk selection during deduplication\n\t */\n\tfunction calculateChunkRelevance(\n\t\tsource: any,\n\t\tquery: string,\n\t\tqueryType: string\n\t): number {\n\t\t// Get chunk content (stored as note in the record)\n\t\tconst chunkContent = source.chunkContent || source.note || \"\";\n\n\t\t// Query intent score (0-10+)\n\t\tconst intentScore = scoreChunkByQueryIntent(chunkContent, query, queryType);\n\n\t\t// Semantic similarity (0-1, converted to 0-10 scale)\n\t\tconst semanticScore =\n\t\t\t(source.semanticSimilarity || source.similarity || 0) * 10;\n\n\t\t// RRF score (0-1, converted to 0-5 scale)\n\t\tconst rrfScore = (source.rrfScore || 0) * 5;\n\n\t\t// Combined score: intent (40%) + semantic (40%) + RRF (20%)\n\t\t// This prioritizes chunks that answer the question over chunks with high keyword matches\n\t\tconst combinedScore =\n\t\t\tintentScore * 0.4 + semanticScore * 0.4 + rrfScore * 0.2;\n\n\t\treturn combinedScore;\n\t}\n\n\tconst citedSources = records\n\t\t.map((record) => {\n\t\t\tconst response = aiResponse.text.toLowerCase();\n\t\t\tconst title = record.title.toLowerCase();\n\t\t\tconst normalizedResponse = normalize(aiResponse.text);\n\t\t\tconst normalizedTitle = normalize(record.title);\n\t\t\tlet score = 0;\n\n\t\t\t// Strong signal: Exact title match (primary citation method)\n\t\t\tif (response.includes(title)) {\n\t\t\t\tscore += 10;\n\t\t\t}\n\n\t\t\t// Strong signal: Normalized title match (handles cleaned up filenames)\n\t\t\tif (normalizedResponse.includes(normalizedTitle)) {\n\t\t\t\tscore += 8; // High score for normalized match\n\t\t\t}\n\n\t\t\t// Strong signal: Title in citation format (Source: Title)\n\t\t\tif (\n\t\t\t\tresponse.includes(`(Source: ${record.title})`) ||\n\t\t\t\tresponse.includes(`(source: ${record.title})`)\n\t\t\t) {\n\t\t\t\tscore += 10;\n\t\t\t}\n\n\t\t\t// Medium signal: Normalized title in citation format\n\t\t\tif (normalizedResponse.includes(`(source: ${normalizedTitle})`)) {\n\t\t\t\tscore += 8;\n\t\t\t}\n\n\t\t\t// Medium signal: Explicit ID mention (fallback for legacy)\n\t\t\tif (\n\t\t\t\tresponse.includes(`id: ${record.id}`) ||\n\t\t\t\tresponse.includes(`record ${record.id}`) ||\n\t\t\t\tresponse.includes(`[${record.id}]`)\n\t\t\t) {\n\t\t\t\tscore += 5;\n\t\t\t}\n\n\t\t\t// Medium signal: Most title words present\n\t\t\tconst titleWords = title\n\t\t\t\t.split(/\\s+/)\n\t\t\t\t.filter((word) => word.length > 3 && !commonWords.has(word));\n\t\t\tconst titleMatches = titleWords.filter((word) => response.includes(word));\n\t\t\tif (titleMatches.length >= Math.min(titleWords.length * 0.6, 3)) {\n\t\t\t\tscore += 3;\n\t\t\t}\n\n\t\t\t// Weak signal: Unique keywords from content\n\t\t\tif (record.note) {\n\t\t\t\tconst noteWords = record.note\n\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t.split(/\\s+/)\n\t\t\t\t\t.filter(\n\t\t\t\t\t\t(word) =>\n\t\t\t\t\t\t\tword.length > 5 && // Longer words are more unique\n\t\t\t\t\t\t\t!commonWords.has(word)\n\t\t\t\t\t)\n\t\t\t\t\t.slice(0, 15); // Check first 15 words\n\n\t\t\t\tconst noteMatches = noteWords.filter((word) => response.includes(word));\n\t\t\t\t// Need at least 3 unique keywords to be confident\n\t\t\t\tif (noteMatches.length >= 3) {\n\t\t\t\t\tscore += 2;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn {\n\t\t\t\tid: record.id,\n\t\t\t\ttitle: record.title,\n\t\t\t\trelevance: record.rrf_score || record.combined_score || record.rank,\n\t\t\t\tsimilarity: record.rrf_score\n\t\t\t\t\t? record.rrf_score * 30\n\t\t\t\t\t: record.semantic_similarity, // Convert RRF to similarity-like value for UI\n\t\t\t\tcitation: record.citation, // Pass through citation data from hybrid search\n\t\t\t\tcitationScore: score,\n\t\t\t\trrfScore: record.rrf_score || 0, // Store RRF score for filtering\n\t\t\t\tchunkContent: record.note || \"\", // Store chunk content for query-intent matching\n\t\t\t\tsemanticSimilarity: record.semantic_similarity || 0, // Store semantic similarity\n\t\t\t};\n\t\t})\n\t\t.filter((source) => {\n\t\t\t// Apply stricter filtering:\n\t\t\t// 1. Minimum citation score (must be explicitly mentioned or strongly matched)\n\t\t\t// 2. Minimum RRF score threshold (filter out weak semantic matches)\n\t\t\tconst minCitationScore = 5; // Increased from 3 to 5\n\t\t\tconst minRrfScore = 0.01; // Minimum RRF score (approximately top 60 results)\n\n\t\t\treturn (\n\t\t\t\tsource.citationScore >= minCitationScore &&\n\t\t\t\tsource.rrfScore >= minRrfScore\n\t\t\t);\n\t\t})\n\t\t// Deduplicate by file ID (keep the chunk that best answers the query)\n\t\t.reduce((acc: any[], source) => {\n\t\t\tconst existing = acc.find((s) => s.id === source.id);\n\t\t\tif (!existing) {\n\t\t\t\tacc.push(source);\n\t\t\t} else {\n\t\t\t\t// NEW: Check if chunk content actually appears in AI response\n\t\t\t\t// This ensures we cite the page that contains the actual answer\n\t\t\t\tconst aiResponseLower = aiResponse.text.toLowerCase();\n\t\t\t\tconst existingContent = (existing.chunkContent || \"\").toLowerCase();\n\t\t\t\tconst sourceContent = (source.chunkContent || \"\").toLowerCase();\n\n\t\t\t\t// Extract key phrases from AI response (words/phrases that might be quoted)\n\t\t\t\tconst extractKeyPhrases = (text: string): string[] => {\n\t\t\t\t\t// Extract quoted text, capitalized phrases, and significant words\n\t\t\t\t\tconst quoted = text.match(/\"([^\"]+)\"/g) || [];\n\t\t\t\t\tconst capitalized =\n\t\t\t\t\t\ttext.match(/\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b/g) || [];\n\t\t\t\t\t// Extract bullet points or list items (often contain specific answers)\n\t\t\t\t\tconst bulletPoints = text.match(/[•\\*\\-]\\s*([^\\n]+)/g) || [];\n\t\t\t\t\t// Extract text after colons (often contains specific answers)\n\t\t\t\t\tconst afterColons = text.match(/:\\s*([^\\n]+)/g) || [];\n\t\t\t\t\tconst significantWords = text\n\t\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t\t.split(/\\s+/)\n\t\t\t\t\t\t.filter((w) => w.length > 3 && !commonWords.has(w))\n\t\t\t\t\t\t.slice(0, 15);\n\n\t\t\t\t\treturn [\n\t\t\t\t\t\t...quoted.map((q) => q.replace(/\"/g, \"\").toLowerCase()),\n\t\t\t\t\t\t...capitalized.map((c) => c.toLowerCase()),\n\t\t\t\t\t\t...bulletPoints.map((b) =>\n\t\t\t\t\t\t\tb\n\t\t\t\t\t\t\t\t.replace(/[•\\*\\-]\\s*/, \"\")\n\t\t\t\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t\t\t\t.trim()\n\t\t\t\t\t\t),\n\t\t\t\t\t\t...afterColons.map((a) =>\n\t\t\t\t\t\t\ta.replace(/:\\s*/, \"\").toLowerCase().trim()\n\t\t\t\t\t\t),\n\t\t\t\t\t\t...significantWords,\n\t\t\t\t\t].filter((phrase) => phrase.length > 2); // Filter out very short phrases\n\t\t\t\t};\n\n\t\t\t\tconst keyPhrases = extractKeyPhrases(aiResponse.text);\n\n\t\t\t\t// Count how many key phrases appear in each chunk\n\t\t\t\tconst existingMatches = keyPhrases.filter((phrase) =>\n\t\t\t\t\texistingContent.includes(phrase)\n\t\t\t\t).length;\n\t\t\t\tconst sourceMatches = keyPhrases.filter((phrase) =>\n\t\t\t\t\tsourceContent.includes(phrase)\n\t\t\t\t).length;\n\n\t\t\t\t// Prefer chunk with more matching phrases from AI response\n\t\t\t\tif (sourceMatches > existingMatches) {\n\t\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\t\tacc[index] = source;\n\t\t\t\t} else if (sourceMatches === existingMatches && sourceMatches > 0) {\n\t\t\t\t\t// If same number of matches, use relevance score as tiebreaker\n\t\t\t\t\tconst existingRelevance = calculateChunkRelevance(\n\t\t\t\t\t\texisting,\n\t\t\t\t\t\tqueryForSearch,\n\t\t\t\t\t\tqueryType\n\t\t\t\t\t);\n\t\t\t\t\tconst sourceRelevance = calculateChunkRelevance(\n\t\t\t\t\t\tsource,\n\t\t\t\t\t\tqueryForSearch,\n\t\t\t\t\t\tqueryType\n\t\t\t\t\t);\n\n\t\t\t\t\tif (sourceRelevance > existingRelevance) {\n\t\t\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\t\t\tacc[index] = source;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Fallback to original logic if no content matches\n\t\t\t\t\tconst existingRelevance = calculateChunkRelevance(\n\t\t\t\t\t\texisting,\n\t\t\t\t\t\tqueryForSearch,\n\t\t\t\t\t\tqueryType\n\t\t\t\t\t);\n\t\t\t\t\tconst sourceRelevance = calculateChunkRelevance(\n\t\t\t\t\t\tsource,\n\t\t\t\t\t\tqueryForSearch,\n\t\t\t\t\t\tqueryType\n\t\t\t\t\t);\n\n\t\t\t\t\tif (sourceRelevance > existingRelevance) {\n\t\t\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\t\t\tacc[index] = source;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn acc;\n\t\t}, [])\n\t\t// Sort by combined score: citation score first, then RRF score\n\t\t.sort((a, b) => {\n\t\t\t// Primary sort: citation score (how explicitly mentioned)\n\t\t\tif (b.citationScore !== a.citationScore) {\n\t\t\t\treturn b.citationScore - a.citationScore;\n\t\t\t}\n\t\t\t// Secondary sort: RRF score (relevance to query)\n\t\t\treturn b.rrfScore - a.rrfScore;\n\t\t})\n\t\t// Limit to top 10 most relevant sources\n\t\t.slice(0, 10)\n\t\t.map(\n\t\t\t({\n\t\t\t\tcitationScore,\n\t\t\t\trrfScore,\n\t\t\t\tchunkContent,\n\t\t\t\tsemanticSimilarity,\n\t\t\t\t...rest\n\t\t\t}) => rest\n\t\t); // Remove internal scores from final output\n\n\tconsole.log(\n\t\t`[CITATION-DEBUG] Created ${citedSources.length} sources, sample:`,\n\t\tcitedSources[0]\n\t);\n\n\tconst sourceTime = timeEnd(\"Source Extraction\", sourceTiming);\n\n\tconsole.log(\n\t\t`[CHAT ANALYSIS] Generated response with ${citedSources.length} cited sources out of ${records.length} total records used in context.`\n\t);\n\tconsole.log(\n\t\t`[ADMIN CHAT] Response generated with ${citedSources.length} sources`\n\t);\n\n\t// Log overall processing time breakdown\n\tconsole.log(`[TIMING-SUMMARY] Post-search processing breakdown:`);\n\tconsole.log(`[TIMING-SUMMARY] - Context preparation: ${contextTime}ms`);\n\tconsole.log(`[TIMING-SUMMARY] - AI response generation: ${aiTime}ms`);\n\tconsole.log(`[TIMING-SUMMARY] - Source extraction: ${sourceTime}ms`);\n\tconsole.log(\n\t\t`[TIMING-SUMMARY] - Total post-search time: ${contextTime + aiTime + sourceTime\n\t\t}ms`\n\t);\n\n\treturn {\n\t\tresponse: aiResponse.text,\n\t\tsources: citedSources,\n\t\tsearchQuery: queryForSearch,\n\t\tsearchMethod,\n\t\tqueryType,\n\t\tanalysisUsed,\n\t\ttokenCount: {\n\t\t\tinput: (analysisInputTokens || 0) + (aiResponse.inputTokens || 0),\n\t\t\toutput: (analysisOutputTokens || 0) + (aiResponse.outputTokens || 0),\n\t\t},\n\t\tstats: searchStats,\n\t\tchartData: (aiResponse as any).chartData,\n\t};\n}\n\n/**\n * Streaming version of processChatMessageEnhanced\n * Yields chunks as they arrive from the AI, along with metadata\n */\nexport async function* processChatMessageEnhancedStream(\n\tquestion: string,\n\tconversationHistory: ChatMessage[] = [],\n\tsearchLimit?: number,\n\tuseEnhancedSearch: boolean = true,\n\topts: { provider?: \"gemini\"; model?: string; keyId?: number } = {},\n\tfilters?: {\n\t\tboardId?: string;\n\t\tsubjectId?: number;\n\t\tchapterId?: number;\n\t}\n): AsyncGenerator<\n\t{\n\t\ttype: \"metadata\" | \"token\" | \"sources\" | \"done\" | \"progress\" | \"data\";\n\t\ttext?: string;\n\t\tsources?: Array<{\n\t\t\tid: number;\n\t\t\ttitle: string;\n\t\t\trelevance?: number;\n\t\t\tsimilarity?: number; // For UI display (converted from RRF score or semantic_similarity)\n\t\t\tcitation?: {\n\t\t\t\tpageNumber: number;\n\t\t\t\timageUrl: string;\n\t\t\t\tboundingBox: any;\n\t\t\t};\n\t\t}>;\n\t\tsearchQuery?: string;\n\t\tsearchMethod?: string;\n\t\tqueryType?: string;\n\t\tanalysisUsed?: boolean;\n\t\ttokenCount?: { input: number; output: number };\n\t\tstats?: any;\n\t\tprogress?: string;\n\t\tchartData?: any;\n\t},\n\tvoid,\n\tunknown\n> {\n\t// All the same setup logic as processChatMessageEnhanced\n\tlet queryForSearch = question;\n\tlet queryType = \"specific_search\";\n\tlet analysisUsed = false;\n\tlet analysisInputTokens = 0;\n\tlet analysisOutputTokens = 0;\n\tlet analysis: any = null;\n\n\ttry {\n\t\tanalysis = await analyzeQueryForSearch(question, conversationHistory, opts);\n\t\tconsole.log(\"[CHAT ANALYSIS] Processing query:\", `\"${question}\"`);\n\t\tconsole.log(\n\t\t\t`[CHAT ANALYSIS] AI Analysis Result:`,\n\t\t\tJSON.stringify(analysis, null, 2)\n\t\t);\n\t\tanalysisInputTokens = analysis.inputTokens || 0;\n\t\tanalysisOutputTokens = analysis.outputTokens || 0;\n\t\tanalysisUsed = true;\n\t\tqueryForSearch =\n\t\t\tanalysis.coreSearchTerms || analysis.instructionalTerms || \"\";\n\t\tqueryType = analysis.queryType;\n\n\t\t// Special handling for queries that want to show all records\n\t\tif (\n\t\t\t!queryForSearch &&\n\t\t\tanalysis.instructionalTerms?.toLowerCase().includes(\"all\")\n\t\t) {\n\t\t\tqueryForSearch = \"\"; // Empty query to match all records\n\t\t\tconsole.log(\n\t\t\t\t'[CHAT ANALYSIS] Using empty search term for \"show all\" query'\n\t\t\t);\n\t\t}\n\n\t\t// Handle list_all queries\n\t\tif (analysis.queryType === \"list_all\") {\n\t\t\tqueryForSearch = \"\";\n\t\t\tconsole.log(\n\t\t\t\t'[CHAT ANALYSIS] Query type is \"list_all\", returning all records'\n\t\t\t);\n\t\t}\n\t} catch (error) {\n\t\tconsole.error(\"[CHAT ANALYSIS] Query analysis failed:\", error);\n\t\tqueryForSearch = question;\n\t}\n\n\t// Search for relevant records\n\tlet records: SearchResult[] = [];\n\tlet searchMethod: string = \"hybrid\";\n\tlet searchStats: any = {};\n\n\tconst configuredLimit = await getSettingInt(\"ai.search.limit\", 30);\n\tconst effectiveSearchLimit = Math.min(searchLimit || configuredLimit, 200);\n\n\tconsole.log(\n\t\t`[CHAT ANALYSIS] Effective search limit: ${effectiveSearchLimit}`\n\t);\n\n\tif (useEnhancedSearch) {\n\t\tconst hasFilters = filters && (filters.boardId || filters.subjectId);\n\t\tif (queryType === \"analytical_query\" && hasFilters) {\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Analytical query with filters - retrieving all records matching filters for complete analysis`\n\t\t\t);\n\t\t\tconst hybridSearchResponse = await HybridSearchService.search(\n\t\t\t\t\"\",\n\t\t\t\teffectiveSearchLimit,\n\t\t\t\t{\n\t\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t\t}\n\t\t\t);\n\t\t\trecords = hybridSearchResponse.results.map((r) => ({\n\t\t\t\t...r,\n\t\t\t\tsubject: r.subject,\n\t\t\t\tnote: r.content,\n\t\t\t\tentry_date_real: r.created_at,\n\t\t\t}));\n\t\t\tsearchMethod = \"analytical_fallback\";\n\t\t\tsearchStats = hybridSearchResponse.stats;\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Retrieved ${records.length} records matching filters for analytical analysis`\n\t\t\t);\n\t\t} else {\n\t\t\tconst hybridSearchResponse = await HybridSearchService.search(\n\t\t\t\tqueryForSearch,\n\t\t\t\teffectiveSearchLimit,\n\t\t\t\t{\n\t\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t\t}\n\t\t\t);\n\t\t\trecords = hybridSearchResponse.results.map((r) => ({\n\t\t\t\t...r,\n\t\t\t\tsubject: r.subject,\n\t\t\t\tnote: r.content,\n\t\t\t\tentry_date_real: r.created_at,\n\t\t\t}));\n\t\t\t// Map RRF search methods to expected types\n\t\t\tconst methodMap: Record<\n\t\t\t\tstring,\n\t\t\t\t| \"hybrid\"\n\t\t\t\t| \"semantic_fallback\"\n\t\t\t\t| \"tsvector_only\"\n\t\t\t\t| \"recent_files\"\n\t\t\t\t| \"analytical_fallback\"\n\t\t\t> = {\n\t\t\t\thybrid: \"hybrid\",\n\t\t\t\tsemantic_fallback: \"semantic_fallback\",\n\t\t\t\ttsvector_only: \"tsvector_only\",\n\t\t\t\tvector_only: \"semantic_fallback\", // Map vector_only to semantic_fallback\n\t\t\t\tkeyword_only: \"tsvector_only\", // Map keyword_only to tsvector_only\n\t\t\t};\n\t\t\tsearchMethod = methodMap[hybridSearchResponse.searchMethod] || \"hybrid\";\n\t\t\tsearchStats = hybridSearchResponse.stats;\n\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT ANALYSIS] Hybrid search completed. Method: ${searchMethod}, Found: ${records.length} records.`\n\t\t\t);\n\t\t}\n\t}\n\n\t// Fallback for analytical queries\n\tif (queryType === \"analytical_query\" && records.length === 0) {\n\t\tconsole.log(\n\t\t\t`[CHAT ANALYSIS] Analytical query returned 0 records, falling back to all records for analysis`\n\t\t);\n\t\tconst configuredLimit = await getSettingInt(\"ai.search.limit\", 30);\n\t\tconst effectiveLimit = Math.min(configuredLimit, 200);\n\n\t\tconst allRecordsResponse = await HybridSearchService.search(\n\t\t\t\"\",\n\t\t\teffectiveLimit,\n\t\t\t{\n\t\t\t\tboardId: filters?.boardId || \"CBSE\",\n\t\t\t\tsubjectId: filters?.subjectId,\n\t\t\t\tchapterId: filters?.chapterId,\n\t\t\t}\n\t\t);\n\t\trecords = allRecordsResponse.results.map((r) => ({\n\t\t\t...r,\n\t\t\tcategory: r.subject,\n\t\t\tnote: r.content,\n\t\t\tentry_date_real: r.created_at,\n\t\t}));\n\t\tsearchMethod = \"analytical_fallback\";\n\t\tsearchStats = allRecordsResponse.stats;\n\t\tconsole.log(\n\t\t\t`[CHAT ANALYSIS] Fallback retrieved ${records.length} records for analytical analysis`\n\t\t);\n\t}\n\n\t// Yield progress after search completes\n\tyield {\n\t\ttype: \"progress\",\n\t\tprogress: `Found ${records.length} record${records.length !== 1 ? \"s\" : \"\"\n\t\t\t}. Preparing response...`,\n\t};\n\n\t// Yield metadata\n\tyield {\n\t\ttype: \"metadata\",\n\t\tsearchQuery: queryForSearch,\n\t\tsearchMethod,\n\t\tqueryType,\n\t\tanalysisUsed,\n\t\tstats: searchStats,\n\t};\n\n\t// Check if we need chunked processing for large analytical queries\n\tlet fullResponseText = \"\";\n\tlet aiInputTokens = 0;\n\tlet aiOutputTokens = 0;\n\n\tif (queryType === \"analytical_query\" && records.length > 20) {\n\t\t// Use chunked processing for large analytical queries\n\t\t// This is more efficient than streaming for large datasets\n\t\tconsole.log(\n\t\t\t`[CHAT PROCESSING] Using chunked processing for ${records.length} records (streaming disabled for large analytical queries)`\n\t\t);\n\n\t\ttry {\n\t\t\t// Calculate chunk count for progress display\n\t\t\tconst CHUNK_SIZE = 15;\n\t\t\tconst chunkCount = Math.ceil(records.length / CHUNK_SIZE);\n\n\t\t\t// Yield initial progress\n\t\t\tyield {\n\t\t\t\ttype: \"progress\",\n\t\t\t\tprogress: `Processing ${records.length} records in ${chunkCount} chunks...`,\n\t\t\t};\n\n\t\t\tconst chunkedResponse = await processChunkedAnalyticalQuery(\n\t\t\t\tquestion,\n\t\t\t\trecords,\n\t\t\t\tconversationHistory\n\t\t\t);\n\n\t\t\tfullResponseText = chunkedResponse.text;\n\t\t\taiInputTokens = chunkedResponse.inputTokens || 0;\n\t\t\taiOutputTokens = chunkedResponse.outputTokens || 0;\n\n\t\t\t// Yield the complete response as one chunk\n\t\t\tyield { type: \"token\", text: fullResponseText };\n\n\t\t\tconsole.log(\n\t\t\t\t`[CHAT PROCESSING] Chunked processing completed for ${records.length} records`\n\t\t\t);\n\t\t} catch (error) {\n\t\t\tconsole.error(\"[CHAT PROCESSING] Chunked processing error:\", error);\n\t\t\tthrow error;\n\t\t}\n\t} else if (queryType === \"visualization\") {\n\t\t// Handle visualization queries using non-streaming generation to get structured data\n\t\tconsole.log(`[CHAT PROCESSING] Handling visualization query`);\n\n\t\t// Yield progress\n\t\tyield {\n\t\t\ttype: \"progress\",\n\t\t\tprogress: \"Generating chart...\",\n\t\t};\n\n\t\tlet context = \"\";\n\t\tif (records.length > 0) {\n\t\t\tcontext = prepareContextForAI(records, queryForSearch, false);\n\t\t}\n\n\t\ttry {\n\t\t\t// Call non-streaming generation to get chart data\n\t\t\tconsole.log(\n\t\t\t\t\"[CHART STREAM] Calling generateAIResponse for visualization\"\n\t\t\t);\n\t\t\tconst response = await generateAIResponse(\n\t\t\t\tquestion,\n\t\t\t\tcontext,\n\t\t\t\tconversationHistory,\n\t\t\t\tqueryType,\n\t\t\t\topts\n\t\t\t);\n\n\t\t\tconsole.log(\n\t\t\t\t\"[CHART STREAM] Response received, has chartData:\",\n\t\t\t\t!!response.chartData\n\t\t\t);\n\t\t\tfullResponseText = response.text;\n\t\t\taiInputTokens = response.inputTokens;\n\t\t\taiOutputTokens = response.outputTokens;\n\n\t\t\t// Yield the text response\n\t\t\tyield { type: \"token\", text: fullResponseText };\n\n\t\t\t// Yield chart data if available\n\t\t\tif (response.chartData) {\n\t\t\t\tconsole.log(\n\t\t\t\t\t\"[CHART STREAM] Yielding chart data:\",\n\t\t\t\t\tJSON.stringify(response.chartData, null, 2)\n\t\t\t\t);\n\t\t\t\tyield {\n\t\t\t\t\ttype: \"data\",\n\t\t\t\t\tchartData: response.chartData,\n\t\t\t\t};\n\t\t\t} else {\n\t\t\t\tconsole.log(\n\t\t\t\t\t\"[CHART STREAM] No chart data in response - chart generation may have failed\"\n\t\t\t\t);\n\t\t\t}\n\n\t\t\t// Yield done event\n\t\t\tyield {\n\t\t\t\ttype: \"done\",\n\t\t\t\ttokenCount: { input: aiInputTokens, output: aiOutputTokens },\n\t\t\t};\n\t\t} catch (error) {\n\t\t\tconsole.error(\"[CHAT PROCESSING] Visualization error:\", error);\n\t\t\tthrow error;\n\t\t}\n\t} else {\n\t\t// Use normal streaming for small queries or non-analytical queries\n\t\tlet context = \"\";\n\t\tconsole.log(\n\t\t\t`[CHAT PROCESSING] Starting context preparation for ${records.length} records`\n\t\t);\n\n\t\t// Yield progress for context preparation\n\t\tyield {\n\t\t\ttype: \"progress\",\n\t\t\tprogress: `Preparing context from ${records.length} record${records.length !== 1 ? \"s\" : \"\"\n\t\t\t\t}...`,\n\t\t};\n\n\t\tcontext = prepareContextForAI(records, queryForSearch, false);\n\t\tconsole.log(`[CHAT PROCESSING] Context size: ${context.length} characters`);\n\n\t\t// Stream AI response\n\t\tconsole.log(`[CHAT PROCESSING] Starting AI response streaming`);\n\n\t\t// Yield progress before AI generation starts\n\t\tyield {\n\t\t\ttype: \"progress\",\n\t\t\tprogress: \"Generating response...\",\n\t\t};\n\n\t\ttry {\n\t\t\tfor await (const chunk of generateAIResponseStream(\n\t\t\t\tquestion,\n\t\t\t\tcontext,\n\t\t\t\tconversationHistory,\n\t\t\t\tqueryType,\n\t\t\t\topts\n\t\t\t)) {\n\t\t\t\tif (chunk.type === \"token\") {\n\t\t\t\t\tfullResponseText += chunk.text || \"\";\n\t\t\t\t\tyield { type: \"token\", text: chunk.text };\n\t\t\t\t} else if (chunk.type === \"done\") {\n\t\t\t\t\taiInputTokens = chunk.inputTokens || 0;\n\t\t\t\t\taiOutputTokens = chunk.outputTokens || 0;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (error) {\n\t\t\tconsole.error(\"[CHAT PROCESSING] Streaming error:\", error);\n\t\t\tthrow error;\n\t\t}\n\t}\n\n\t// Extract sources after streaming completes\n\tconsole.log(`[CHAT PROCESSING] Starting source extraction`);\n\tconst commonWords = new Set([\n\t\t\"this\",\n\t\t\"that\",\n\t\t\"with\",\n\t\t\"from\",\n\t\t\"they\",\n\t\t\"were\",\n\t\t\"been\",\n\t\t\"have\",\n\t\t\"file\",\n\t\t\"record\",\n\t\t\"document\",\n\t\t\"report\",\n\t\t\"information\",\n\t\t\"data\",\n\t\t\"about\",\n\t\t\"found\",\n\t\t\"created\",\n\t\t\"updated\",\n\t]);\n\n\t// Normalize function: remove extension, replace underscores/dashes with spaces\n\tconst normalize = (s: string) =>\n\t\ts\n\t\t\t.toLowerCase()\n\t\t\t.replace(/\\.[^/.]+$/, \"\") // Remove file extension\n\t\t\t.replace(/[_-]/g, \" \") // Replace underscores and dashes with spaces\n\t\t\t.replace(/\\s+/g, \" \") // Normalize multiple spaces to single space\n\t\t\t.trim();\n\n\t/**\n\t * Score chunk by query intent - determines how well a chunk answers the question type\n\t * (Same function as in non-streaming version)\n\t */\n\tfunction scoreChunkByQueryIntent(\n\t\tchunkContent: string,\n\t\tquery: string,\n\t\tqueryType: string\n\t): number {\n\t\tif (!chunkContent) return 0;\n\n\t\tconst contentLower = chunkContent.toLowerCase();\n\t\tconst queryLower = query.toLowerCase();\n\n\t\t// Extract question words\n\t\tconst questionWords = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"];\n\t\tconst questionWord = questionWords.find((w) => queryLower.startsWith(w));\n\n\t\tlet intentScore = 0;\n\n\t\tif (questionWord === \"who\") {\n\t\t\t// Look for names, titles, people identifiers\n\t\t\tconst namePatterns =\n\t\t\t\t/\\b(mr|mrs|ms|miss|dr|prof|professor|sir|madam)\\s+\\w+/gi;\n\t\t\tif (namePatterns.test(contentLower)) intentScore += 5;\n\n\t\t\t// Look for age indicators (often associated with people)\n\t\t\tif (/\\b(age|years?\\s*old|aged)\\b/i.test(contentLower)) intentScore += 2;\n\n\t\t\t// Look for person-related keywords\n\t\t\tconst personKeywords = [\n\t\t\t\t\"victim\",\n\t\t\t\t\"accused\",\n\t\t\t\t\"suspect\",\n\t\t\t\t\"person\",\n\t\t\t\t\"individual\",\n\t\t\t\t\"man\",\n\t\t\t\t\"woman\",\n\t\t\t\t\"child\",\n\t\t\t\t\"national\",\n\t\t\t];\n\t\t\tpersonKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"what\") {\n\t\t\t// Look for definitions, descriptions, events\n\t\t\tconst definitionPatterns = /\\b(is|are|was|were|means?|refers?\\s+to)\\b/i;\n\t\t\tif (definitionPatterns.test(contentLower)) intentScore += 3;\n\n\t\t\t// Look for event descriptions\n\t\t\tconst eventKeywords = [\n\t\t\t\t\"happened\",\n\t\t\t\t\"occurred\",\n\t\t\t\t\"incident\",\n\t\t\t\t\"event\",\n\t\t\t\t\"case\",\n\t\t\t\t\"reported\",\n\t\t\t];\n\t\t\teventKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"when\") {\n\t\t\t// Look for dates, time references\n\t\t\tconst datePatterns =\n\t\t\t\t/\\b(\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}|\\d{4}|\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec))/gi;\n\t\t\tif (datePatterns.test(contentLower)) intentScore += 5;\n\n\t\t\t// Look for time indicators\n\t\t\tconst timeKeywords = [\n\t\t\t\t\"at around\",\n\t\t\t\t\"at approximately\",\n\t\t\t\t\"on\",\n\t\t\t\t\"date\",\n\t\t\t\t\"time\",\n\t\t\t\t\"am\",\n\t\t\t\t\"pm\",\n\t\t\t\t\"hours?\",\n\t\t\t];\n\t\t\ttimeKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t} else if (questionWord === \"where\") {\n\t\t\t// Look for locations, places\n\t\t\tconst locationKeywords = [\n\t\t\t\t\"location\",\n\t\t\t\t\"place\",\n\t\t\t\t\"district\",\n\t\t\t\t\"area\",\n\t\t\t\t\"region\",\n\t\t\t\t\"border\",\n\t\t\t\t\"near\",\n\t\t\t\t\"at\",\n\t\t\t\t\"in\",\n\t\t\t\t\"found\",\n\t\t\t];\n\t\t\tlocationKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\n\t\t\t// Look for location patterns (capitalized words often indicate places)\n\t\t\tconst capitalizedWords = contentLower.match(/\\b[A-Z][a-z]+\\b/g);\n\t\t\tif (capitalizedWords && capitalizedWords.length > 2) intentScore += 2;\n\t\t} else if (questionWord === \"why\" || questionWord === \"how\") {\n\t\t\t// Look for explanations, reasons, methods\n\t\t\tconst explanationKeywords = [\n\t\t\t\t\"because\",\n\t\t\t\t\"due to\",\n\t\t\t\t\"reason\",\n\t\t\t\t\"caused\",\n\t\t\t\t\"result\",\n\t\t\t\t\"method\",\n\t\t\t\t\"process\",\n\t\t\t\t\"by\",\n\t\t\t];\n\t\t\texplanationKeywords.forEach((keyword) => {\n\t\t\t\tif (contentLower.includes(keyword)) intentScore += 1;\n\t\t\t});\n\t\t}\n\n\t\t// Boost score if chunk content directly contains query keywords (excluding common words)\n\t\tconst queryWords = queryLower\n\t\t\t.split(/\\s+/)\n\t\t\t.filter((w) => w.length > 3 && !commonWords.has(w));\n\t\tconst matchingQueryWords = queryWords.filter((word) =>\n\t\t\tcontentLower.includes(word)\n\t\t);\n\t\tif (matchingQueryWords.length > 0) {\n\t\t\tintentScore += matchingQueryWords.length * 0.5;\n\t\t}\n\n\t\treturn intentScore;\n\t}\n\n\t/**\n\t * Calculate combined relevance score for chunk selection during deduplication\n\t * (Same function as in non-streaming version)\n\t */\n\tfunction calculateChunkRelevance(\n\t\tsource: any,\n\t\tquery: string,\n\t\tqueryType: string\n\t): number {\n\t\t// Get chunk content (stored as note in the record)\n\t\tconst chunkContent = source.chunkContent || source.note || \"\";\n\n\t\t// Query intent score (0-10+)\n\t\tconst intentScore = scoreChunkByQueryIntent(chunkContent, query, queryType);\n\n\t\t// Semantic similarity (0-1, converted to 0-10 scale)\n\t\tconst semanticScore =\n\t\t\t(source.semanticSimilarity || source.similarity || 0) * 10;\n\n\t\t// RRF score (0-1, converted to 0-5 scale)\n\t\tconst rrfScore = (source.rrfScore || 0) * 5;\n\n\t\t// Combined score: intent (40%) + semantic (40%) + RRF (20%)\n\t\t// This prioritizes chunks that answer the question over chunks with high keyword matches\n\t\tconst combinedScore =\n\t\t\tintentScore * 0.4 + semanticScore * 0.4 + rrfScore * 0.2;\n\n\t\treturn combinedScore;\n\t}\n\n\tconst citedSources: Array<{\n\t\tid: string | number;\n\t\ttitle: string;\n\t\trelevance?: number;\n\t\tsimilarity?: number;\n\t\tcitation?: any;\n\t\tcitationScore: number;\n\t\trrfScore: number;\n\t\tchunkContent?: string;\n\t\tsemanticSimilarity?: number;\n\t}> = [];\n\tconst responseLower = fullResponseText.toLowerCase();\n\tconst normalizedResponse = normalize(fullResponseText);\n\n\tfor (const record of records) {\n\t\tlet citationScore = 0;\n\n\t\t// Check for exact title match (primary citation method)\n\t\tconst titleLower = record.title.toLowerCase();\n\t\tif (responseLower.includes(titleLower)) {\n\t\t\tcitationScore += 10;\n\t\t}\n\n\t\t// Check for normalized title match (handles cleaned up filenames)\n\t\tconst normalizedTitle = normalize(record.title);\n\t\tif (normalizedResponse.includes(normalizedTitle)) {\n\t\t\tcitationScore += 8; // High score for normalized match\n\t\t}\n\n\t\t// Check for title in citation format (Source: Title)\n\t\tif (responseLower.includes(`(source: ${titleLower})`)) {\n\t\t\tcitationScore += 10;\n\t\t}\n\n\t\t// Check for normalized title in citation format\n\t\tif (normalizedResponse.includes(`(source: ${normalizedTitle})`)) {\n\t\t\tcitationScore += 8;\n\t\t}\n\n\t\t// Medium signal: Explicit ID mention (fallback for legacy)\n\t\tif (\n\t\t\tresponseLower.includes(`id: ${record.id}`) ||\n\t\t\tresponseLower.includes(`[${record.id}]`) ||\n\t\t\tresponseLower.includes(`record ${record.id}`)\n\t\t) {\n\t\t\tcitationScore += 5;\n\t\t}\n\n\t\t// Check for most title words present\n\t\tconst titleWords = titleLower\n\t\t\t.split(/\\s+/)\n\t\t\t.filter((word) => word.length > 3 && !commonWords.has(word));\n\t\tconst titleMatches = titleWords.filter((word) =>\n\t\t\tresponseLower.includes(word)\n\t\t);\n\t\tif (titleMatches.length >= Math.min(titleWords.length * 0.6, 3)) {\n\t\t\tcitationScore += 3;\n\t\t}\n\n\t\t// Check for unique keywords from content\n\t\tif (record.note) {\n\t\t\tconst noteWords = record.note\n\t\t\t\t.toLowerCase()\n\t\t\t\t.split(/\\s+/)\n\t\t\t\t.filter((word) => word.length > 4 && !commonWords.has(word))\n\t\t\t\t.slice(0, 15);\n\n\t\t\tconst noteMatches = noteWords.filter((word) =>\n\t\t\t\tresponseLower.includes(word)\n\t\t\t);\n\t\t\tif (noteMatches.length >= 3) {\n\t\t\t\tcitationScore += 1;\n\t\t\t}\n\t\t}\n\n\t\t// Only include if score is significant enough and has minimum relevance\n\t\tconst minCitationScore = 5; // Increased from 3 to 5\n\t\tconst minRrfScore = 0.01; // Minimum RRF score threshold\n\t\tconst rrfScore = record.rrf_score || 0;\n\n\t\tif (citationScore >= minCitationScore && rrfScore >= minRrfScore) {\n\t\t\tcitedSources.push({\n\t\t\t\tid: record.id,\n\t\t\t\ttitle: record.title,\n\t\t\t\trelevance: record.rrf_score || record.combined_score || record.rank,\n\t\t\t\tsimilarity: record.rrf_score\n\t\t\t\t\t? record.rrf_score * 30\n\t\t\t\t\t: record.semantic_similarity, // Convert RRF to similarity-like value for UI\n\t\t\t\tcitation: record.citation, // Pass through citation data from hybrid search\n\t\t\t\tcitationScore, // Store for sorting/deduplication\n\t\t\t\trrfScore, // Store for sorting/deduplication\n\t\t\t\tchunkContent: record.note || \"\", // Store chunk content for query-intent matching\n\t\t\t\tsemanticSimilarity: record.semantic_similarity || 0, // Store semantic similarity\n\t\t\t});\n\t\t}\n\t}\n\n\t// Deduplicate by file ID (keep the chunk that best answers the query)\n\tconst uniqueSources = citedSources.reduce((acc: any[], source) => {\n\t\tconst existing = acc.find((s) => s.id === source.id);\n\t\tif (!existing) {\n\t\t\tacc.push(source);\n\t\t} else {\n\t\t\t// NEW: Check if chunk content actually appears in AI response\n\t\t\t// This ensures we cite the page that contains the actual answer\n\t\t\tconst aiResponseLower = fullResponseText.toLowerCase();\n\t\t\tconst existingContent = (existing.chunkContent || \"\").toLowerCase();\n\t\t\tconst sourceContent = (source.chunkContent || \"\").toLowerCase();\n\n\t\t\t// Extract key phrases from AI response (words/phrases that might be quoted)\n\t\t\tconst extractKeyPhrases = (text: string): string[] => {\n\t\t\t\t// Extract quoted text, capitalized phrases, and significant words\n\t\t\t\tconst quoted = text.match(/\"([^\"]+)\"/g) || [];\n\t\t\t\tconst capitalized =\n\t\t\t\t\ttext.match(/\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b/g) || [];\n\t\t\t\t// Extract bullet points or list items (often contain specific answers)\n\t\t\t\tconst bulletPoints = text.match(/[•\\*\\-]\\s*([^\\n]+)/g) || [];\n\t\t\t\t// Extract text after colons (often contains specific answers)\n\t\t\t\tconst afterColons = text.match(/:\\s*([^\\n]+)/g) || [];\n\t\t\t\tconst significantWords = text\n\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t.split(/\\s+/)\n\t\t\t\t\t.filter((w) => w.length > 3 && !commonWords.has(w))\n\t\t\t\t\t.slice(0, 15);\n\n\t\t\t\treturn [\n\t\t\t\t\t...quoted.map((q) => q.replace(/\"/g, \"\").toLowerCase()),\n\t\t\t\t\t...capitalized.map((c) => c.toLowerCase()),\n\t\t\t\t\t...bulletPoints.map((b) =>\n\t\t\t\t\t\tb\n\t\t\t\t\t\t\t.replace(/[•\\*\\-]\\s*/, \"\")\n\t\t\t\t\t\t\t.toLowerCase()\n\t\t\t\t\t\t\t.trim()\n\t\t\t\t\t),\n\t\t\t\t\t...afterColons.map((a) => a.replace(/:\\s*/, \"\").toLowerCase().trim()),\n\t\t\t\t\t...significantWords,\n\t\t\t\t].filter((phrase) => phrase.length > 2); // Filter out very short phrases\n\t\t\t};\n\n\t\t\tconst keyPhrases = extractKeyPhrases(fullResponseText);\n\n\t\t\t// Count how many key phrases appear in each chunk\n\t\t\tconst existingMatches = keyPhrases.filter((phrase) =>\n\t\t\t\texistingContent.includes(phrase)\n\t\t\t).length;\n\t\t\tconst sourceMatches = keyPhrases.filter((phrase) =>\n\t\t\t\tsourceContent.includes(phrase)\n\t\t\t).length;\n\n\t\t\t// Prefer chunk with more matching phrases from AI response\n\t\t\tif (sourceMatches > existingMatches) {\n\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\tacc[index] = source;\n\t\t\t} else if (sourceMatches === existingMatches && sourceMatches > 0) {\n\t\t\t\t// If same number of matches, use relevance score as tiebreaker\n\t\t\t\tconst existingRelevance = calculateChunkRelevance(\n\t\t\t\t\texisting,\n\t\t\t\t\tqueryForSearch,\n\t\t\t\t\tqueryType\n\t\t\t\t);\n\t\t\t\tconst sourceRelevance = calculateChunkRelevance(\n\t\t\t\t\tsource,\n\t\t\t\t\tqueryForSearch,\n\t\t\t\t\tqueryType\n\t\t\t\t);\n\n\t\t\t\tif (sourceRelevance > existingRelevance) {\n\t\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\t\tacc[index] = source;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Fallback to original logic if no content matches\n\t\t\t\tconst existingRelevance = calculateChunkRelevance(\n\t\t\t\t\texisting,\n\t\t\t\t\tqueryForSearch,\n\t\t\t\t\tqueryType\n\t\t\t\t);\n\t\t\t\tconst sourceRelevance = calculateChunkRelevance(\n\t\t\t\t\tsource,\n\t\t\t\t\tqueryForSearch,\n\t\t\t\t\tqueryType\n\t\t\t\t);\n\n\t\t\t\tif (sourceRelevance > existingRelevance) {\n\t\t\t\t\tconst index = acc.indexOf(existing);\n\t\t\t\t\tacc[index] = source;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn acc;\n\t}, []);\n\n\t// Sort by combined score: citation score first, then RRF score\n\tconst sortedSources = uniqueSources\n\t\t.sort((a, b) => {\n\t\t\t// Primary sort: citation score (how explicitly mentioned)\n\t\t\tif (b.citationScore !== a.citationScore) {\n\t\t\t\treturn b.citationScore - a.citationScore;\n\t\t\t}\n\t\t\t// Secondary sort: RRF score (relevance to query)\n\t\t\treturn b.rrfScore - a.rrfScore;\n\t\t})\n\t\t// Limit to top 10 most relevant sources\n\t\t.slice(0, 10)\n\t\t.map(\n\t\t\t({\n\t\t\t\tcitationScore,\n\t\t\t\trrfScore,\n\t\t\t\tchunkContent,\n\t\t\t\tsemanticSimilarity,\n\t\t\t\t...rest\n\t\t\t}) => rest\n\t\t); // Remove internal scores from final output\n\n\tconsole.log(\n\t\t`[ADMIN CHAT] Response generated with ${sortedSources.length} sources (filtered from ${citedSources.length} candidates)`\n\t);\n\n\t// Yield sources\n\tyield {\n\t\ttype: \"sources\",\n\t\tsources: sortedSources,\n\t};\n\n\t// Yield final completion with token counts\n\tyield {\n\t\ttype: \"done\",\n\t\ttokenCount: {\n\t\t\tinput: analysisInputTokens + aiInputTokens,\n\t\t\toutput: analysisOutputTokens + aiOutputTokens,\n\t\t},\n\t};\n}\n\n/**\n * Update search vectors for all chapter chunks (maintenance function)\n */\nexport async function updateSearchVectors(): Promise<void> {\n\ttry {\n\t\tawait prisma.$executeRaw`\n      UPDATE chapter_chunks\n      SET search_vector =\n        setweight(to_tsvector('english', COALESCE((SELECT title FROM chapters WHERE id = chapter_id), '')), 'A') ||\n        setweight(to_tsvector('english', COALESCE((SELECT s.name FROM chapters c JOIN subjects s ON c.subject_id = s.id WHERE c.id = chapter_id), '')), 'B') ||\n        setweight(to_tsvector('english', COALESCE(content, '')), 'C')\n      WHERE search_vector IS NULL OR content IS NOT NULL\n    `;\n\n\t\tconsole.log(\n\t\t\t\"[SEARCH VECTORS] Updated search vectors for all chapter chunks\"\n\t\t);\n\t} catch (error) {\n\t\tconsole.error(\"Error updating search vectors:\", error);\n\t\tthrow new Error(\"Failed to update search vectors\");\n\t}\n}\n\n// --- Exam Prep & Quiz Generation ---\n\nexport interface QuizGenerationConfig {\n\tsubject: string;\n\ttopic: string;\n\tchapterTitle?: string;\n\tdifficulty: \"easy\" | \"medium\" | \"hard\";\n\tquestionCount: number;\n\tquestionTypes: (\n\t\t| \"MCQ\"\n\t\t| \"TRUE_FALSE\"\n\t\t| \"FILL_IN_BLANK\"\n\t\t| \"SHORT_ANSWER\"\n\t\t| \"LONG_ANSWER\"\n\t)[];\n\tcontext: string; // The content to generate questions from\n}\n\nconst QuizQuestionSchema = z.object({\n\tquestion_text: z.string(),\n\tquestion_type: z.enum([\n\t\t\"MCQ\",\n\t\t\"TRUE_FALSE\",\n\t\t\"FILL_IN_BLANK\",\n\t\t\"SHORT_ANSWER\",\n\t\t\"LONG_ANSWER\",\n\t]),\n\toptions: z\n\t\t.array(z.string())\n\t\t.optional()\n\t\t.describe(\"Array of options for MCQ/TF. Null for others.\"),\n\tcorrect_answer: z\n\t\t.union([z.string(), z.number(), z.array(z.string())])\n\t\t.describe(\"The correct answer. MUST be the exact string from the options array for MCQ/TrueFalse.\"),\n\tpoints: z.number().default(1),\n\texplanation: z.string().describe(\"Explanation of why the answer is correct\"),\n});\n\nconst QuizSchema = z.object({\n\ttitle: z.string(),\n\tdescription: z.string(),\n\tquestions: z.array(QuizQuestionSchema),\n});\n\n// Schema for Batch Question Generation\nconst BatchQuestionSchema = z.object({\n\tquestions: z.array(z.object({\n\t\tquestion_text: z.string().describe(\"The question text\"),\n\t\tquestion_type: z.enum([\"MCQ\", \"TRUE_FALSE\", \"FILL_IN_BLANK\", \"SHORT_ANSWER\", \"LONG_ANSWER\"]),\n\t\tdifficulty: z.enum([\"easy\", \"medium\", \"hard\"]),\n\t\toptions: z.array(z.string()).optional().describe(\"Options for MCQ (4 options) or TRUE_FALSE (2 options)\"),\n\t\tcorrect_answer: z.any().describe(\"The correct answer. For MCQ/TF/FIB: string. For Short/Long: model answer string.\"),\n\t\texplanation: z.string().describe(\"Detailed explanation of why the answer is correct\"),\n\t\tpoints: z.number().describe(\"Points value: Easy=1, Medium=3, Hard=5\")\n\t}))\n});\n\nexport interface BatchQuestionConfig {\n\tcontext: string;\n\tchapterTitle: string;\n\tconfig: {\n\t\teasy: { [key in QuestionType]?: number };\n\t\tmedium: { [key in QuestionType]?: number };\n\t\thard: { [key in QuestionType]?: number };\n\t};\n}\n\n/**\n * Generate a batch of questions based on specific counts per difficulty/type\n * Used for pre-generating the Question Bank\n */\nexport async function generateBatchQuestions(input: BatchQuestionConfig) {\n\tconst { context, chapterTitle, config } = input;\n\n\t// Construct a detailed request list\n\tlet requestList: string[] = [];\n\tlet totalQuestions = 0;\n\n\t(['easy', 'medium', 'hard'] as const).forEach(diff => {\n\t\tObject.entries(config[diff]).forEach(([type, count]) => {\n\t\t\tif (count && count > 0) {\n\t\t\t\trequestList.push(`${count} ${diff.toUpperCase()} ${type} questions`);\n\t\t\t\ttotalQuestions += count;\n\t\t\t}\n\t\t});\n\t});\n\n\tif (totalQuestions === 0) return [];\n\n\tconst prompt = `You are an expert educational content creator.\nYour task is to generate exactly ${totalQuestions} questions for the chapter section: \"${chapterTitle}\".\n\n=== SOURCE MATERIAL ===\n${context}\n=== END SOURCE MATERIAL ===\n\nREQUIREMENTS:\nGenerate the following mix of questions based STRICTLY on the source material above:\n${requestList.map(r => `• ${r}`).join('\\n')}\n\nRULES:\n1. Questions must be high-quality, clear, and unambiguous.\n2. COVERAGE: Ensure questions cover different parts of the text, not just the first paragraph.\n3. DIFFICULTY:\n   - EASY: Recall facts, definitions, simple concepts.\n   - MEDIUM: Apply concepts, compare/contrast, explain \"why\".\n   - HARD: Analyze, synthesize, evaluate, complex scenarios.\n4. TYPES:\n   - MCQ: Provide 4 distinct options. One correct.\n   - TRUE_FALSE: Provide \"True\" and \"False\" as options.\n   - FILL_IN_BLANK: The answer should be a specific word or short phrase from the text.\n   - SHORT_ANSWER: Model answer should be 1-3 sentences.\n   - LONG_ANSWER: Model answer should be a detailed paragraph.\n5. EXPLANATION: Provide a helpful explanation for the correct answer.\n6. SELF-CONTAINED QUESTIONS:\n   - DO NOT reference external materials like \"the provided algorithm\", \"the given diagram\", \"the figure\", \"the table\", \"Case 1/2/3\", \"the image\", \"the flowchart\", etc.\n   - Questions must be fully self-contained and understandable without any visual aids or external references.\n   - Include all necessary context within the question itself.\n7. PHRASING:\n   - AVOID: \"According to the text, ...\" or \"The text states that ...\"\n   - PREFER: Direct questions (e.g., \"What is the time complexity of...?\") OR \"According to the chapter, ...\" if needed.\n   - Questions should sound natural and professional, as if from an exam paper.\n8. NO META-QUESTIONS: Do not ask \"What does the text say about...\", just ask the question directly.\n\nOutput a JSON object with a \"questions\" array.`;\n\n\ttry {\n\t\t// Get API key and initialize provider\n\t\t// We use a dummy keyId here or allow the system to pick a default\n\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\" });\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\n\t\tif (!keyToUse) {\n\t\t\tthrow new Error(\"No Gemini API key found\");\n\t\t}\n\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\t// Model selection strategy\n\t\tconst modelsToTry: string[] = [];\n\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\tmodelsToTry.push(...dbModels);\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\tmodelsToTry.push(fallbackModel);\n\t\tconst uniqueModels = [...new Set(modelsToTry)];\n\n\t\tfor (const modelName of uniqueModels) {\n\t\t\ttry {\n\t\t\t\tconsole.log(`[AI-BATCH] Attempting to generate questions with model: ${modelName}`);\n\n\t\t\t\tconst result = await generateObject({\n\t\t\t\t\tmodel: google(modelName),\n\t\t\t\t\tschema: BatchQuestionSchema,\n\t\t\t\t\tprompt: prompt,\n\t\t\t\t\tmode: 'json',\n\t\t\t\t});\n\n\t\t\t\t// Normalize points based on question type (AI sometimes ignores this)\n\t\t\t\tconst normalizedQuestions = result.object.questions.map(q => {\n\t\t\t\t\tlet correctPoints = 1; // default\n\t\t\t\t\tswitch (q.question_type) {\n\t\t\t\t\t\tcase \"MCQ\":\n\t\t\t\t\t\tcase \"TRUE_FALSE\":\n\t\t\t\t\t\tcase \"FILL_IN_BLANK\":\n\t\t\t\t\t\t\tcorrectPoints = 1;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"SHORT_ANSWER\":\n\t\t\t\t\t\t\tcorrectPoints = 3;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"LONG_ANSWER\":\n\t\t\t\t\t\t\tcorrectPoints = 5;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (q.points !== correctPoints) {\n\t\t\t\t\t\tconsole.log(`[AI-BATCH] Correcting points for ${q.question_type}: ${q.points} → ${correctPoints}`);\n\t\t\t\t\t}\n\n\t\t\t\t\treturn { ...q, points: correctPoints };\n\t\t\t\t});\n\n\t\t\t\treturn normalizedQuestions;\n\t\t\t} catch (error: any) {\n\t\t\t\tconsole.warn(`[AI-BATCH] Failed with model ${modelName}: ${error.message}`);\n\t\t\t\t// Continue to next model\n\t\t\t}\n\t\t}\n\n\t\tthrow new Error(\"All models failed to generate questions\");\n\n\t} catch (error) {\n\t\tconsole.error(\"[AI-SERVICE] Batch question generation failed:\", error);\n\t\treturn [];\n\t}\n}\n\nexport async function generateQuiz(\n\tconfig: QuizGenerationConfig,\n\topts: { model?: string; keyId?: number; board?: string; level?: string } = {}\n) {\n\ttry {\n\t\tconst { client, keyId } = await getGeminiClient({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\n\t\t// Note: We use the Vercel AI SDK's generateObject which needs a provider instance\n\t\t// We'll reuse the key from getGeminiClient but we need to instantiate the provider\n\t\tconst { apiKey } = await getProviderApiKey({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\t\t// Fallback to environment variable if no key found in database\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\t\tif (!keyToUse) {\n\t\t\tthrow new Error(\n\t\t\t\t\"No Gemini API key found. Add a key in admin settings or set GEMINI_API_KEY.\"\n\t\t\t);\n\t\t}\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\t// Calculate question type distribution\n\t\tconst typeDistribution = config.questionTypes.map((type, idx) => {\n\t\t\tconst count = Math.floor(config.questionCount / config.questionTypes.length) +\n\t\t\t\t(idx < config.questionCount % config.questionTypes.length ? 1 : 0);\n\t\t\treturn `${count}x ${type}`;\n\t\t}).join(\", \");\n\t\tconst boardContext = opts.board ? `You are an expert ${opts.board} question setter.` : \"\";\n\t\tconst levelContext = opts.level ? `The target audience is ${opts.level} students.` : \"\";\n\n\t\tconst prompt = `${boardContext} ${levelContext}\nYou are creating a ${config.difficulty}-level educational quiz for students studying \"${config.subject}: ${config.topic}\".\n\n**Subject**: ${config.subject}\n**Chapter**: ${config.topic}\n**Difficulty**: ${config.difficulty}\n\n=== EDUCATIONAL MATERIAL ===\nThe following is the study material from the textbook chapter on this topic. Use this to create meaningful questions that test students' understanding of the concepts, facts, and knowledge they should learn from this chapter.\n\n${config.context}\n\n=== END OF EDUCATIONAL MATERIAL ===\n\nQUIZ REQUIREMENTS:\n• Total: ${config.questionCount} questions (${typeDistribution})\n• Types: ONLY ${config.questionTypes.join(\", \")}\n• ALL questions must test understanding of concepts and knowledge from the educational material above\n• MCQ: 4 options, correct_answer = exact option text, 1 point\n  ${config.difficulty === \"hard\" ? \"• **HARD DIFFICULTY MCQs**: For hard difficulty, include 20-30% multi-select MCQs where multiple options are correct. Format: correct_answer = array of exact option texts (e.g., [\\\"A. option1\\\", \\\"B. option2\\\"]). Question text MUST include keywords like \\\"correct reason(s)\\\", \\\"correct options are\\\", or use pattern (i), (ii), (iii), (iv) to indicate multi-select.\" : \"\"}\n• TRUE_FALSE: 2 options (\"True\", \"False\"), correct_answer = exact text, 1 point  \n• FILL_IN_BLANK: correct_answer = missing word/phrase, 1 point\n• SHORT_ANSWER: correct_answer = 2-3 sentence model answer, 2 points\n• LONG_ANSWER: correct_answer = 5+ sentence detailed answer, 5 points\n\nCRITICAL RULES - QUESTIONS MUST:\n✓ Test actual subject knowledge and concepts\n✓ Be completely self-contained and understandable without any visual aids\n✓ Be answerable using the knowledge from the educational material\n✓ Focus on \"what\", \"why\", and \"how\" of the subject matter\n✓ Include all necessary context within the question itself\n\nSTRICTLY PROHIBITED - DO NOT CREATE:\n✗ Questions referencing unavailable materials: \"the provided algorithm\", \"the given diagram\", \"the figure\", \"the table\", \"Case 1/2/3\", \"the image\", \"the flowchart\", \"the graph\"\n✗ Questions about document structure (e.g., \"what number appears in the content\")\n✗ Questions referencing \"Activity X.X\", \"Figure X.X\", \"Table X.X\", or \"Box X.X\" numbers\n✗ Questions using phrases like \"According to the text\", \"The text states\", \"the provided text\", \"the content above\", \"the material shown\"\n✗ Questions about formatting, layout, or visual presentation\n✗ Questions that reference section numbers, page numbers, or document organization\n✗ Meta-questions about the text itself rather than the subject matter\n\nPHRASING GUIDELINES:\n✓ GOOD: Direct questions (e.g., \"What is the time complexity of binary search?\")\n✓ ACCEPTABLE: \"According to the chapter, what is...\"\n✗ AVOID: \"According to the text, what is...\"\n✗ AVOID: \"The text states that...\"\n\nEXAMPLES:\n❌ BAD: \"In the provided 'Remove' algorithm, which case is executed when...?\"\n✅ GOOD: \"In a linked list removal operation, what happens when the list contains only one node that matches the value to be removed?\"\n\n❌ BAD: \"Which of the following numbers is shown in the provided content?\"\n✅ GOOD: \"What is the boiling point of water in Celsius?\"\n\n❌ BAD: \"According to the text, which data structure is used for BFS?\"\n✅ GOOD: \"Which data structure is traditionally used in the implementation of breadth-first traversal?\"\n\n❌ BAD: \"According to Figure 2.1, what process is shown?\"\n✅ GOOD: \"What is the process by which water vapor turns into liquid water?\"\n\n❌ BAD: \"In Activity 1.3, what was demonstrated?\"\n✅ GOOD: \"What happens when you mix an acid with a base?\"\n\nRemember: You are testing students' knowledge of ${config.subject}, not their ability to read the textbook layout! Questions should be professional exam-style questions that stand alone without any external references.`;\n\n\n\n\t\t// Model fallback strategy\n\t\t// Priority: 1. Requested model (if any), 2. Admin-configured models, 3. .env fallback\n\t\tconst modelsToTry: string[] = [];\n\t\tif (opts.model) modelsToTry.push(opts.model);\n\n\t\t// Add admin-configured models\n\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\tmodelsToTry.push(...dbModels);\n\n\t\t// Add .env fallback\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\tmodelsToTry.push(fallbackModel);\n\n\t\t// Remove duplicates\n\t\tconst uniqueModels = [...new Set(modelsToTry)];\n\n\t\tlet lastError;\n\n\t\tfor (const modelName of uniqueModels) {\n\t\t\ttry {\n\t\t\t\tconsole.log(`[AI-QUIZ] Attempting to generate quiz with model: ${modelName}`);\n\n\t\t\t\tconst resultPromise = generateObject({\n\t\t\t\t\tmodel: google(modelName),\n\t\t\t\t\tschema: QuizSchema,\n\t\t\t\t\tprompt: prompt,\n\t\t\t\t});\n\n\t\t\t\tconst result = await Promise.race([\n\t\t\t\t\tresultPromise,\n\t\t\t\t\tnew Promise((_, reject) =>\n\t\t\t\t\t\tsetTimeout(() => reject(new Error(\"Quiz generation timed out after 90 seconds\")), 90000)\n\t\t\t\t\t)\n\t\t\t\t]) as typeof resultPromise extends Promise<infer T> ? T : never;\n\n\t\t\t\tconsole.log(`[AI-QUIZ] Successfully generated quiz with ${result.object.questions.length} questions`);\n\n\t\t\t\t// Normalize points based on question type (AI sometimes ignores this)\n\t\t\t\tresult.object.questions = result.object.questions.map(q => {\n\t\t\t\t\tlet correctPoints = 1; // default\n\t\t\t\t\tswitch (q.question_type) {\n\t\t\t\t\t\tcase \"MCQ\":\n\t\t\t\t\t\tcase \"TRUE_FALSE\":\n\t\t\t\t\t\tcase \"FILL_IN_BLANK\":\n\t\t\t\t\t\t\tcorrectPoints = 1;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"SHORT_ANSWER\":\n\t\t\t\t\t\t\tcorrectPoints = 2;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase \"LONG_ANSWER\":\n\t\t\t\t\t\t\tcorrectPoints = 5;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (q.points !== correctPoints) {\n\t\t\t\t\t\tconsole.log(`[AI-QUIZ] Correcting points for ${q.question_type}: ${q.points} → ${correctPoints}`);\n\t\t\t\t\t}\n\n\t\t\t\t\treturn { ...q, points: correctPoints };\n\t\t\t\t});\n\n\t\t\t\tif (keyId) await recordKeyUsage(keyId, true);\n\t\t\t\treturn result.object;\n\n\t\t\t} catch (error: any) {\n\t\t\t\tconsole.warn(`[AI-QUIZ] Failed with model ${modelName}: ${error.message}`);\n\t\t\t\tlastError = error;\n\n\t\t\t\t// If it's not a quota/availability issue (e.g. schema validation), maybe we shouldn't retry?\n\t\t\t\t// But for now, let's assume most errors are model-related and try the next one.\n\t\t\t\t// Specifically check for 429 (Resource Exhausted) or 404 (Not Found)\n\t\t\t\tconst isRetryable =\n\t\t\t\t\terror.message.includes(\"429\") ||\n\t\t\t\t\terror.message.includes(\"404\") ||\n\t\t\t\t\terror.message.includes(\"quota\") ||\n\t\t\t\t\terror.message.includes(\"not found\") ||\n\t\t\t\t\terror.message.includes(\"overloaded\");\n\n\t\t\t\tif (!isRetryable && opts.model) {\n\t\t\t\t\t// If user specified a specific model and it failed with non-retryable, throw immediately\n\t\t\t\t\tthrow error;\n\t\t\t\t}\n\n\t\t\t\t// Continue to next model\n\t\t\t}\n\t\t}\n\n\t\t// If we get here, all models failed\n\t\tthrow lastError || new Error(\"Failed to generate quiz with all available models\");\n\n\t} catch (error: any) {\n\t\tconsole.error(\"Error generating quiz:\", error);\n\t\tthrow new Error(`Failed to generate quiz: ${error.message || String(error)}`);\n\t}\n}\n\nexport async function gradeQuiz(\n\tquestions: {\n\t\tquestion_text: string;\n\t\tuser_answer: string;\n\t\tcorrect_answer: string;\n\t\ttype: string;\n\t}[],\n\topts: { model?: string; keyId?: number } = {}\n) {\n\ttry {\n\t\t// Filter for subjective questions that need AI grading\n\t\tconst subjectiveQuestions = questions.filter((q) =>\n\t\t\t[\"SHORT_ANSWER\", \"LONG_ANSWER\"].includes(q.type)\n\t\t);\n\n\t\tif (subjectiveQuestions.length === 0) {\n\t\t\treturn [];\n\t\t}\n\n\t\tconst { client, keyId } = await getGeminiClient({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\n\t\t// Use priority: 1) opts.model, 2) admin-configured models, 3) .env fallback\n\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\";\n\t\tconst modelName = opts.model || dbModels[0] || fallbackModel;\n\t\tconst { apiKey } = await getProviderApiKey({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\t\t// Fallback to environment variable if no key found in database\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\t\tif (!keyToUse) {\n\t\t\tthrow new Error(\n\t\t\t\t\"No Gemini API key found. Add a key in admin settings or set GEMINI_API_KEY.\"\n\t\t\t);\n\t\t}\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\tconst GradingSchema = z.object({\n\t\t\tgrades: z.array(\n\t\t\t\tz.object({\n\t\t\t\t\tquestion_text: z.string(),\n\t\t\t\t\tis_correct: z.boolean(),\n\t\t\t\t\tscore_percentage: z.number().min(0).max(100),\n\t\t\t\t\tfeedback: z.string(),\n\t\t\t\t})\n\t\t\t),\n\t\t});\n\n\t\tconst prompt = `Grade the following student answers based on the model answer.\n        \n        Questions:\n        ${JSON.stringify(subjectiveQuestions, null, 2)}\n        \n        Provide a score (0-100) and feedback for each. Be lenient on phrasing but strict on factual accuracy.`;\n\n\t\tconst result = await generateObject({\n\t\t\tmodel: google(modelName),\n\t\t\tschema: GradingSchema,\n\t\t\tprompt: prompt,\n\t\t});\n\n\t\tif (keyId) await recordKeyUsage(keyId, true);\n\n\t\treturn result.object.grades;\n\t} catch (error) {\n\t\tconsole.error(\"Error grading quiz:\", error);\n\t\t// Fallback: mark all as needing manual review or give full credit?\n\t\t// For now, throw error\n\t\tthrow new Error(\"Failed to grade quiz\");\n\t}\n}\n\n// --- Study Materials Generation ---\n\nexport interface StudyMaterialsConfig {\n\tsubject: string;\n\tchapterTitle: string;\n\tcontent: string; // The chapter content to generate materials from\n}\n\nconst StudyMaterialSchema = z.object({\n\tsummary_markdown: z.string().describe(\"A comprehensive 5-minute read summary of the chapter in markdown format\"),\n\tkey_terms: z.array(z.object({\n\t\tterm: z.string(),\n\t\tdefinition: z.string(),\n\t})).describe(\"Glossary of important terms and concepts with clear definitions\"),\n\tflashcards: z.array(z.object({\n\t\tfront: z.string().describe(\"Question or term\"),\n\t\tback: z.string().describe(\"Answer or definition\"),\n\t})).min(10).max(20).describe(\"10-20 flashcard pairs for quick revision\"),\n\tyoutube_search_queries: z.array(z.string()).length(3).describe(\"3 precise search terms to find the best educational videos for this topic\"),\n\tmind_map_mermaid: z.string().describe(\"Mermaid.js flowchart syntax representing the chapter's concept hierarchy\"),\n\timportant_formulas: z.array(z.object({\n\t\tname: z.string(),\n\t\tformula: z.string(),\n\t\texplanation: z.string(),\n\t})).optional().describe(\"Key formulas if this is a math/science chapter\"),\n});\n\nexport async function generateStudyMaterials(\n\tconfig: StudyMaterialsConfig,\n\topts: { model?: string; keyId?: number } = {}\n) {\n\ttry {\n\t\t// Use getGeminiClient to respect admin-configured models\n\t\tconst { client, keyId } = await getGeminiClient({\n\t\t\tprovider: \"gemini\",\n\t\t\tkeyId: opts.keyId,\n\t\t});\n\n\t\t// Use priority: 1) opts.model, 2) admin-configured models, 3) .env fallback\n\t\tconst dbModels = await getActiveModelNames(\"gemini\");\n\t\tconst fallbackModel = process.env.GEMINI_DEFAULT_MODEL || \"gemini-2.0-flash\t\";\n\t\tconst modelName = opts.model || dbModels[0] || fallbackModel;\n\n\t\t// Get API key for generateObject\n\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\", keyId: opts.keyId });\n\t\tif (!apiKey) throw new Error(\"No Gemini API key found\");\n\t\tconst google = createGoogleGenerativeAI({ apiKey });\n\n\t\t// Limit content to avoid token limits\n\t\tconst contentSnippet = config.content.substring(0, 40000);\n\n\t\tconst prompt = `Generate comprehensive study materials for the following chapter.\n\nChapter: ${config.subject} - ${config.chapterTitle}\n\nContent:\n${contentSnippet}\n\nGenerate the following study materials:\n1. A comprehensive summary (5-minute read) formatted in **Markdown**.\n   - **Formatting:**\n   - Use **bold** for key terms.\n   - Use bullet points for lists.\n   - Use \\`code blocks\\` for code.\n   - **NO TABLES:** Do not use Markdown tables. They do not render well on mobile devices. Use bulleted lists or clear text structures instead.\n   - **Newlines:** Use double newlines between paragraphs for better readability.\n   - Use bullet points for lists.\n   - Use **bold** for key concepts.\n2. A glossary of important terms and definitions.\n3. 10-20 flashcard pairs (front: question/term, back: answer/definition).\n4. 3 specific YouTube search queries to find the best educational videos.\n5. A Mermaid.js **flowchart** diagram to visualize the concepts.\n   - Start with \"graph TD\" (Top-Down).\n   - Use simple node labels like [Main Topic] --> [Subtopic].\n   - AVOID special characters like (), {}, or quotes inside the node text.\n   - Example:\n     graph TD\n       A[Main Topic] --> B[Subtopic 1]\n       A --> C[Subtopic 2]\n       B --> D[Detail 1]\n6. If applicable, list important formulas with explanations.\n\nMake the materials student-friendly, clear, and focused on exam preparation.`;\n\n\t\tconst result = await generateObject({\n\t\t\tmodel: google(modelName),\n\t\t\tschema: StudyMaterialSchema,\n\t\t\tprompt: prompt,\n\t\t});\n\n\t\tif (keyId) await recordKeyUsage(keyId, true);\n\n\t\treturn result.object;\n\n\t} catch (error) {\n\t\tconsole.error(\"Error generating study materials:\", error);\n\t\tthrow new Error(\"Failed to generate study materials\");\n\t}\n}\n\n// ==========================================\n// QUESTION BANK UPLOAD HELPERS\n// ==========================================\n\nconst ExtractedQuestionSchema = z.object({\n\tquestion_text: z.string().describe(\"The full text of the question\"),\n\tquestion_type: z.enum([\"MCQ\", \"SHORT_ANSWER\", \"LONG_ANSWER\", \"TRUE_FALSE\", \"FILL_IN_THE_BLANK\"]).describe(\"The type of question inferred from format\"),\n\tpoints: z.number().optional().describe(\"Marks allocated to this question if specified\"),\n\toptions: z.array(z.string()).optional().describe(\"For MCQs, the list of options\"),\n\tquestion_number: z.string().optional().describe(\"The question number as it appears in the paper (e.g. '1', '2(a)')\"),\n});\n\nconst ExtractedPaperSchema = z.object({\n\tquestions: z.array(ExtractedQuestionSchema).describe(\"List of all extracted questions\")\n});\n\nconst AnswerGenerationSchema = z.object({\n\tcorrect_answer: z.string().describe(\"The correct answer to the question\"),\n\texplanation: z.string().describe(\"Detailed explanation of why this is the correct answer\"),\n});\n\n/**\n * Extracts structured questions from a parsed exam paper markdown\n */\nexport async function extractQuestionsFromPaper(pdfMarkdown: string) {\n\ttry {\n\t\tconsole.log(`[AI-EXTRACT] Extracting questions from paper (${pdfMarkdown.length} chars)...`);\n\n\t\t// Initialize Google Provider\n\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\" });\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\t\tif (!keyToUse) throw new Error(\"No Gemini API key found\");\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\tconst prompt = `\nYou are an expert exam paper parser. Your task is to extract questions from the provided exam paper content.\n\nINPUT CONTENT:\n${pdfMarkdown.slice(0, 30000)} // Limit context to avoid token limits\n\nINSTRUCTIONS:\n1. Identify all questions in the text.\n2. CRITICAL: The paper may contain both English and Hindi text. IGNORE all Hindi text/translations. Extract ONLY the English version of the questions.\n3. For each question, determine its type (MCQ, SHORT_ANSWER, LONG_ANSWER, etc.).\n4. Extract the points/marks if mentioned (e.g. \"[1 Mark]\", \"(3)\").\n5. For MCQs, extract all options into an array (English only).\n   - **CRITICAL**: Remove ONLY the outermost option label (e.g., \"A.\", \"B.\", \"(a)\", \"(b)\", \"1.\", \"2.\").\n   - **PRESERVE** internal numbering or references like \"(i)\", \"(ii)\", \"1.\", \"2.\" if they are part of the answer content.\n   - Example: If text is \"A. (i) and (ii)\", store \"(i) and (ii)\".\n   - Example: If text is \"(b) Statement 1 is correct\", store \"Statement 1 is correct\".\n6. Preserve the exact question text.\n7. HANDLING DIAGRAMS:\n   - If a question refers to a diagram (e.g., \"In the given circuit\"), look for any text description provided in the input.\n   - If the diagram is described in text (e.g., \"Circuit with 1 ohm and 2 ohm resistors\"), include that description in the question text.\n   - If the question is purely visual and impossible to solve without seeing the image, SKIP IT.\n8. Ignore instructions like \"All questions are compulsory\" or section headers unless relevant.\n\nOUTPUT FORMAT:\nReturn a JSON object with a \"questions\" array containing the extracted data.\n`;\n\n\t\tconst result = await generateObject({\n\t\t\tmodel: google(\"gemini-2.5-pro\"), // Use Pro for better reasoning on complex layouts\n\t\t\tschema: ExtractedPaperSchema,\n\t\t\tprompt: prompt,\n\t\t});\n\n\t\tconsole.log(`[AI-EXTRACT] Successfully extracted ${result.object.questions.length} questions.`);\n\t\treturn result.object.questions;\n\n\t} catch (error) {\n\t\tconsole.error(\"[AI-EXTRACT] Error extracting questions:\", error);\n\t\tthrow error;\n\t}\n}\n\n/**\n * Generates an answer for a question using chapter context\n */\nexport async function generateAnswerForQuestion(question: string, context: string, marks: number = 1) {\n\ttry {\n\t\t// Initialize Google Provider\n\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\" });\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\t\tif (!keyToUse) throw new Error(\"No Gemini API key found\");\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\tconst prompt = `\nYou are an expert subject tutor. Your task is to answer an exam question based strictly on the provided textbook content.\n\nQUESTION:\n${question}\n\nMARKS: ${marks} (Answer length and detail should be appropriate for these marks)\n\nTEXTBOOK CONTENT:\n${context.slice(0, 20000)}\n\nINSTRUCTIONS:\n1. Read the question and the textbook content carefully.\n2. **CRITICAL FOR MCQs**:\n   - Select the correct option(s) from the provided choices.\n   - The \"correct_answer\" field must contain the **EXACT TEXT** of the correct option(s).\n   - If multiple options are correct (e.g., \"Both A and B\"), state that clearly using the option text.\n3. Generate the CORRECT ANSWER based on the textbook.\n4. Provide a detailed EXPLANATION referencing the textbook concepts.\n5. If the answer cannot be found in the context, use your general knowledge but mention that it wasn't in the provided text.\n\nOUTPUT FORMAT:\nReturn a JSON object with \"correct_answer\" and \"explanation\".\n`;\n\n\t\tconst result = await generateObject({\n\t\t\tmodel: google(\"gemini-2.0-flash\"), // Flash is sufficient for answering\n\t\t\tschema: AnswerGenerationSchema,\n\t\t\tprompt: prompt,\n\t\t});\n\n\t\treturn result.object;\n\n\t} catch (error) {\n\t\tconsole.error(\"[AI-SOLVE] Error generating answer:\", error);\n\t\t// Return a fallback structure instead of throwing, to allow partial success\n\t\treturn {\n\t\t\tcorrect_answer: \"Could not generate answer.\",\n\t\t\texplanation: \"AI failed to generate an answer for this question.\"\n\t\t};\n\t}\n}\n\nconst BatchAnswerSchema = z.object({\n\tanswers: z.array(z.object({\n\t\tquestion_number: z.string().optional(),\n\t\tquestion_text_snippet: z.string().describe(\"First few words of question to identify it\"),\n\t\tcorrect_answer: z.union([z.string(), z.array(z.string())]),\n\t\texplanation: z.string()\n\t}))\n});\n\n/**\n * Generates answers for a BATCH of questions using chapter context\n * Much more efficient than one-by-one\n */\n/**\n * Generates answers for a BATCH of questions using chapter context\n * Much more efficient than one-by-one\n */\nexport async function generateAnswersForBatch(\n\tquestions: any[],\n\tcontext: string,\n\tmetadata: { board?: string, level?: string, subject?: string, chapter?: string } = {}\n) {\n\ttry {\n\t\t// Initialize Google Provider\n\t\tconst { apiKey } = await getProviderApiKey({ provider: \"gemini\" });\n\t\tconst keyToUse = apiKey || process.env.GEMINI_API_KEY;\n\t\tif (!keyToUse) throw new Error(\"No Gemini API key found\");\n\t\tconst google = createGoogleGenerativeAI({ apiKey: keyToUse });\n\n\t\tconsole.log(`[AI-SOLVE] Generating answers for batch of ${questions.length} questions...`);\n\n\t\tconst questionsText = questions.map((q, i) =>\n\t\t\t`Q${i + 1} [${q.points || 1} Marks] (${q.question_type}): ${q.question_text}`\n\t\t).join(\"\\n\\n\");\n\n\t\tconst boardContext = metadata.board ? `You are an expert ${metadata.board} question setter.` : \"You are an expert subject tutor.\";\n\t\tconst levelContext = metadata.level ? `The current level is ${metadata.level}.` : \"\";\n\t\tconst subjectChapter = metadata.subject && metadata.chapter\n\t\t\t? `**Subject**: ${metadata.subject}\\n**Chapter**: ${metadata.chapter}\\n`\n\t\t\t: \"\";\n\n\t\tconst prompt = `\n${boardContext} ${levelContext}\nYour task is to answer exam questions accurately.\n\n${subjectChapter}\nQUESTIONS:\n${questionsText}\n\nTEXTBOOK CONTENT:\n${context}\n\nINSTRUCTIONS:\n1. For EACH question, generate the CORRECT ANSWER and EXPLANATION.\n2. **CRITICAL FOR MCQs - FOLLOW EXACTLY**: \n   - You MUST select from the options provided in the question.\n   - The \"correct_answer\" field MUST be the **CLEAN TEXT** of the correct option.\n   - **STRICT RULE**: Do NOT include the *outer* option label (A, B, C, D).\n   - **PRESERVE** content like \"(i) and (ii)\" if it is part of the option text.\n   - Example: If option is \"(i) and (ii)\", answer MUST be \"(i) and (ii)\".\n   - Example: If option is \"A. (i) and (ii)\", answer MUST be \"(i) and (ii)\" (Strip 'A.', keep '(i)...').\n   - **NEVER** create an answer that is not in the options list (after stripping outer prefixes).\n   - **NEVER** return just the letter (e.g., \"B\").\n   - **NEVER** hallucinate an answer (e.g., do not write \"Grass → Hawk\" if it is not an option).\n   - If multiple options are correct, list all correct option texts separated by \" and \".\n3. **SOURCE PRIORITY**:\n   - First, check the provided TEXTBOOK CONTENT.\n   - If the answer is NOT in the textbook, **YOU MUST USE YOUR GENERAL KNOWLEDGE**.\n   - **NEVER** return \"Cannot be determined\" or \"Not found in text\" for standard academic questions. Always provide the correct academic answer.\n4. **LEVEL**: Keep answers at a ${metadata.level || \"High School / CBSE\"} level.\n\nSTRICTLY PROHIBITED IN EXPLANATIONS:\n- Do not refer to \"the provided text\", \"the context\", \"the above material\", \"Activity X.X\", \"Figure X.X\", \"Table X.X\", \"Reaction X.X\", \"Law X.X\" or similar references.\n- Do not say \"According to the text\" or \"As mentioned in the chapter\".\n- Explanations must be self-contained and based on general subject knowledge + context facts, without meta-references.\n\nOUTPUT FORMAT:\nReturn a JSON object with an \"answers\" array.\n`;\n\n\t\tconst result = await generateObject({\n\t\t\tmodel: google(\"gemini-2.0-flash\"), // Flash is great for large context\n\t\t\tschema: BatchAnswerSchema,\n\t\t\tprompt: prompt,\n\t\t});\n\n\t\treturn result.object.answers;\n\n\t} catch (error) {\n\t\tconsole.error(\"[AI-SOLVE] Error generating batch answers:\", error);\n\t\tthrow error;\n\t}\n}\n"],"names":[],"mappings":"+CAAA,IAAA,EAAA,EAAA,CAAA,CAAA,QACA,EAAA,EAAA,CAAA,CAAA,yCAqBO,OAAM,EAKZ,aAAa,OACZ,CAAa,CACb,EAAgB,EAAE,CAClB,CAIC,CAcC,CACF,GAAI,KAkCC,EA/BJ,GAFA,QAAQ,GAAG,CAAC,CAAC,mCAAmC,EAAE,EAAM,CAAC,CAAC,CAAE,GAExD,CAAC,EAAQ,OAAO,CACnB,CADqB,KACf,AAAI,MAAM,kCAMjB,IAAM,EAAe,CAAC,GAAS,CAAC,EAAM,IAAI,GACpC,EAAa,EAAQ,SAAS,EAAI,EAAQ,SAAS,CAEzD,GAAI,GAAgB,CAAC,EACpB,MAAO,CACN,GAF+B,KAEtB,EAAE,CACX,aAAc,gBACd,MAAO,CAAE,gBAAiB,EAAG,gBAAiB,EAAG,aAAc,CAAE,CAClE,EAID,IAAM,EAAY,EACf,MAAM,EAAA,qBAAqB,CAAC,iBAAiB,CAAC,SAC9C,AADuD,MACjD,EAAA,qBAAqB,CAAC,MAD+D,WAC9C,CAAC,GAG3C,EAAU,EAAQ,OAAO,CACzB,EAAY,EAAQ,SAAS,EAAI,KACjC,EAAY,EAAQ,SAAS,EAAI,KACjC,EAAkB,EAAY,OAAO,GAAa,KAMxD,GAAI,GAAgB,EAEnB,QAAQ,EAFuB,CAEpB,CACV,CAAC,qEAAqE,CAAC,EAExE,QAAQ,GAAG,CACV,CAAC,8BAA8B,EAAE,EAAQ,YAAY,EAAE,EAAU,YAAY,EAAE,EAAU,QAAQ,EAAE,OAAO,EAAU,CAAC,CAAC,EAKnH,GACH,QAAQ,GAAG,CACV,CAAC,CAFkB,wFAEuE,CAAC,EAE5F,EAAU,MAAM,EAAA,EAAM,CAAC,eAAe,CACrC,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;MA2BD,CAAC,CACD,EACA,EACA,IAID,EAJO,AAIG,KAJE,CAII,EAAA,EAAM,CAAC,eAAe,CACrC,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;MA4BD,CAAC,CACD,EACA,EACA,GAIF,GAJQ,KAAK,AAIL,GAAG,CAAC,CAAC,4BAA4B,EAAE,EAAQ,MAAM,CAAC,QAAQ,CAAC,MAC7D,CAGN,IAAM,EAAkB,EAAY,OAAO,GAAa,KAEpD,GAEH,QAAQ,GAAG,CACV,CAAC,CAHkB,sEAGqD,CAAC,EAE1E,EAAU,MAAM,EAAA,EAAM,CAAC,eAAe,CACrC,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAqEJ,CAAC,CACE,CAAC,CAAC,EAAE,EAAU,IAAI,CAAC,KAAK,CAAC,CAAC,CAC1B,EACA,EACA,EACA,IAID,EAJO,AAIG,KAJE,CAII,EAAA,EAAM,CAAC,eAAe,CACrC,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAuEJ,CAAC,CACE,CAAC,CAAC,EAAE,EAAU,IAAI,CAAC,KAAK,CAAC,CAAC,CAC1B,EACA,EACA,EACA,EAGH,CAEA,GALS,KAKD,AALM,GAKH,CAAC,CAAC,mBAAmB,EAAE,EAAQ,MAAM,CAAC,SAAS,CAAC,EAE3D,IAAM,EAAgB,EAAQ,MAAM,CAAC,AAAC,GAAM,EAAE,aAAa,EAAE,MAAM,CAC7D,EAAe,EAAQ,MAAM,CAAE,AAAD,GAAO,EAAE,YAAY,EAAE,MAAM,CAE7D,EAA0D,SA6C9D,OA5CI,EAAgB,GAAsB,IAAjB,EAAoB,EAAe,cACnD,EAAe,GAAuB,IAAlB,GAC5B,GAAe,cAAA,EA0CT,CACN,QAzC2C,CAyClC,CAzC0C,GAAG,CAAC,AAAC,IACxD,IASI,EATA,EAAa,EAAE,IAAI,CACvB,GAAsB,AAQkB,UARR,AAA5B,OAAO,EAAE,IAAI,CAChB,GAAI,CACH,EAAa,KAAK,KAAK,CAAC,EAAE,IAAI,CAC/B,CAAE,MAAO,EAAG,CACX,EAAa,IACd,CAYD,OARI,GAAc,MAAM,OAAO,CAAC,IAAe,EAAW,MAAM,CAAG,GAAG,CACxC,UAAzB,OAAO,CAAU,CAAC,EAAE,EAAiB,CAAU,CAAC,EAAE,CAAC,IAAI,CAC1D,CAD4D,CAC9C,CAAU,CAAC,EAAE,CAAC,IAAI,CACtB,AAAyB,UAAU,OAA5B,CAAU,CAAC,EAAE,GAC9B,EAAc,CAAA,GAIT,CACN,GAAI,EAAE,EAAE,CAAC,QAAQ,GACjB,QAAS,EAAE,OAAO,CAClB,MAAO,EAAE,KAAK,CACd,QAAS,EAAE,OAAO,CAClB,WAAY,EAAE,UAAU,CACxB,UAAW,EAAE,SAAS,CACtB,cAAe,EAAE,aAAa,CAC9B,aAAc,EAAE,YAAY,CAC5B,SAAU,EAAE,WAAW,CACpB,CACD,WAAY,EAAE,WAAW,CACzB,SAAU,EAAE,SAAS,EAAI,GACzB,YAAa,GAAe,CAAC,EAAG,EAAG,EAAG,EAAE,CACxC,aAAc,EAAE,OAAO,EAAI,GAC3B,MAAO,EAAE,KAAK,AACf,OACE,CACJ,CACD,gBAIC,EACA,MAAO,CACN,gBAAiB,EACjB,gBAAiB,EACjB,aAAc,EAAQ,MAAM,AAC7B,CACD,CACD,CAAE,MAAO,EAAO,CAEf,OADA,QAAQ,KAAK,CAAC,sBAAuB,GAC9B,CACN,QAAS,EAAE,CACX,aAAc,eACd,MAAO,CAAE,gBAAiB,EAAG,gBAAiB,EAAG,aAAc,CAAE,CAClE,CACD,CACD,CACD,oFC3ZA,IAAA,EAAA,EAAA,CAAA,CAAA,QAKA,IAAI,EAA0C,KAC9C,eAAe,WACb,AAAI,GACJ,GAAoB,CAAC,UACnB,EAFqB,CAEjB,CAEF,KAJ0B,CAIpB,EAAA,OAAM,CAAC,iBAAiB,CAC5B,CAAC,kEAAkE,CAAC,EAEtE,GAAI,CACF,MAAM,EAAA,OAAM,CAAC,iBAAiB,CAC5B,CAAC;;;;;YAKC,CAAC,EAEL,MAAM,EAAA,OAAM,CAAC,iBAAiB,CAC5B,CAAC;;;;;;+BAMoB,CAAC,EAExB,MAAM,EAAA,OAAM,CAAC,iBAAiB,CAC5B,CAAC;;;;;;;;;kBASO,CAAC,CAEb,QAAU,CACR,MAAM,EAAA,OAAM,CAAC,iBAAiB,CAC5B,CAAC,oEAAoE,CAAC,CAE1E,CACF,CAAE,MAAO,EAAG,CAEV,QAAQ,IAAI,CAAC,gDAAiD,EAChE,EACF,CAAC,EAAA,CAEH,CAEO,eAAe,EAAW,CAAW,EAC1C,GAAI,CACF,MAAM,IACN,IAAM,EAAiC,MAAM,EAAA,OAAM,CAAC,eAAe,CACjE,CAAC,qDAAqD,CAAC,CACvD,GAEF,GAAI,GAAQ,EAAK,MAAM,CAAG,EAAG,OAAO,CAAI,CAAC,EAAE,CAAC,KAAK,CACjD,OAAO,IACT,CAAE,MAAO,EAAG,CAEV,OADA,QAAQ,IAAI,CAAC,mCAAoC,GAC1C,IACT,CACF,CAaO,eAAe,EAAc,CAAW,CAAE,CAAoB,EACnE,IAAM,EAAI,MAAM,EAAW,GAC3B,GAAS,MAAL,EAAW,OAAO,EACtB,IAAM,EAAI,OAAO,SACjB,AAAI,CAAC,OAAO,QAAQ,CAAC,IAAM,GAAK,EAAU,CAAP,CAC5B,KAAK,KAAK,CAAC,EACpB,2GCxFA,IAAA,EAAA,EAAA,CAAA,CAAA,iBAmGA,eAAe,EACd,CAAwB,CACxB,CAAwB,CACxB,CAAkC,CAClC,CAA6B,EAE7B,IAAM,EAAyB,EAAE,CAGjC,IAAK,IAAI,EAAI,EAAG,EAAI,EAAO,MAAM,CAAE,KAAK,AAAyB,CAChE,IAAM,EAAQ,EAAO,KAAK,CAAC,EAAG,IAAI,CAC5B,EAAgB,EAAM,GAAG,CAAC,CAAC,EAAO,IACvC,EACC,EACA,EAAI,EACJ,EAAO,MAAM,CACb,EACA,IAII,EAAW,KAAK,KAAK,CAAC,IAAI,CAA2B,EACrD,EAAe,KAAK,IAAI,CAAC,EAAO,MAAM,CAnHd,EAmHiB,CAO/C,AA1HiC,GAqHjC,QAAQ,GAAG,CACV,CAAC,sBAtHqE,gBAsH/B,EAAE,EAAS,CAAC,EAAE,EAAa,MAAM,EAAE,EAAM,MAAM,CAAC,OAAO,CAAC,EAI5F,EAAY,CACf,IAAM,EAAiB,EACjB,EAAc,EAAO,MAAM,CACjC,EACC,CAAC,kBAAkB,EAAE,EAAiB,EAAE,CAAC,EAAE,KAAK,GAAG,CAClD,EAAiB,EAAM,MAAM,CAC7B,GACC,IAAI,EAAE,EAAY,GAAG,CAAC,CAE1B,CAEA,IAAM,EAAe,MAAM,QAAQ,GAAG,CAAC,GACvC,EAAQ,IAAI,IAAI,EACjB,CAEA,OAAO,CACR,CA4CA,eAAe,EACd,CAAqB,CACrB,CAAa,CACb,CAAmB,CACnB,CAAwB,CACxB,CAAkC,EAElC,QAAQ,GAAG,CACV,CAAC,oCAAoC,EAAE,EAAQ,EAAE,CAAC,EAAE,EAAY,MAAM,EACrE,EAAM,MAAM,CACZ,QAAQ,CAAC,EAEX,IAAM,EAAY,KAAK,GAAG,GAE1B,GAAI,CACH,IAAM,EAAe,AAtDvB,SAAS,AAAuB,CAAuB,EACtD,GAAuB,GAAG,CAAtB,EAAQ,MAAM,CACjB,MAAO,4BAIR,IAAI,EAAU,CAAC,eAAe,EAAE,EAAQ,MAAM,CAAC;AAAA;AAAc,CAAC,CA2B9D,OAxBA,GAAW,kBACX,EAAQ,OAAO,CAAC,CAAC,EAAQ,KACxB,GAAW,CAAC,CAAC,EAAE,EAAQ,EAAE,SAAS,EAAE,EAAO,KAAK,CAAC,YAAY,EAC5D,EAAO,OAAO,EAAI,gBAClB,SAAS,EACT,EAAO,eAAe,EAAE,sBAAwB,eAChD;AAAE,CAAC,AACL,GAGA,GAAW,wBACX,EAAQ,OAAO,CAAC,CAAC,EAAQ,KACxB,IAAM,EAAU,EAAO,IAAI,EAAI,uBAS/B,GAPW,CAAC;AAAA,QAAU,EAAE,EAAQ,EACpB,AADsB,CAAC;AACvB,OAAS,EAAE,EAAO,KAAK,EAAE;AACzB,SAAW,EAAE,EAAO,OAAO,EAAI,iBAAiB;AAChD,MAAQ,EACnB,EAAO,eAAe,EAAE,sBAAwB,gBAC/C;AACU,SAAW,EAAE;AAAS;AAAT,AACzB,CADkC,AAEnC,AARqC,GAU9B,CACR,EAJa,AAwBiC,GAGtC,EAAgB,MAAM,CAAA,EAAA,EAAA,kBAAA,AAAkB,EAC7C,EACA,EACA,EACA,oBAGK,EAAU,KAAK,GAAG,GAAK,EAU7B,OATA,QAAQ,GAAG,CACV,CAAC,qCAAqC,EACrC,EAAQ,EACR,CAAC,EAAE,EAAY,IAAI,EAAE,EAAQ,YAAY,EACzC,EAAc,WAAW,CACzB,KAAK,EAAE,EAAc,YAAY,CAAC,KAAK,CAAC,EAInC,CACN,KAAM,EAAc,IAAI,CACxB,YAAa,EAAc,WAAW,EAAI,EAC1C,aAAc,EAAc,YAAY,EAAI,EAC5C,OAAO,CACR,CACD,CAAE,MAAO,EAAO,CACf,IAAM,EACL,aAAiB,MAAQ,EAAM,OAAO,CAAG,gBAS1C,OARA,QAAQ,KAAK,CACZ,CAAC,8CAA8C,EAC9C,EAAQ,EACR,CAAC,EAAE,EAAY,CAAC,CAAC,CAClB,GAIM,CACN,KAAM,GACN,YAAa,EACb,aAAc,EACd,OAAO,eACP,CACD,CACD,CACD,CAMO,eAAe,EACrB,CAAgB,CAChB,CAAuB,CACvB,EAAqC,EAAE,CACvC,CAA6B,EAE7B,MAAM,EAAS,EAAE,CACX,EAAc,EAAQ,MAAM,CAGlC,IAAK,IAAI,EAAI,EAAG,EAAI,EAAa,KAAK,CACrC,EAAO,IAAI,CAAC,EAAQ,CAD6B,IACxB,CAAC,EAAG,EApQZ,EAoQgB,EApQZ,CAuQtB,QAAQ,GAAG,CACV,CAAC,2BAxQ6D,KAwQ7B,EAAE,YAAY,IAAc,EAAO,MAAT,AAAe,CAAC,iBAAiB,EAAE,oBAA6C,EAIxI,EAJmH,CAKtH,EACC,CAAC,MAFa,KAEF,EAAE,EAAY,CAN+G,WAMnG,EAAE,EAAO,MAAM,CAAC,UAAU,CAAC,EAKnE,IAAM,EA7PF,CAHE,EAAgB,EAAS,WAAW,CAgQjB,GA7PP,KAAK,CAAC,wCAChB,CAAC,AADwD;;qDAGb,CAAC,CAIjD,EAAc,KAAK,CAAC,2CAChB,CAAC,AAD2D;;8CAGvB,CAAC,CAI1C,EAAc,KAAK,CAAC,gDAChB,CADiE,AAChE;;wDAE8C,CAAC,CAIpD,EAAc,KAAK,CAAC,6CAChB,CAD8D,AAC7D;;;;oEAI0D,CAAC,CAIhE,EAAc,KAAK,CAAC,sCAChB,CADuD,AACtD;;+CAEqC,CAAC,CAI3C,EAAc,KAAK,CAAC,oDAChB,CADqE,AACpE,0DAA0D,EAuNjB,AAvNmB,EAAS;;;;;qDAK1B,CAAC,CAKpD,EAAc,KAAK,CAClB,qDAGM,CAAC,AADP;;oDAGiD,CAAC,CAI7C,CAAC,sDAAsD,EAAE,EAAS;;;wDAGlB,CAAC,CAiMxD,QAAQ,GAAG,CACV,CAAC,gDAAgD,EAAE,EACjD,KAAK,CAAC,KAAK,CAAC,EAAE,CACd,SAAS,CAAC,EAAG,IAAI,GAAG,CAAC,EAIxB,IAAI,EAAmB,EACnB,EAAoB,EAGlB,EAAiB,KAAK,GAAG,GACzB,EAAe,MAAM,EAC1B,EACA,EACA,EACA,GAEK,EAAsB,KAAK,GAAG,GAAK,EAGzC,EAAa,OAAO,CAAC,AAAC,IACrB,GAAoB,EAAO,WAAW,CACtC,GAAqB,EAAO,YAAY,AACzC,GAGA,IAAM,EAAe,EAAa,MAAM,CAAC,AAAC,GAAM,EAAE,KAAK,EACjD,EAAmB,EAAa,MAAM,CAAC,AAAC,GAAM,CAAC,EAAE,KAAK,EAExD,EAAa,MAAM,CAAG,GAAG,CAC5B,QAAQ,IAAI,CACX,CAAC,wBAAwB,EAAE,EAAa,MAAM,CAAC,CAAC,EAAE,EAAO,MAAM,CAAC,0CAA0C,CAAC,EAE5G,EAAa,OAAO,CAAC,CAAC,EAAO,KAC5B,QAAQ,IAAI,CACX,CAAC,kCAAkC,EAAE,EAAM,EAAE,EAAE,EAAE,EAAM,YAAY,CAAA,CAAE,CAEvE,IAID,IAAM,EAAgB,EAAiB,GAAG,CAAC,AAAC,GAAM,EAAE,IAAI,EAExD,QAAQ,GAAG,CACV,CAAC,6CAA6C,EAAE,EAAoB,IAAI,EAAE,EAAiB,MAAM,CAAC,CAAC,EAAE,EAAO,MAAM,CAAC,wCAAwC,EAAE,EAAiB,KAAK,EAAE,EAAkB,IAAI,CAAC,EAI7M,IAAI,EAAe,CAAC,4BAA4B,EAAE,EAAiB,MAAM,CAAC,CAAC,EAAE,EAAO,MAAM,CAAC,SAAS,EAAE,EAAY,gBAAgB,CAAC,CAG/H,EAAa,MAAM,CAAG,GAAG,CAC5B,GAAgB,CAAC;AAAA;AAAA,YAAgB,EAAE,EAAa,MAAM,CAAC,0EAA0E,EAAE,EAAiB,MAAM,CAAC;CAA2C,AAAC,EAKxM,GAFgB,CAAC,YAED,CAAC;AAFA;AAAI,EAAE,EAAc,IAAI,CAAC;AAEzB;AAAA,4GAAgH,EAAE,EAAS,CAAC,CAFpE,AAEqE,CAG1I,EAAa,MAAM,CAAG,GAAG,CAC5B,GAAgB,CAAC;AAAA;AAAA,0LAA6L,AAAC,EAIhN,IAAM,EAAmB,EAAa,MAAM,CAC5C,QAAQ,GAAG,CACV,CAAC,yCAAyC,EAAE,EAAiB,cAAc,EAAE,KAAK,KAAK,CACtF,EAAmB,GAClB,QAAQ,CAAC,EAGR,EA3V2B,KA4V9B,CA5VoC,OA4V5B,IAAI,AADU,CAErB,CAAC,uBAF6C,iBA3V0C,cA6VjC,EAAE,EAAiB,sHAAsH,CAAC,EAK/L,GACH,EACC,CAAC,MAFa,2BAEoB,EAAE,EAAiB,MAAM,CAAC,UAAU,CAAC,EAKzE,IAAM,EAAqB,KAAK,GAAG,GAC7B,EAAgB,MAAM,CAAA,EAAA,EAAA,kBAAA,AAAkB,EAC7C,EACA,EACA,EACA,oBAEK,EAAgB,KAAK,GAAG,GAAK,EAqBnC,OAlBA,GAAoB,EAAc,WAAW,EAAI,EACjD,GAAqB,EAAc,YAAY,EAAI,EAEnD,QAAQ,GAAG,CACV,CAAC,kDAAkD,EAAE,EAAc,YAAY,EAAE,EAAc,WAAW,CAAC,KAAK,EAAE,EAAc,YAAY,CAAC,KAAK,CAAC,EAEpJ,QAAQ,GAAG,CACV,CAAC,8CAA8C,EAC9C,EAAsB,EACtB,EAAE,CAAC,EAEL,QAAQ,GAAG,CACV,CAAC,0CAA0C,EAAE,EAAiB,SAAS,EAAE,EAAkB,UAAU,EACpG,EAAmB,EACnB,OAAO,CAAC,EAIH,CACN,KAAM,EAAc,IAAI,CACxB,YAAa,EACb,aAAc,CACf,CACD,yEClZA,IAAA,EAAA,EAAA,CAAA,CAAA,QAEO,IAAM,EAAc,EAAA,CAAC,CAAC,MAAM,CAAC,CAChC,MAAO,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,4CAC3B,KAAM,EAAA,CAAC,CAAC,IAAI,CAAC,CAAC,MAAO,OAAQ,MAAO,OAAO,EAAE,QAAQ,CAAC,+BACtD,YAAa,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,wCACjC,SAAU,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,kEAC9B,WAAY,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,QAAQ,CAAC,wEACzC,KAAM,EAAA,CAAC,CAAC,KAAK,CAAC,CAAC,EAAA,CAAC,CAAC,MAAM,GAAI,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,EAAA,CAAC,CAAC,GAAG,KAAK,EAAE,QAAQ,CAAC,2DACrE,mMCTA,EAAA,CAAA,CAAA,QACA,IAAA,EAAA,EAAA,CAAA,CAAA,QAMA,EAAA,EAAA,CAAA,CAAA,QACA,EAAA,CAAA,CAAA,QACA,IAAA,EAAA,EAAA,CAAA,CAAA,QACA,EAAA,EAAA,CAAA,CAAA,OACA,EAAA,EAAA,CAAA,CAAA,QACA,EAAA,EAAA,CAAA,CAAA,QACA,EAAA,EAAA,CAAA,CAAA,mBAkEA,eAAe,EACd,CAAmB,CACnB,KAAkC,CAClC,EAAoB,IADA,SACa,EAEjC,IAAM,EAAiB,IAAI,QAAe,CAAC,EAAG,KAC7C,WAAW,KACV,EACC,AAAI,MAAM,CAAA,EAAG,EAAU,iBAAiB,EAAE,EAAY,IAAK,QAAQ,CAAC,EAEtE,EAAG,EACJ,GAEA,OAAO,QAAQ,IAAI,CAAC,CAAC,EAAS,EAAe,CAC9C,CAitCA,SAAS,EAAmB,CAAY,EACvC,OAAO,KAAK,IAAI,CAAC,EAAK,MAAM,CAAG,EAChC,CAKO,eAAe,EACrB,CAAgB,CAChB,CAAe,CACf,EAAqC,EAAE,CACvC,EAAoB,iBAAiB,CACrC,EAAgE,CAAC,CAAC,EAQxC,AAStB,AAAC,CARJ,kBACA,mBACA,YACA,cACA,UACA,eACA,gBACA,CACsB,QAAQ,CAAC,KAC/B,EAAY,KAD+B,YAC/B,EAEb,GAAM,QAAE,CAAM,OAAE,CAAK,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,eAAA,AAAe,EAAC,CAC/C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GACM,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAA,AAAmB,EAAC,UACrC,EAAgB,MAAM,IAAI,CAC/B,IAAI,IAAI,CAAC,EAAK,KAAK,IAAK,EAAS,CAAC,MAAM,CAAC,WAG1C,GAA6B,IAAzB,EAAc,MAAM,CAAQ,CAC/B,IAAM,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,mBAC1D,EAAc,IAAI,CAAC,EACpB,CAGA,IAAM,EAAgB,EAAoB,KAAK,CAAC,CAAC,GAC3C,CAD+C,CAEpD,EAAc,MAAM,CAAG,EACpB,CAAC,MAHmE;AAGnE;AAAyB,EAAE,EAC5B,GAAG,CAAC,AAAC,GAAQ,CAAA,EAAG,EAAI,IAAI,CAAC,WAAW,GAAG,EAAE,EAAE,EAAI,OAAO,CAAA,CAAE,EACxD,IAAI,CAAC,MAAM;AAAE,CAAC,CACd,GAGA,EAAmB,GACvB,OAAQ,GACP,IAAK,mBACJ,EAAmB,CAAC;;;;;;;;;AASvB,CAAC,CACE,KACD,KAAK,YACJ,EAAmB,CAAC;;;;;AAKvB,CAAC,CACE,KACD,KAAK,cACJ,EAAmB,CAAC;;;;;AAKvB,CAAC,CACE,KACD,KAAK,eACJ,EAAmB,CAAC;;;;;AAKvB,CAAC,CACE,KACD,SACC,EAAmB,CAAC;;;;;;AAMvB,CAAC,AACA,CAEA,IAAM,EAAS,CAAC;;;;AAIjB,EAAE,QAAQ;;;AAGV,EAAE,eAAe;;;CAGhB,EAAE,EAAS;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAwCZ,EAAE,iBAAiB;;;;OAIZ,CAAC,CAGP,GAAkB,iBAAiB,CAA/B,EACH,GAAI,CACH,QAAQ,GAAG,CAAC,yDAEZ,IAAM,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAA,AAAmB,EAAC,UACrC,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,mBACpD,EAAY,EAAK,KAAK,EAAI,CAAQ,CAAC,EAAE,EAAI,EAEzC,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,iBAAA,AAAiB,EAAC,CAAE,SAAU,QAAS,GAC1D,EAAW,GAAU,QAAQ,GAAG,CAAC,cAAc,CAErD,GAAI,CAAC,EACJ,MAAM,AAAI,EADI,IACE,qDAGjB,IAAM,EAAS,CAAA,EAAA,EAAA,wBAAA,AAAwB,EAAC,CACvC,OAAQ,CACT,GAGM,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,cAAc,AAAd,EAAe,CACvC,MAAO,EAAO,GACd,OAAQ,EAAA,WAAW,CACnB,OAAQ,CAAC;;;AAGb,EAAE,QAAQ;;;AAGV,EAAE,eAAe;;YAEL,EAAE,SAAS;;;;;AAKvB,CAAC,AACE,GAEI,EAAkB,EAAO,IAAI,CACjC,GAA2B,UAAvB,AAAiC,OAA1B,EAAO,IAAI,CACrB,GAAI,CACH,EAAa,KAAK,KAAK,CAAC,EAAO,IAAI,CACpC,CAAE,MAAO,EAAG,CACX,QAAQ,KAAK,CAAC,qCAAsC,GACpD,EAAa,EACd,AADgB,CAIjB,IAAM,EAAc,CAAE,GAAG,CAAM,CAAE,KAAM,CAAW,EAKlD,OAJA,QAAQ,GAAG,CACV,kCACA,KAAK,SAAS,CAAC,EAAa,KAAM,IAE5B,CACN,KAAM,CAAC,YAAY,EAAE,EAAO,IAAI,CAAC,uCAAuC,CAAC,CACzE,YAAa,EACb,aAAc,EACd,UAAW,CACZ,CACD,CAAE,MAAO,EAAO,CACf,QAAQ,KAAK,CAAC,oCAAqC,GACnD,QAAQ,KAAK,CACZ,yBACA,aAAiB,MAAQ,EAAM,OAAO,CAAG,OAAO,IAGjD,QAAQ,GAAG,CAAC,6CAEb,CAGD,IAAI,EAAiB,KACrB,IAAK,IAAM,KAAa,EACvB,GAAI,OACH,EAFqC,MAE7B,GAAG,CACV,CAAC,8CAA8C,EAAE,EAAU,eAAe,EAAE,EAAO,MAAM,CAAC,WAAW,CAAC,EAEvG,IAAM,EAz+CT,AAy+CyB,SAz+ChB,AAAU,CAAa,EAC/B,IAAM,EAAc,CAAA,EAAG,EAAM,CAAC,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,KAAK,MAAM,GACvD,QAAQ,CAAC,IACT,MAAM,CAAC,EAAG,GAAA,CAAI,CAGhB,OAFA,QAAQ,IAAI,CAAC,CAAC,SAAS,EAAE,EAAA,CAAa,EACtC,QAAQ,GAAG,CAAC,CAAC,eAAe,EAAE,EAAA,CAAO,EAC9B,CAAE,UAAW,KAAK,GAAG,eAAI,CAAY,CAC7C,EAk+CmC,mBAC1B,EAAQ,EAAO,kBAAkB,CAAC,CAAE,MAAO,CAAoB,GAEjE,EAAc,EAClB,GAAI,CACH,IAAM,EAAgB,MAAM,EAAM,WAAW,CAAC,CAC7C,SAAU,CACT,CACC,KAAM,OACN,MAAO,CAAC,CAAE,KAAM,CAAO,EAAE,AAC1B,EAEF,AADE,GAEF,EAAe,GAAU,aACxB,GAAU,iBACV,CACF,CAAE,MAAO,EAAG,CAEX,EAAc,EAAmB,GACjC,QAAQ,IAAI,CAAC,+CAAgD,EAC9D,CACA,IAAM,EAAS,MAAM,EACpB,EAAM,eAAe,CAAC,GAthDH,IAwhDnB,CAAC,CADD,CAvhD0B,aAAa,SAwhDd,EAAE,EAAU,CAAC,CAAC,EAElC,EAAW,MAAM,EAAO,QAAQ,CAChC,EAAO,EAAS,IAAI,IA3/C7B,AA4/CG,SA5/CM,AACR,CAAa,CACb,CAAsD,EAEtD,IAAM,EAAU,KAAK,GAAG,GAAK,EAAW,SAAS,CACjD,QAAQ,OAAO,CAAC,CAAC,SAAS,EAAE,EAAW,WAAW,CAAA,CAAE,EACpD,QAAQ,GAAG,CAAC,CAAC,aAAa,EAAE,EAAM,MAAM,EAAE,EAAQ,EAAE,CAAC,CAEtD,EAo/CW,kBAAmB,GAG3B,IAAM,EAAc,GAAkB,cAChC,EAAgB,GAAO,sBAC5B,GAAO,iBACP,EAAmB,GASpB,OAt+C8B,EA+9Ce,CAC5C,CAh+CuC,aAi+CvC,yBACA,CACD,EAj+CD,QAAQ,GAAG,CAAC,CAAC,iBAAiB,EAAE,SAAS,+BAC5B,IAAT,GACH,IADuB,IACf,GAAG,CAAC,GAEb,QAAQ,GAAG,CAAC,OA89CP,GAAO,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,GAAO,GAEhC,CACN,KAAM,cACN,eACA,CACD,CACD,CAAE,MAAO,EAAY,CACpB,EAAY,EACR,GAAO,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,GAAO,GAEnC,CADa,GAAO,SAAW,OAAO,EAAA,EAC7B,QAAQ,CAAC,aACrB,CADmC,OAC3B,IAAI,CACX,CAAC,2CAA2C,EAAE,gBAC7B,CAD8C,EACnC,EAG7B,CAHE,OAGM,IAAI,CAAC,CAAC,CAHE,8BAG6B,EAAE,EAAA,CAAW,CAAE,GAE7D,QACD,CAGD,GADA,QAAQ,KAAK,CAAC,oDAAqD,GAC/D,GAAW,SAAW,EAAU,OAAO,CAAC,QAAQ,CAAC,OACpD,CAD4D,KACtD,AAAI,MAAM,uBAGjB,IAAM,EAAe,EAClB,CAAC,EAAE,EAAE,EAAU,OAAO,EAAI,OAAO,GAAA,CAAY,CAC7C,sCACH,OAAM,AAAI,MAAM,CAAC,8BAA8B,EAAE,EAAA,CAAc,CAChE,4BAq3DA,IAAM,EAAqB,EAAA,CAAC,CAAC,MAAM,CAAC,CACnC,cAAe,EAAA,CAAC,CAAC,MAAM,GACvB,cAAe,EAAA,CAAC,CAAC,IAAI,CAAC,CACrB,MACA,aACA,gBACA,eACA,cACA,EACD,QAAS,EAAA,CAAC,CACR,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IACd,QAAQ,GACR,QAAQ,CAAC,iDACX,eAAgB,EAAA,CAAC,CACf,KAAK,CAAC,CAAC,EAAA,CAAC,CAAC,MAAM,GAAI,EAAA,CAAC,CAAC,MAAM,GAAI,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,EACnD,QAAQ,CAAC,0FACX,OAAQ,EAAA,CAAC,CAAC,MAAM,GAAG,OAAO,CAAC,GAC3B,YAAa,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,2CAClC,GAEM,EAAa,EAAA,CAAC,CAAC,MAAM,CAAC,CAC3B,MAAO,EAAA,CAAC,CAAC,MAAM,GACf,YAAa,EAAA,CAAC,CAAC,MAAM,GACrB,UAAW,EAAA,CAAC,CAAC,KAAK,CAAC,EACpB,GAGM,EAAsB,EAAA,CAAC,CAAC,MAAM,CAAC,CACpC,UAAW,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,CAC3B,cAAe,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,qBACnC,cAAe,EAAA,CAAC,CAAC,IAAI,CAAC,CAAC,MAAO,aAAc,gBAAiB,eAAgB,cAAc,EAC3F,WAAY,EAAA,CAAC,CAAC,IAAI,CAAC,CAAC,OAAQ,SAAU,OAAO,EAC7C,QAAS,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,QAAQ,GAAG,QAAQ,CAAC,yDACjD,eAAgB,EAAA,CAAC,CAAC,GAAG,GAAG,QAAQ,CAAC,oFACjC,YAAa,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,qDACjC,OAAQ,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,yCAC7B,GACD,GAgBO,eAAe,EAAuB,CAA0B,EACtE,GAAM,SAAE,CAAO,cAAE,CAAY,QAAE,CAAM,CAAE,CAAG,EAGtC,EAAwB,EAAE,CAC1B,EAAiB,EAWrB,GATC,CAAC,OAAQ,SAAU,OAAO,CAAW,OAAO,CAAC,IAC7C,OAAO,OAAO,CAAC,CAAM,CAAC,EAAK,EAAE,OAAO,CAAC,CAAC,CAAC,EAAM,EAAM,IAC9C,GAAS,EAAQ,GAAG,CACvB,EAAY,IAAI,CAAC,CAAA,EAAG,EAAM,CAAC,EAAE,EAAK,WAAW,GAAG,CAAC,EAAE,EAAK,UAAU,CAAC,EACnE,GAAkB,EAEpB,EACD,GAEuB,IAAnB,EAAsB,MAAO,EAAE,CAEnC,IAAM,EAAS,CAAC;iCACgB,EAAE,EAAe,qCAAqC,EAAE,EAAa;;;AAGtG,EAAE,QAAQ;;;;;AAKV,EAAE,EAAY,GAAG,CAAC,GAAK,CAAC,EAAE,EAAE,EAAA,CAAG,EAAE,IAAI,CAAC,MAAM;;;;;;;;;;;;;;;;;;;;;;;;;;8CA0BE,CAAC,CAE9C,GAAI,CAGH,GAAM,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,iBAAA,AAAiB,EAAC,CAAE,SAAU,QAAS,GAC1D,EAAW,GAAU,QAAQ,GAAG,CAAC,cAAc,CAErD,GAAI,CAAC,EACJ,MAAM,AAAI,EADI,IACE,2BAGjB,IAAM,EAAS,CAAA,EAAA,EAAA,wBAAA,AAAwB,EAAC,CAAE,OAAQ,CAAS,GAGrD,EAAwB,EAAE,CAC1B,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAA,AAAmB,EAAC,UAC3C,EAAY,IAAI,IAAI,GACpB,IAAM,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,mBAI1D,IAAK,IAAM,KAHX,EAAY,IAAI,CAAC,CAGO,EAFH,IAAI,IAAI,GAES,CAFL,GAAa,EAG7C,GAAI,CAkCH,OAjCA,AAiCO,QAjCC,GAAG,CAAC,CAAC,wDAAwD,EAAE,EAAA,CAAW,EAUtD,CARb,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,CACnC,MAAO,EAAO,GACd,OAAQ,EACR,OAAQ,EACR,KAAM,MACP,EAAA,EAGmC,MAAM,CAAC,SAAS,CAAC,GAAG,CAAC,IACvD,IAAI,EAAgB,EACpB,CADuB,MACf,EAAE,EADuB,WACV,EACtB,IAAK,MACL,IAAK,aACL,IAAK,gBACJ,EAAgB,EAChB,KACD,KAAK,eACJ,EAAgB,EAChB,KACD,KAAK,cACJ,EAAgB,CAElB,CAMA,OAJI,EAAE,MAAM,GAAK,GAChB,QAAQ,GAAG,CADoB,AACnB,CAAC,iCAAiC,EAAE,EAAE,aAAa,CAAC,EAAE,EAAE,EAAE,MAAM,CAAC,GAAG,EAAE,EAAA,CAAe,EAG3F,CAAE,GAAG,CAAC,CAAE,OAAQ,CAAc,CACtC,EAGD,CAAE,MAAO,EAAY,CACpB,QAAQ,IAAI,CAAC,CAAC,6BAA6B,EAAE,EAAU,EAAE,EAAE,EAAM,OAAO,CAAA,CAAE,CAE3E,CAGD,MAAM,AAAI,MAAM,0CAEjB,CAAE,MAAO,EAAO,CAEf,OADA,QAAQ,KAAK,CAAC,iDAAkD,GACzD,EAAE,AACV,CACD,CAEO,eAAe,EACrB,CAA4B,CAC5B,EAA2E,CAAC,CAAC,EAE7E,GAAI,CACH,IAgHI,EAhHE,QAAE,CAAM,OAAE,CAAK,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,eAAA,AAAe,EAAC,CAC/C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GAIM,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,iBAAA,AAAiB,EAAC,CAC1C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GAEM,EAAW,GAAU,QAAQ,GAAG,CAAC,cAAc,CACrD,GAAI,CAAC,EACJ,MAAM,AAAI,EADI,IAEb,+EAGF,IAAM,EAAS,CAAA,EAAA,EAAA,wBAAA,AAAwB,EAAC,CAAE,OAAQ,CAAS,GAGrD,EAAmB,EAAO,aAAa,CAAC,GAAG,CAAC,CAAC,EAAM,KACxD,IAAM,EAAQ,KAAK,KAAK,CAAC,EAAO,aAAa,CAAG,EAAO,aAAa,CAAC,MAAM,IAC1E,CAAC,EAAM,EAAO,aAAa,CAAG,EAAO,aAAa,CAAC,MAAA,AAAM,EAC1D,CAD6D,IAAI,CAAC,AAC3D,CAAA,EAAG,EAAM,EAAE,EAAE,EAAA,CAAM,AAC3B,GAAG,IAAI,CAAC,MACF,EAAe,EAAK,KAAK,CAAG,CAAC,kBAAkB,EAAE,EAAK,KAAK,CAAC,iBAAiB,CAAC,CAAG,GACjF,EAAe,EAAK,KAAK,CAAG,CAAC,uBAAuB,EAAE,EAAK,KAAK,CAAC,UAAU,CAAC,CAAG,GAE/E,EAAS,CAAA,EAAG,EAAa,CAAC,EAAE,aAAa;mBAC9B,EAAE,EAAO,UAAU,CAAC,+CAA+C,EAAE,EAAO,OAAO,CAAC,EAAE,EAAE,EAAO,KAAK,CAAC;;aAE3G,EAAE,EAAO,OAAO,CAAC;aACjB,EAAE,EAAO,KAAK,CAAC;gBACZ,EAAE,EAAO,UAAU,CAAC;;;;;AAKpC,EAAE,EAAO,OAAO,CAAC;;;;;SAKR,EAAE,EAAO,aAAa,CAAC,YAAY,EAAE,EAAiB;cACjD,EAAE,EAAO,aAAa,CAAC,IAAI,CAAC,MAAM;;;EAG9C,EAAwB,SAAtB,EAAO,UAAU,CAAc,wWAAkX,GAAG;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;iDA4CvW,EAAE,EAAO,OAAO,CAAC,wJAAwJ,CAAC,CAMnN,EAAwB,EAAE,CAC5B,EAAK,KAAK,EAAE,EAAY,IAAI,CAAC,EAAK,KAAK,EAG3C,IAAM,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAmB,AAAnB,EAAoB,UAC3C,EAAY,IAAI,IAAI,GAGpB,IAAM,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,mBAQ1D,IAAK,IAAM,KAPX,EAAY,IAAI,CAAC,CAOO,EAJH,IAAI,IAAI,GAIS,CAJL,GAAa,EAK7C,GAAI,CACH,QAAQ,GAAG,CAAC,CAAC,kDAAkD,EAAE,EAAA,CAAW,EAE5E,IAAM,EAAgB,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,CACpC,MAAO,EAAO,GACd,OAAQ,EACR,OAAQ,CACT,GAEM,EAAS,MAAM,QAAQ,IAAI,CAAC,CACjC,EACA,IAAI,QAAQ,CAAC,EAAG,IACf,WAAW,IAAM,EAAO,AAAI,MAAM,+CAAgD,MAEnF,EA6BD,OA3BA,QAAQ,GAAG,CAAC,CAAC,2CAA2C,EAAE,EAAO,MAAM,CAAC,SAAS,CAAC,MAAM,CAAC,UAAU,CAAC,EAGpG,EAAO,MAAM,CAAC,SAAS,CAAG,EAAO,MAAM,CAAC,SAAS,CAAC,GAAG,CAAC,IACrD,IAAI,EAAgB,EACpB,CADuB,MACf,EAAE,EADuB,WACV,EACtB,IAAK,MACL,IAAK,aACL,IAAK,gBACJ,EAAgB,EAChB,KACD,KAAK,eACJ,EAAgB,EAChB,KACD,KAAK,cACJ,EAAgB,CAElB,CAMA,OAJI,EAAE,MAAM,GAAK,GAChB,QAAQ,GAAG,CADoB,AACnB,CAAC,gCAAgC,EAAE,EAAE,aAAa,CAAC,EAAE,EAAE,EAAE,MAAM,CAAC,GAAG,EAAE,EAAA,CAAe,EAG1F,CAAE,GAAG,CAAC,CAAE,OAAQ,CAAc,CACtC,GAEI,GAAO,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,GAAO,GAChC,EAAO,MAAM,AAErB,CAAE,MAAO,EAAY,CAcpB,GAbA,QAAQ,IAAI,CAAC,CAAC,4BAA4B,EAAE,EAAU,EAAE,EAAE,EAAM,OAAO,CAAA,CAAE,EACzE,EAAY,EAYR,CAAC,CANJ,EAAM,OAAO,CAAC,QAAQ,CAAC,QACvB,EAAM,OAAO,CAAC,QAAQ,CAAC,QACvB,EAAM,OAAO,CAAC,QAAQ,CAAC,UACvB,EAAM,OAAO,CAAC,QAAQ,CAAC,cACvB,EAAM,OAAO,CAAC,QAAQ,CAAC,aAAA,GAEJ,EAAK,KAAK,CAE7B,CAF+B,KAEzB,CAIR,CAID,MAAM,GAAa,AAAI,MAAM,oDAE9B,CAAE,MAAO,EAAY,CAEpB,MADA,QAAQ,KAAK,CAAC,yBAA0B,GAClC,AAAI,MAAM,CAAC,yBAAyB,EAAE,EAAM,OAAO,EAAI,OAAO,GAAA,CAAQ,CAC7E,CACD,CAEO,eAAe,EACrB,CAKG,CACH,EAA2C,CAAC,CAAC,EAE7C,GAAI,CAEH,IAAM,EAAsB,EAAU,MAAM,CAAC,AAAC,GAC7C,CAAC,eAAgB,cAAc,CAAC,QAAQ,CAAC,EAAE,IAAI,GAGhD,GAAmC,GAAG,CAAlC,EAAoB,MAAM,CAC7B,MAAO,EAAE,CAGV,GAAM,QAAE,CAAM,OAAE,CAAK,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,eAAA,AAAe,EAAC,CAC/C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GAGM,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAA,AAAmB,EAAC,UACrC,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,mBACpD,EAAY,EAAK,KAAK,EAAI,CAAQ,CAAC,EAAE,EAAI,EACzC,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,iBAAA,AAAiB,EAAC,CAC1C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GAEM,EAAW,GAAU,QAAQ,GAAG,CAAC,cAAc,CACrD,GAAI,CAAC,EACJ,MAAM,AAAI,EADI,IAEb,+EAGF,IAAM,EAAS,CAAA,EAAA,EAAA,wBAAA,AAAwB,EAAC,CAAE,OAAQ,CAAS,GAErD,EAAgB,EAAA,CAAC,CAAC,MAAM,CAAC,CAC9B,OAAQ,EAAA,CAAC,CAAC,KAAK,CACd,EAAA,CAAC,CAAC,MAAM,CAAC,CACR,cAAe,EAAA,CAAC,CAAC,MAAM,GACvB,WAAY,EAAA,CAAC,CAAC,OAAO,GACrB,iBAAkB,EAAA,CAAC,CAAC,MAAM,GAAG,GAAG,CAAC,GAAG,GAAG,CAAC,KACxC,SAAU,EAAA,CAAC,CAAC,MAAM,EACnB,GAEF,GAEM,EAAS,CAAC;;;QAGV,EAAE,KAAK,SAAS,CAAC,EAAqB,KAAM,GAAG;;6GAEsD,CAAC,CAEtG,EAAS,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,CACnC,MAAO,EAAO,GACd,OAAQ,EACR,OAAQ,CACT,GAIA,OAFI,GAAO,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,GAAO,GAEhC,EAAO,MAAM,CAAC,MAAM,AAC5B,CAAE,MAAO,EAAO,CAIf,MAHA,QAAQ,KAAK,CAAC,sBAAuB,GAG/B,AAAI,MAAM,uBACjB,CACD,CAUA,IAAM,EAAsB,EAAA,CAAC,CAAC,MAAM,CAAC,CACpC,iBAAkB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,2EACtC,UAAW,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,CAC3B,KAAM,EAAA,CAAC,CAAC,MAAM,GACd,WAAY,EAAA,CAAC,CAAC,MAAM,EACrB,IAAI,QAAQ,CAAC,mEACb,WAAY,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,CAC5B,MAAO,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,oBAC3B,KAAM,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,uBAC3B,IAAI,GAAG,CAAC,IAAI,GAAG,CAAC,IAAI,QAAQ,CAAC,4CAC7B,uBAAwB,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,MAAM,CAAC,GAAG,QAAQ,CAAC,6EAC/D,iBAAkB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,4EACtC,mBAAoB,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,CACpC,KAAM,EAAA,CAAC,CAAC,MAAM,GACd,QAAS,EAAA,CAAC,CAAC,MAAM,GACjB,YAAa,EAAA,CAAC,CAAC,MAAM,EACtB,IAAI,QAAQ,GAAG,QAAQ,CAAC,iDACzB,GAEO,eAAe,EACrB,CAA4B,CAC5B,EAA2C,CAAC,CAAC,EAE7C,GAAI,CAEH,GAAM,QAAE,CAAM,OAAE,CAAK,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,eAAA,AAAe,EAAC,CAC/C,SAAU,SACV,MAAO,EAAK,KAAK,AAClB,GAGM,EAAW,MAAM,CAAA,EAAA,EAAA,mBAAA,AAAmB,EAAC,UACrC,EAAgB,QAAQ,GAAG,CAAC,oBAAoB,EAAI,oBACpD,EAAY,EAAK,KAAK,EAAI,CAAQ,CAAC,EAAE,EAAI,EAGzC,QAAE,CAAM,CAAE,CAAG,MAAM,CAAA,EAAA,EAAA,iBAAA,AAAiB,EAAC,CAAE,SAAU,SAAU,MAAO,EAAK,KAAK,AAAC,GACnF,GAAI,CAAC,EAAQ,MAAM,AAAI,MAAM,2BAC7B,IAAM,EAAS,CAAA,EAAA,EAAA,wBAAA,AAAwB,EAAC,QAAE,CAAO,GAG3C,EAAiB,EAAO,OAAO,CAAC,SAAS,CAAC,EAAG,KAE7C,EAAS,CAAC;;SAET,EAAE,EAAO,OAAO,CAAC,GAAG,EAAE,EAAO,YAAY,CAAC;;;AAGnD,EAAE,eAAe;;;;;;;;;;;;;;;;;;;;;;;;;;4EA0B2D,CAAC,CAErE,EAAS,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,CACnC,MAAO,EAAO,GACd,OAAQ,EACR,OAAQ,CACT,GAIA,OAFI,GAAO,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,GAAO,GAEhC,EAAO,MAAM,AAErB,CAAE,MAAO,EAAO,CAEf,MADA,QAAQ,KAAK,CAAC,oCAAqC,GACzC,AAAJ,MAAU,qCACjB,CACD,CAMA,IAAM,EAA0B,EAAA,CAAC,CAAC,MAAM,CAAC,CACxC,cAAe,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,iCACnC,cAAe,EAAA,CAAC,CAAC,IAAI,CAAC,CAAC,MAAO,eAAgB,cAAe,aAAc,oBAAoB,EAAE,QAAQ,CAAC,6CAC1G,OAAQ,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ,CAAC,iDACvC,QAAS,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,QAAQ,GAAG,QAAQ,CAAC,iCACjD,gBAAiB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ,CAAC,oEACjD,GAE6B,EAAA,CAAC,CAAC,MAAM,CAAC,CACrC,UAAW,EAAA,CAAC,CAAC,KAAK,CAAC,GAAyB,QAAQ,CAAC,kCACtD,GAE+B,EAAA,CAAC,CAAC,MAAM,CAAC,CACvC,eAAgB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,sCACpC,YAAa,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,yDAClC,GA+G0B,EAAA,CAAC,CAAC,MAAM,CAAC,CAClC,QAAS,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,CAAC,CACzB,gBAAiB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,GACpC,sBAAuB,EAAA,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,8CAC3C,eAAgB,EAAA,CAAC,CAAC,KAAK,CAAC,CAAC,EAAA,CAAC,CAAC,MAAM,GAAI,EAAA,CAAC,CAAC,KAAK,CAAC,EAAA,CAAC,CAAC,MAAM,IAAI,EACzD,YAAa,EAAA,CAAC,CAAC,MAAM,EACtB,GACD"}